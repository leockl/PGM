{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from sklearn.utils.validation import check_is_fitted\n",
    "from sklearn.utils.multiclass import check_classification_targets\n",
    "from sklearn.preprocessing import normalize\n",
    "from joblib import Parallel, delayed\n",
    "from scipy import linalg\n",
    "\n",
    "class PGMHQC_fast(BaseEstimator, ClassifierMixin):\n",
    "    \"\"\"The Pretty Good Measurement (PGM) - Helstrom Quantum Centroid (HQC) classifier is a \n",
    "    quantum-inspired supervised classification approach for data with multiple classes.\n",
    "                         \n",
    "    Parameters\n",
    "    ----------\n",
    "    rescale : int or float, default = 1\n",
    "        The dataset rescaling factor. A parameter used for rescaling the dataset. \n",
    "    encoding : str, default = 'amplit'\n",
    "        The encoding method used to encode vectors into quantum densities. Possible values:\n",
    "        'amplit', 'stereo'. 'amplit' means using the amplitude encoding method. 'stereo' means \n",
    "        using the inverse of the standard stereographic projection encoding method. Default set \n",
    "        to 'amplit'.\n",
    "    n_copies : int, default = 1\n",
    "        The number of copies to take for each quantum density. This is equivalent to taking \n",
    "        the n-fold Kronecker tensor product for each quantum density.\n",
    "    measure : str, default = 'pgm'\n",
    "        The measurement used to distinguish between quantum states. Possible values: 'pgm', \n",
    "        'hels'. The value 'pgm' stands for \"Pretty Good Measurement\", 'hels' stands for \n",
    "        \"Helstrom measurement\" (applicable only for binary classification). Default set to \n",
    "        'pgm'.\n",
    "    class_weight : str, default = None \n",
    "        Weights associated with classes. This is the class weights assigned to the quantum \n",
    "        centroids in the Pretty Good Measurement or Helstrom observable. Possible values: None,\n",
    "        'balanced'. If None given, all classes are supposed to have weight one. The 'balanced' \n",
    "        mode uses the values of y to automatically adjust weights inversely proportional to class\n",
    "        frequencies in the input data as n_samples / (n_classes * np.bincount(y)). Default set\n",
    "        to None.\n",
    "    n_jobs : int, default = None\n",
    "        The number of CPU cores used when parallelizing. If -1 all CPUs are used. If 1 is given, \n",
    "        no parallel computing code is used at all. For n_jobs below -1, (n_cpus + 1 + n_jobs) \n",
    "        are used. Thus for n_jobs = -2, all CPUs but one are used. None is a marker for ‘unset’ \n",
    "        that will be interpreted as n_jobs = 1.\n",
    "    n_splits : int, default = 1\n",
    "        The number of subset splits performed on the input dataset row-wise and on the number \n",
    "        of eigenvalues/eigenvectors of the Quantum Helstrom observable for optimal speed \n",
    "        performance. If 1 is given, no splits are performed. For optimal speed, recommend using \n",
    "        n_splits = int(numpy.ceil(number of CPU cores used/number of classes)). If memory blow-out \n",
    "        occurs, reduce n_splits. When n_splits = 1 and memory blow-out still occurs, reduce n_jobs.\n",
    "    \n",
    "    Attributes\n",
    "    ----------\n",
    "    classes_ : ndarray, shape (n_classes,)\n",
    "        Sorted classes.\n",
    "    qcentroids_ : ndarray, shape (n_classes, (n_features + 1)**n_copies, (n_features + 1)**n_copies)\n",
    "        Quantum Centroids for each class.\n",
    "    pgms_ : list, shape (n_classes, (n_features + 1)**n_copies, (n_features + 1)**n_copies)\n",
    "        Values for the Pretty Good Measurements. Only applicable when Pretty Good Measurement is \n",
    "        selected.\n",
    "    pgm_bound_ : float\n",
    "        Pretty Good Measurement bound is the upper bound on the probability that one can correctly\n",
    "        discriminate whether a quantum density is of which of the (multiclass) N quantum density \n",
    "        patterns. Only applicable when Pretty Good Measurement is selected.\n",
    "    proj_sums_ : list, shape (n_classes, (n_features + 1)**n_copies, (n_features + 1)**n_copies)\n",
    "        Sum of the projectors of the Quantum Helstrom observable's unit eigenvectors, which has\n",
    "        corresponding positive and negative eigenvalues respectively. Only applicable when Helstrom\n",
    "        Measurement is selected.\n",
    "    hels_bound_ : float\n",
    "        Helstrom bound is the upper bound on the probability that one can correctly \n",
    "        discriminate whether a quantum density is of which of the two binary quantum density \n",
    "        patterns. Only applicable when Helstrom Measurement is selected.         \n",
    "    \"\"\"\n",
    "    # Initialize model hyperparameters\n",
    "    def __init__(self, \n",
    "                 rescale = 1,\n",
    "                 encoding = 'amplit',\n",
    "                 n_copies = 1, \n",
    "                 measure = 'pgm',\n",
    "                 class_weight = None, \n",
    "                 n_jobs = None, \n",
    "                 n_splits = 1):\n",
    "        self.rescale = rescale\n",
    "        self.encoding = encoding\n",
    "        self.n_copies = n_copies\n",
    "        self.measure = measure\n",
    "        self.class_weight = class_weight\n",
    "        self.n_jobs = n_jobs\n",
    "        self.n_splits = n_splits\n",
    "        \n",
    "\n",
    "    # Function for X_prime, set as global function\n",
    "    global X_prime_func\n",
    "    def X_prime_func(self, X, m):\n",
    "        # Cast X to float to ensure all following calculations below are done in float\n",
    "        # rather than integer\n",
    "        X = X.astype(float)\n",
    "        \n",
    "        # Rescale X\n",
    "        X = self.rescale*X\n",
    "        \n",
    "        # Calculate sum of squares of each row (sample) in X\n",
    "        X_sq_sum = (X**2).sum(axis = 1)\n",
    "        \n",
    "        # Calculate X' using amplitude or inverse of the standard stereographic projection\n",
    "        # encoding method\n",
    "        if self.encoding == 'amplit':\n",
    "            X_prime = normalize(np.concatenate((X, np.ones(m).reshape(-1, 1)), axis = 1))\n",
    "        elif self.encoding == 'stereo':\n",
    "            X_prime = (1 / (X_sq_sum + 1)).reshape(-1, 1) \\\n",
    "                      *(np.concatenate((2*X, (X_sq_sum - 1).reshape(-1, 1)), axis = 1))\n",
    "        else:\n",
    "            raise ValueError('encoding should be \"amplit\" or \"stereo\"')\n",
    "        return X_prime  \n",
    "    \n",
    "        \n",
    "    # Function for kronecker tensor product with broadcasting, set as global function\n",
    "    global kronecker\n",
    "    def kronecker(A, B):\n",
    "        return np.einsum('nab,ncd->nacbd', A, B).reshape(A.shape[0],\n",
    "                                                         A.shape[1]*B.shape[1],\n",
    "                                                         A.shape[2]*B.shape[2])\n",
    "    \n",
    "\n",
    "    # Set np.einsum subscripts (between unnested and nested objects) as a constant, set as global \n",
    "    # variable\n",
    "    global einsum_unnest, einsum_nest\n",
    "    einsum_unnest = 'ij,ji->'\n",
    "    einsum_nest = 'bij,ji->b'\n",
    "    \n",
    "    \n",
    "    # Function for fit\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"Perform PGM-HQC classification with the amplitude and inverse of the standard \n",
    "        stereographic projection encoding methods, with the option to rescale the dataset prior \n",
    "        to encoding.\n",
    "                \n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like, shape (n_samples, n_features)\n",
    "            The training input samples. An array of int or float.\n",
    "        y : array-like, shape (n_samples,)\n",
    "            The training input binary target values. An array of str, int or float.\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        self : object\n",
    "            Returns self.\n",
    "        \"\"\"\n",
    "        # Check data in X and y as required by scikit-learn v0.25\n",
    "        X, y = self._validate_data(X, y, reset = True)\n",
    "        \n",
    "        # Ensure target y is of non-regression type  \n",
    "        # Added as required by sklearn check_estimator\n",
    "        check_classification_targets(y)\n",
    "    \n",
    "        # Store classes and encode y into class indexes\n",
    "        self.classes_, y_class_index = np.unique(y, return_inverse = True)\n",
    "                \n",
    "        # Number of classes, set as global variable\n",
    "        global num_classes\n",
    "        num_classes = len(self.classes_)\n",
    "        \n",
    "        # Raise error when there are more than 2 classes and Helstrom measurement is specified\n",
    "        if num_classes > 2 and self.measure == 'hels':\n",
    "            raise ValueError('Helstrom measurement can be applied for binary classification only')\n",
    "        else:\n",
    "            # Number of rows and columns in X\n",
    "            m, n = X.shape[0], X.shape[1]\n",
    "            \n",
    "            # Calculate X_prime\n",
    "            X_prime = X_prime_func(self, X, m)\n",
    "        \n",
    "            # Number of columns in X', set as global variable\n",
    "            global n_prime\n",
    "            n_prime = n + 1\n",
    "        \n",
    "            # Function to calculate number of rows (samples) and Quantum Centroids for each class\n",
    "            def qcentroids_terms_func(i):\n",
    "                # Determine rows (samples) in X' belonging to each class\n",
    "                X_prime_class = X_prime[y_class_index == i]\n",
    "            \n",
    "                # Number of rows (samples) in X' belonging to either class\n",
    "                m_class = X_prime_class.shape[0]\n",
    "            \n",
    "                # Split X' belonging to each class into n_splits subsets, row-wise\n",
    "                X_prime_class_split = np.array_split(X_prime_class,\n",
    "                                                     indices_or_sections = self.n_splits,\n",
    "                                                     axis = 0)\n",
    "            \n",
    "                # Function to calculate the Quantum Centroids for each class, per subset split\n",
    "                def X_prime_class_split_func(j):\n",
    "                    # Counter for j-th split of X'\n",
    "                    X_prime_class_split_jth = X_prime_class_split[j]\n",
    "                \n",
    "                    # Number of rows (samples) in j-th split of X'\n",
    "                    m_class_split = X_prime_class_split_jth.shape[0]\n",
    "                \n",
    "                    # Encode vectors into quantum densities\n",
    "                    density_split = np.matmul(X_prime_class_split_jth.reshape(m_class_split, \n",
    "                                                                              n_prime, 1),\n",
    "                                              X_prime_class_split_jth.reshape(m_class_split, \n",
    "                                                                              1, n_prime))\n",
    "                                        \n",
    "                    # Calculate n-fold Kronecker tensor product\n",
    "                    if self.n_copies == 1:\n",
    "                        density_split = density_split\n",
    "                    else:\n",
    "                        density_split_copy = density_split\n",
    "                        for _ in range(self.n_copies - 1):\n",
    "                            density_split = kronecker(density_split, density_split_copy)\n",
    "                \n",
    "                    # Calculate sum of quantum densities\n",
    "                    density_split_sum = density_split.sum(axis = 0)\n",
    "                \n",
    "                    # Calculate Quantum Centroid for each class, per subset split\n",
    "                    # Added ZeroDivisionError as required by scikit-learn check_estimator()\n",
    "                    try:\n",
    "                        qcentroid = (1/m_class)*density_split_sum\n",
    "                    except ZeroDivisionError:\n",
    "                        qcentroid = 0\n",
    "                    return qcentroid\n",
    "                return m_class, np.sum(Parallel(n_jobs = self.n_jobs)(delayed(X_prime_class_split_func)(j) \n",
    "                                                                      for j in range(self.n_splits)), axis = 0)\n",
    "        \n",
    "            # Calculate number of rows (samples) and Quantum Centroids for each class\n",
    "            # Added dtype = object as required by NumPy v19.0 when creating ndarray from ragged nested sequences\n",
    "            qcentroids_terms = np.array(Parallel(n_jobs = self.n_jobs)(delayed(qcentroids_terms_func)(i) \n",
    "                                                                       for i in range(num_classes)), dtype = object)\n",
    "\n",
    "            # Determine Quantum Centroids\n",
    "            self.qcentroids_ = qcentroids_terms[:, 1]\n",
    "            \n",
    "            # Calculate class weight\n",
    "            if self.class_weight == None:\n",
    "                class_weight_terms = qcentroids_terms[:, 0]/m\n",
    "            elif self.class_weight == 'balanced':\n",
    "                class_weight_terms = np.array([1/num_classes for k in range(num_classes)])\n",
    "            else:\n",
    "                raise ValueError('class_weight should be None or \"balanced\"')\n",
    "                \n",
    "            # When Pretty Good Measurement is specified\n",
    "            if self.measure == 'pgm':                    \n",
    "                # Function to calculate R\n",
    "                def R_func(a):\n",
    "                    return class_weight_terms[a]*self.qcentroids_[a]\n",
    "                \n",
    "                # Calculate R\n",
    "                R = np.sum(Parallel(n_jobs = self.n_jobs)(delayed(R_func)(a) for a in range(num_classes)), axis = 0)\n",
    "                \n",
    "                # Calculate square root of the pseudo inverse of R, and remove complex part of the matrix\n",
    "                # created due to numerical precision/rounding issues in machine language\n",
    "                sqrt_pinv_R = np.real(linalg.sqrtm(np.linalg.pinv(R, hermitian = True)))\n",
    "                    \n",
    "                # Calculate kernel of R\n",
    "                ker_R = linalg.null_space(R)\n",
    "                    \n",
    "                # Calculate projector of kernel of R\n",
    "                proj_ker_R = np.dot(ker_R, ker_R.T)\n",
    "                    \n",
    "                # Function to calculate Pretty Good Measurement\n",
    "                def pgm_func(b):\n",
    "                    return np.linalg.multi_dot([sqrt_pinv_R, class_weight_terms[b]*self.qcentroids_[b], \\\n",
    "                                                sqrt_pinv_R]) + (1/num_classes)*proj_ker_R\n",
    "            \n",
    "                # Calculate Pretty Good Measurement\n",
    "                self.pgms_ = Parallel(n_jobs = self.n_jobs)(delayed(pgm_func)(b) for b in range(num_classes))\n",
    "                \n",
    "                # Function to calculate PGM bound\n",
    "                def pgm_bound_func(c):\n",
    "                    return class_weight_terms[c]*np.einsum(einsum_unnest, self.qcentroids_[c], self.pgms_[c])\n",
    "                \n",
    "                # Calculate PGM bound\n",
    "                self.pgm_bound_ = np.sum(Parallel(n_jobs = self.n_jobs)(delayed(pgm_bound_func)(c) \n",
    "                                                                        for c in range(num_classes)), axis = 0)\n",
    "            # When Helstrom measurement is specified\n",
    "            elif self.measure == 'hels':               \n",
    "                # Calculate quantum Helstrom observable\n",
    "                hels_obs = class_weight_terms[0]*self.qcentroids_[0] \\\n",
    "                           - class_weight_terms[1]*self.qcentroids_[1]\n",
    "            \n",
    "                # Number of rows/columns in density matrix, set as global variable\n",
    "                global density_nrow_ncol\n",
    "                density_nrow_ncol = hels_obs.shape[0]\n",
    "            \n",
    "                # Calculate eigenvalues w and unit eigenvectors v of the quantum Helstrom observable\n",
    "                w, v = np.linalg.eigh(hels_obs)\n",
    "                \n",
    "                # Length of w\n",
    "                len_w = len(w)\n",
    "                \n",
    "                # Initialize array eigval_class\n",
    "                eigval_class = np.empty_like(w)\n",
    "                for b in range(len_w):\n",
    "                    # Create an array of 0s and 1s to indicate positive and negative eigenvalues\n",
    "                    # respectively\n",
    "                    if w[b] > 0:\n",
    "                        eigval_class[b] = 0\n",
    "                    else:\n",
    "                        eigval_class[b] = 1\n",
    "                        \n",
    "                # Transpose matrix v containing unit eigenvectors to row-wise\n",
    "                eigvec = v.T\n",
    "                \n",
    "                # Function to calculate sum of the projectors corresponding to positive and negative\n",
    "                # eigenvalues respectively\n",
    "                def sum_proj_func(c):\n",
    "                    # Determine unit eigenvectors belonging to positive and negative eigenvalues \n",
    "                    # respectively\n",
    "                    eigvec_class = eigvec[eigval_class == c]\n",
    "                    \n",
    "                    # Split unit eigenvectors into n_splits subsets\n",
    "                    eigvec_class_split = np.array_split(eigvec_class,\n",
    "                                                        indices_or_sections = self.n_splits,\n",
    "                                                        axis = 0)\n",
    "                    \n",
    "                    # Function to calculate sum of the projectors corresponding to positive and negative\n",
    "                    # eigenvalues respectively, per subset split\n",
    "                    def eigvec_class_split_func(d):\n",
    "                        # Counter for d-th split of eigvec_class_split\n",
    "                        eigvec_class_split_dth = eigvec_class_split[d]\n",
    "                    \n",
    "                        # Number of rows (samples) in d-th split of eigvec_class_split\n",
    "                        m_eigvec_class_split = eigvec_class_split_dth.shape[0]\n",
    "                    \n",
    "                        # Calculate projectors corresponding to positive and negative eigenvalues\n",
    "                        # respectively, per subset split\n",
    "                        proj_split = np.matmul(eigvec_class_split_dth.reshape(m_eigvec_class_split,\n",
    "                                                                              density_nrow_ncol, 1),\n",
    "                                               eigvec_class_split_dth.reshape(m_eigvec_class_split,\n",
    "                                                                              1, density_nrow_ncol))\n",
    "\n",
    "                        # Calculate sum of projectors\n",
    "                        proj_split_sum = proj_split.sum(axis = 0)\n",
    "                        return proj_split_sum\n",
    "                    return np.sum(Parallel(n_jobs = self.n_jobs)(delayed(eigvec_class_split_func)(d) \n",
    "                                                                 for d in range(self.n_splits)), axis = 0)\n",
    "                \n",
    "                # Calculate sum of the projectors corresponding to positive and negative eigenvalues\n",
    "                # respectively\n",
    "                self.proj_sums_ = Parallel(n_jobs = self.n_jobs)(delayed(sum_proj_func)(c) for c in range(2))\n",
    "                \n",
    "                # Function to calculate Helstrom bound\n",
    "                def hels_bound_func(e):\n",
    "                    return class_weight_terms[e]*np.einsum(einsum_unnest, self.qcentroids_[e], self.proj_sums_[e])\n",
    "                \n",
    "                # Calculate Helstrom bound\n",
    "                self.hels_bound_ = np.sum(Parallel(n_jobs = self.n_jobs)(delayed(hels_bound_func)(e) \\\n",
    "                                                                         for e in range(2)))\n",
    "            # When Pretty Good Measurement or Helstrom measurement is misspecified\n",
    "            else:\n",
    "                raise ValueError('measure should be \"pgm\" or \"hels\"')\n",
    "        return self             \n",
    "\n",
    "    \n",
    "    # Function for predict_proba\n",
    "    def predict_proba(self, X):\n",
    "        \"\"\"Performs PMG-HQC classification on X and returns the trace of the dot product of the \n",
    "        densities and the POV (positive operator-valued) measure, i.e. the class probabilities.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like, shape (n_samples, n_features)\n",
    "            The input samples. An array of int or float.       \n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        trace_matrix : ndarray, shape (n_samples, n_classes)\n",
    "            Each column corresponds to the trace of the dot product of the densities and the POV \n",
    "            (positive operator-valued) measure for each class, i.e. each column corresponds to the \n",
    "            class probabilities. An array of float.\n",
    "        \"\"\"\n",
    "        # Check if fit had been called\n",
    "        if self.measure == 'pgm':\n",
    "            check_is_fitted(self, ['pgms_'])\n",
    "        else:\n",
    "            check_is_fitted(self, ['proj_sums_'])\n",
    "\n",
    "        # Check data in X as required by scikit-learn v0.25\n",
    "        X = self._validate_data(X, reset = False)\n",
    "        \n",
    "        # Number of rows in X\n",
    "        m = X.shape[0]\n",
    "        \n",
    "        # Calculate X_prime\n",
    "        X_prime = X_prime_func(self, X, m)\n",
    "               \n",
    "        # Function to calculate trace values for each class\n",
    "        def trace_func(i):\n",
    "            # Split X' into n_splits subsets, row-wise\n",
    "            X_prime_split = np.array_split(X_prime, \n",
    "                                           indices_or_sections = self.n_splits, \n",
    "                                           axis = 0)\n",
    "            \n",
    "            # Function to calculate trace values for each class, per subset split\n",
    "            def trace_split_func(j):\n",
    "                # Counter for j-th split X'\n",
    "                X_prime_split_jth = X_prime_split[j]\n",
    "                \n",
    "                # Number of rows (samples) in j-th split X'\n",
    "                X_prime_split_m = X_prime_split_jth.shape[0]\n",
    "                \n",
    "                # Encode vectors into quantum densities\n",
    "                density_chunk = np.matmul(X_prime_split_jth.reshape(X_prime_split_m, n_prime, 1),\n",
    "                                          X_prime_split_jth.reshape(X_prime_split_m, 1, n_prime))\n",
    "                \n",
    "                # Calculate n-fold Kronecker tensor product\n",
    "                if self.n_copies == 1:\n",
    "                    density_chunk = density_chunk\n",
    "                else:\n",
    "                    density_chunk_copy = density_chunk\n",
    "                    for _ in range(self.n_copies - 1):\n",
    "                        density_chunk = kronecker(density_chunk, density_chunk_copy)\n",
    "                        \n",
    "                # When Pretty Good Measurement is specified\n",
    "                if self.measure == 'pgm':\n",
    "                    # Calculate trace of the dot product of density of each row and Pretty Good \n",
    "                    # Measurement\n",
    "                    trace_class_split = np.einsum(einsum_nest, density_chunk, self.pgms_[i])\n",
    "                # When Helstrom measurement is specified\n",
    "                else:               \n",
    "                    # Calculate trace of the dot product of density of each row and sum of \n",
    "                    # projectors with corresponding positive and negative eigenvalues respectively\n",
    "                    trace_class_split = np.einsum(einsum_nest, density_chunk, self.proj_sums_[i])\n",
    "                return trace_class_split\n",
    "            \n",
    "            # Calculate trace values for each class, per subset split\n",
    "            trace_class = Parallel(n_jobs = self.n_jobs)(delayed(trace_split_func)(j) \n",
    "                                                         for j in range(self.n_splits))\n",
    "            return np.concatenate(trace_class, axis = 0)\n",
    "        \n",
    "        # Calculate trace values for each class\n",
    "        trace_matrix = np.transpose(Parallel(n_jobs = self.n_jobs)(delayed(trace_func)(i) \n",
    "                                                                   for i in range(num_classes)))\n",
    "        return trace_matrix\n",
    "        \n",
    "    \n",
    "    # Function for predict\n",
    "    def predict(self, X):\n",
    "        \"\"\"Performs PGM-HQC classification on X and returns the classes.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like, shape (n_samples, n_features)\n",
    "            The input samples. An array of int or float.\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        self.classes_[predict_trace_index] : ndarray, shape (n_samples,)\n",
    "            The predicted classes. An array of str, int or float.\n",
    "        \"\"\"\n",
    "        # Determine column index with the higher trace value in trace_matrix\n",
    "        # If columns have the same trace value, returns column with the smallest column index value\n",
    "        predict_trace_index = np.argmax(self.predict_proba(X), axis = 1)\n",
    "        # Returns the predicted classes\n",
    "        return self.classes_[predict_trace_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import model_selection\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.multiclass import OneVsRestClassifier, OneVsOneClassifier\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Surpress warnings (not errors) when some classes have no predicted samples \n",
    "# (for eg. OneVsRestClassifier with precision_score)\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hqc_ovr_ovo_fast(file_path, file_name, strat, n_copies_val, n_jobs_val, n_splits_val):\n",
    "    df = pd.read_csv(file_path + r'\\Datasets' + r'\\\\' + file_name + '.tsv', delimiter='\\t')\n",
    "    X = df.drop('target', axis=1).values\n",
    "    y = df['target'].values\n",
    "\n",
    "    # classes = np.unique(y)\n",
    "    # num_classes = len(classes)\n",
    "\n",
    "    # Use 80/20% train/test split and stratified sampling\n",
    "    X_train, X_test, y_train, y_test = model_selection.train_test_split(X, y, test_size=0.2, random_state=0, stratify=y)\n",
    "\n",
    "    scores_all = ['precision_weighted',\n",
    "                  'recall_weighted',\n",
    "                  'balanced_accuracy',\n",
    "                  'f1_weighted',\n",
    "                  'roc_auc_ovr_weighted',\n",
    "                  'roc_auc_ovo_weighted']\n",
    "\n",
    "    # Create rescale hyperparamter list [0.1, 0.5, 1, 1.5,...,10.0]\n",
    "    rescale_list = [0.1]\n",
    "    rescale_list_add = np.linspace(0.5, 10, 20).tolist()\n",
    "    rescale_list.extend(rescale_list_add)\n",
    "\n",
    "    param_grid = {'estimator__rescale':rescale_list,\n",
    "                  'estimator__encoding':['amplit', 'stereo'],\n",
    "                  'estimator__class_weight':[None, 'balanced']}\n",
    "\n",
    "    # n_copies_val = [1,2,3,4]\n",
    "\n",
    "    # n_jobs_val = 16\n",
    "    # n_splits_val = int(np.ceil(n_jobs_val/num_classes))\n",
    "    \n",
    "    if strat == 'ovr, ovo':\n",
    "        # Initialize array containing strings with 13 characters or less\n",
    "        best_score_std_dev_n_copies_ovr = np.empty((len(n_copies_val), len(scores_all)), dtype='<U13')\n",
    "\n",
    "        for j, nc in enumerate(n_copies_val):\n",
    "            # Initialize array containing strings with 13 characters or less\n",
    "            best_score_std_dev = np.empty(len(scores_all), dtype='<U13')\n",
    "\n",
    "            for i, sc in enumerate(scores_all):\n",
    "                # Fitting model, using GridSearchCV with default 5 folds and default stratified sampling\n",
    "                models = GridSearchCV(OneVsRestClassifier(PGMHQC_fast(n_copies=nc, measure='hels', n_jobs=n_jobs_val, \\\n",
    "                                                                      n_splits=n_splits_val)), param_grid, scoring=sc) \\\n",
    "                                                                      .fit(X_train, y_train)\n",
    "        \n",
    "                results_table = pd.DataFrame(models.cv_results_)\n",
    "                results_table.to_excel(file_path + r'\\OvO and OvR\\Output datasets\\gridsearchcv'\n",
    "                                       + r'\\\\' + file_name + '.tsv'\n",
    "                                       + '-hqc' + f'{nc}'\n",
    "                                       + '-ovr'\n",
    "                                       + '-' + sc\n",
    "                                       + '-gridsearchcv.xlsx')                      \n",
    "                              \n",
    "                # Get best score on test set\n",
    "                best_model = models.best_estimator_\n",
    "\n",
    "                if sc=='precision_weighted':\n",
    "                    y_hat = best_model.predict(X_test)\n",
    "                    best_score = metrics.precision_score(y_test, y_hat, average='weighted')\n",
    "                if sc=='recall_weighted':\n",
    "                    y_hat = best_model.predict(X_test)\n",
    "                    best_score = metrics.recall_score(y_test, y_hat, average='weighted')\n",
    "                if sc=='balanced_accuracy':\n",
    "                    y_hat = best_model.predict(X_test)\n",
    "                    best_score = metrics.balanced_accuracy_score(y_test, y_hat)\n",
    "                if sc=='f1_weighted':\n",
    "                    y_hat = best_model.predict(X_test)\n",
    "                    best_score = metrics.f1_score(y_test, y_hat, average='weighted')\n",
    "                if sc=='roc_auc_ovr_weighted':\n",
    "                    y_score = best_model.predict_proba(X_test)\n",
    "                    best_score = metrics.roc_auc_score(y_test, y_score, average='weighted', multi_class='ovr')\n",
    "                if sc=='roc_auc_ovo_weighted':\n",
    "                    y_score = best_model.predict_proba(X_test)\n",
    "                    best_score = metrics.roc_auc_score(y_test, y_score, average='weighted', multi_class='ovo')\n",
    "    \n",
    "                # Get best std. dev. on validation set\n",
    "                best_std_dev = models.cv_results_['std_test_score'][models.best_index_]\n",
    "                          \n",
    "                best_score_std_dev[i] = format(best_score, '.3f') + ' ± ' + format(best_std_dev, '.3f')\n",
    "    \n",
    "            best_score_std_dev_n_copies_ovr[j,:] = best_score_std_dev.reshape(1,-1)\n",
    "\n",
    "    \n",
    "        # Initialize array containing strings with 13 characters or less\n",
    "        best_score_std_dev_n_copies_ovo = np.empty((len(n_copies_val), len(scores_all)), dtype='<U13')\n",
    "                              \n",
    "        for k, nc in enumerate(n_copies_val):\n",
    "            # Initialize array containing strings with 13 characters or less\n",
    "            best_score_std_dev = np.empty(len(scores_all), dtype='<U13')\n",
    "                              \n",
    "            for i, sc in enumerate(scores_all):\n",
    "                # OneVsOneClassifier does not have roc_auc\n",
    "                if sc not in ['roc_auc_ovr_weighted', 'roc_auc_ovo_weighted']:\n",
    "                    # Fitting model, using GridSearchCV with default 5 folds and default stratified sampling\n",
    "                    models = GridSearchCV(OneVsOneClassifier(PGMHQC_fast(n_copies=nc, measure='hels', n_jobs=n_jobs_val, \\\n",
    "                                                                         n_splits=n_splits_val)), param_grid, scoring=sc) \\\n",
    "                                                                         .fit(X_train, y_train)\n",
    "        \n",
    "                    results_table = pd.DataFrame(models.cv_results_)\n",
    "                    results_table.to_excel(file_path + r'\\OvO and OvR\\Output datasets\\gridsearchcv'\n",
    "                                           + r'\\\\' + file_name + '.tsv'\n",
    "                                           + '-hqc' + f'{nc}'\n",
    "                                           + '-ovo'\n",
    "                                           + '-' + sc\n",
    "                                           + '-gridsearchcv.xlsx')\n",
    "        \n",
    "                    # Get best score on test set\n",
    "                    best_model = models.best_estimator_\n",
    "        \n",
    "                    if sc=='precision_weighted':\n",
    "                        y_hat = best_model.predict(X_test)\n",
    "                        best_score = metrics.precision_score(y_test, y_hat, average='weighted')\n",
    "                    if sc=='recall_weighted':\n",
    "                        y_hat = best_model.predict(X_test)\n",
    "                        best_score = metrics.recall_score(y_test, y_hat, average='weighted')\n",
    "                    if sc=='balanced_accuracy':\n",
    "                        y_hat = best_model.predict(X_test)\n",
    "                        best_score = metrics.balanced_accuracy_score(y_test, y_hat)\n",
    "                    if sc=='f1_weighted':\n",
    "                        y_hat = best_model.predict(X_test)\n",
    "                        best_score = metrics.f1_score(y_test, y_hat, average='weighted')\n",
    "                    # if sc=='roc_auc_ovr_weighted':\n",
    "                        # y_score = best_model.predict_proba(X_test)   \n",
    "                        # best_score = metrics.roc_auc_score(y_test, y_score, average='weighted', multi_class='ovr')\n",
    "                    # if sc=='roc_auc_ovo_weighted':\n",
    "                        # y_score = best_model.predict_proba(X_test)\n",
    "                        # best_score = metrics.roc_auc_score(y_test, y_score, average='weighted', multi_class='ovo')\n",
    "            \n",
    "                    # Get best std. dev. on validation set\n",
    "                    best_std_dev = models.cv_results_['std_test_score'][models.best_index_]\n",
    "        \n",
    "                    best_score_std_dev[i] = format(best_score, '.3f') + ' ± ' + format(best_std_dev, '.3f')\n",
    "                else:\n",
    "                    best_score_std_dev[i] = '-'\n",
    "        \n",
    "            best_score_std_dev_n_copies_ovo[k,:] = best_score_std_dev.reshape(1,-1)\n",
    "    \n",
    "        best_score_std_dev_n_copies_ovr_ovo = np.concatenate([best_score_std_dev_n_copies_ovr, \n",
    "                                                              best_score_std_dev_n_copies_ovo], axis=0)\n",
    "\n",
    "        index_names = [f'Helstrom Quantum Centroid {s} (OvR)' for s in n_copies_val]\n",
    "        index_names_ovo = [f'Helstrom Quantum Centroid {t} (OvO)' for t in n_copies_val]\n",
    "        index_names.extend(index_names_ovo)\n",
    "\n",
    "        columns_names = ['Precision (Weighted)',\n",
    "                         'Recall (Weighted)',\n",
    "                         'Balanced Accuracy',\n",
    "                         'F-measure (Weighted)',\n",
    "                         'AUROC - OvR (Weighted)',\n",
    "                         'AUROC - OvO (Weighted)']\n",
    "\n",
    "        df = pd.DataFrame(best_score_std_dev_n_copies_ovr_ovo, index=index_names, columns=columns_names)\n",
    "        df.to_excel(file_path + r'\\OvO and OvR\\Output datasets' \n",
    "                    + r'\\\\' + file_name + '.tsv' + '-' + strat + '-' + str(n_copies_val) + '-results.xlsx')\n",
    "\n",
    "    if strat == 'ovr':\n",
    "        # Initialize array containing strings with 13 characters or less\n",
    "        best_score_std_dev_n_copies_ovr = np.empty((len(n_copies_val), len(scores_all)), dtype='<U13')\n",
    "        \n",
    "        for j, nc in enumerate(n_copies_val):\n",
    "            # Initialize array containing strings with 13 characters or less\n",
    "            best_score_std_dev = np.empty(len(scores_all), dtype='<U13')\n",
    "            \n",
    "            for i, sc in enumerate(scores_all):\n",
    "                # Fitting model, using GridSearchCV with default 5 folds and default stratified sampling\n",
    "                models = GridSearchCV(OneVsRestClassifier(PGMHQC_fast(n_copies=nc, measure='hels', n_jobs=n_jobs_val, \\\n",
    "                                                                      n_splits=n_splits_val)), param_grid, scoring=sc) \\\n",
    "                                                                      .fit(X_train, y_train)\n",
    "                \n",
    "                results_table = pd.DataFrame(models.cv_results_)\n",
    "                results_table.to_excel(file_path + r'\\OvO and OvR\\Output datasets\\gridsearchcv'\n",
    "                                       + r'\\\\' + file_name + '.tsv'\n",
    "                                       + '-hqc' + f'{nc}'\n",
    "                                       + '-ovr'\n",
    "                                       + '-' + sc\n",
    "                                       + '-gridsearchcv.xlsx')\n",
    "                \n",
    "                # Get best score on test set\n",
    "                best_model = models.best_estimator_\n",
    "                \n",
    "                if sc=='precision_weighted':\n",
    "                    y_hat = best_model.predict(X_test)\n",
    "                    best_score = metrics.precision_score(y_test, y_hat, average='weighted')\n",
    "                if sc=='recall_weighted':\n",
    "                    y_hat = best_model.predict(X_test)\n",
    "                    best_score = metrics.recall_score(y_test, y_hat, average='weighted')\n",
    "                if sc=='balanced_accuracy':\n",
    "                    y_hat = best_model.predict(X_test)\n",
    "                    best_score = metrics.balanced_accuracy_score(y_test, y_hat)\n",
    "                if sc=='f1_weighted':\n",
    "                    y_hat = best_model.predict(X_test)\n",
    "                    best_score = metrics.f1_score(y_test, y_hat, average='weighted')\n",
    "                if sc=='roc_auc_ovr_weighted':\n",
    "                    y_score = best_model.predict_proba(X_test)\n",
    "                    best_score = metrics.roc_auc_score(y_test, y_score, average='weighted', multi_class='ovr')\n",
    "                if sc=='roc_auc_ovo_weighted':\n",
    "                    y_score = best_model.predict_proba(X_test)\n",
    "                    best_score = metrics.roc_auc_score(y_test, y_score, average='weighted', multi_class='ovo')\n",
    "                    \n",
    "                # Get best std. dev. on validation set\n",
    "                best_std_dev = models.cv_results_['std_test_score'][models.best_index_]\n",
    "                          \n",
    "                best_score_std_dev[i] = format(best_score, '.3f') + ' ± ' + format(best_std_dev, '.3f')\n",
    "                \n",
    "            best_score_std_dev_n_copies_ovr[j,:] = best_score_std_dev.reshape(1,-1)\n",
    "            \n",
    "        index_names = [f'Helstrom Quantum Centroid {s} (OvR)' for s in n_copies_val]\n",
    "        \n",
    "        columns_names = ['Precision (Weighted)',\n",
    "                         'Recall (Weighted)',\n",
    "                         'Balanced Accuracy',\n",
    "                         'F-measure (Weighted)',\n",
    "                         'AUROC - OvR (Weighted)',\n",
    "                         'AUROC - OvO (Weighted)']\n",
    "        \n",
    "        df = pd.DataFrame(best_score_std_dev_n_copies_ovr, index=index_names, columns=columns_names)\n",
    "        df.to_excel(file_path + r'\\OvO and OvR\\Output datasets' \n",
    "                    + r'\\\\' + file_name + '.tsv' + '-' + strat + '-' + str(n_copies_val) + '-results.xlsx')\n",
    "        \n",
    "    if strat == 'ovo':\n",
    "        # Initialize array containing strings with 13 characters or less\n",
    "        best_score_std_dev_n_copies_ovo = np.empty((len(n_copies_val), len(scores_all)), dtype='<U13')\n",
    "        \n",
    "        for k, nc in enumerate(n_copies_val):\n",
    "            # Initialize array containing strings with 13 characters or less\n",
    "            best_score_std_dev = np.empty(len(scores_all), dtype='<U13')\n",
    "            \n",
    "            for i, sc in enumerate(scores_all):\n",
    "                # OneVsOneClassifier does not have roc_auc\n",
    "                if sc not in ['roc_auc_ovr_weighted', 'roc_auc_ovo_weighted']:\n",
    "                    # Fitting model, using GridSearchCV with default 5 folds and default stratified sampling\n",
    "                    models = GridSearchCV(OneVsOneClassifier(PGMHQC_fast(n_copies=nc, measure='hels', n_jobs=n_jobs_val, \\\n",
    "                                                                         n_splits=n_splits_val)), param_grid, scoring=sc) \\\n",
    "                                                                         .fit(X_train, y_train)  \n",
    "                    \n",
    "                    results_table = pd.DataFrame(models.cv_results_)\n",
    "                    results_table.to_excel(file_path + r'\\OvO and OvR\\Output datasets\\gridsearchcv'\n",
    "                                           + r'\\\\' + file_name + '.tsv'\n",
    "                                           + '-hqc' + f'{nc}'\n",
    "                                           + '-ovo'\n",
    "                                           + '-' + sc\n",
    "                                           + '-gridsearchcv.xlsx')\n",
    "        \n",
    "                    # Get best score on test set\n",
    "                    best_model = models.best_estimator_\n",
    "                \n",
    "                    if sc=='precision_weighted':\n",
    "                        y_hat = best_model.predict(X_test)\n",
    "                        best_score = metrics.precision_score(y_test, y_hat, average='weighted')\n",
    "                    if sc=='recall_weighted':\n",
    "                        y_hat = best_model.predict(X_test)\n",
    "                        best_score = metrics.recall_score(y_test, y_hat, average='weighted')\n",
    "                    if sc=='balanced_accuracy':\n",
    "                        y_hat = best_model.predict(X_test)\n",
    "                        best_score = metrics.balanced_accuracy_score(y_test, y_hat)\n",
    "                    if sc=='f1_weighted':\n",
    "                        y_hat = best_model.predict(X_test)\n",
    "                        best_score = metrics.f1_score(y_test, y_hat, average='weighted')\n",
    "                    # if sc=='roc_auc_ovr_weighted':\n",
    "                        # y_score = best_model.predict_proba(X_test)   \n",
    "                        # best_score = metrics.roc_auc_score(y_test, y_score, average='weighted', multi_class='ovr')\n",
    "                    # if sc=='roc_auc_ovo_weighted':\n",
    "                        # y_score = best_model.predict_proba(X_test)\n",
    "                        # best_score = metrics.roc_auc_score(y_test, y_score, average='weighted', multi_class='ovo')\n",
    "                        \n",
    "                    # Get best std. dev. on validation set\n",
    "                    best_std_dev = models.cv_results_['std_test_score'][models.best_index_]\n",
    "        \n",
    "                    best_score_std_dev[i] = format(best_score, '.3f') + ' ± ' + format(best_std_dev, '.3f')\n",
    "                else:\n",
    "                    best_score_std_dev[i] = '-'\n",
    "                    \n",
    "            best_score_std_dev_n_copies_ovo[k,:] = best_score_std_dev.reshape(1,-1)\n",
    "    \n",
    "        index_names_ovo = [f'Helstrom Quantum Centroid {t} (OvO)' for t in n_copies_val]\n",
    "        \n",
    "        columns_names = ['Precision (Weighted)',\n",
    "                         'Recall (Weighted)',\n",
    "                         'Balanced Accuracy',\n",
    "                         'F-measure (Weighted)',\n",
    "                         'AUROC - OvR (Weighted)',\n",
    "                         'AUROC - OvO (Weighted)']\n",
    "        \n",
    "        df = pd.DataFrame(best_score_std_dev_n_copies_ovo, index=index_names_ovo, columns=columns_names)\n",
    "        df.to_excel(file_path + r'\\OvO and OvR\\Output datasets'\n",
    "                    + r'\\\\' + file_name + '.tsv' + '-' + strat + '-' + str(n_copies_val) + '-results.xlsx')\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# n_splits_val = int(np.ceil(n_jobs_val/num_classes)) = int(np.ceil(16/2)) = 8\n",
    "# num_classes = 2 since using measure = 'hels'\n",
    "hqc_ovr_ovo_fast(file_path = r'C:\\Users\\server\\Desktop\\LEO\\PGM\\PGM paper', \n",
    "                 file_name = 'krkopt',\n",
    "                 strat = 'ovo',\n",
    "                 n_copies_val = [4], \n",
    "                 n_jobs_val = 48, \n",
    "                 n_splits_val = 24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
