{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "PGM-HQC-gpu-dtype.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "2SjbUqx76_TV"
      },
      "source": [
        "### This is a GPU implementation for the HQC classifier using Scikit-learn's methods, but with PyTorch as the backend. ###"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s9I9rWGT6_Td"
      },
      "source": [
        "# I have implemented the code below in such a way that you would only need to input X and y as numpy arrays and the\n",
        "# output y_hat would also be a numpy array (rather than PyTorch tensors). This would make it easier to use the package\n",
        "# below with minimal knowledge of PyTorch tensors.\n",
        "\n",
        "# Take note of the parameter n_splits, where the implementation of n_splits now is different to the one in the CPU case.\n",
        "# Please read the description of n_splits below."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PuJShFTnWgjb"
      },
      "source": [
        "import numpy as np\n",
        "from sklearn.base import BaseEstimator, ClassifierMixin\n",
        "from sklearn.utils.validation import check_is_fitted\n",
        "from sklearn.utils.multiclass import check_classification_targets\n",
        "import torch\n",
        "from torch.nn.functional import normalize\n",
        "from scipy import linalg\n",
        "\n",
        "class PGMHQC_gpu_dtype(BaseEstimator, ClassifierMixin):\n",
        "    \"\"\"The Pretty Good Measurement (PGM) - Helstrom Quantum Centroid (HQC) classifier is a \n",
        "    quantum-inspired supervised classification approach for data with multiple classes.\n",
        "                         \n",
        "    Parameters\n",
        "    ----------\n",
        "    rescale : int or float, default = 1\n",
        "        The dataset rescaling factor. A parameter used for rescaling the dataset. \n",
        "    encoding : str, default = 'amplit'\n",
        "        The encoding method used to encode vectors into quantum densities. Possible values:\n",
        "        'amplit', 'stereo'. 'amplit' means using the amplitude encoding method. 'stereo' means \n",
        "        using the inverse of the standard stereographic projection encoding method. Default set \n",
        "        to 'amplit'.\n",
        "    n_copies : int, default = 1\n",
        "        The number of copies to take for each quantum density. This is equivalent to taking \n",
        "        the n-fold Kronecker tensor product for each quantum density.\n",
        "    measure : str, default = 'pgm'\n",
        "        The measurement used to distinguish between quantum states. Possible values: 'pgm', \n",
        "        'hels'. The value 'pgm' stands for \"Pretty Good Measurement\", 'hels' stands for \n",
        "        \"Helstrom measurement\" (applicable only for binary classification). Default set to \n",
        "        'pgm'. \n",
        "    class_wgt : str, default = None        \n",
        "        Applicable only when \"Helstrom measurement\" is selected. This is the class weights \n",
        "        assigned to the Quantum Helstrom observable terms. Possible values: 'equi', 'weighted', \n",
        "        None. 'equi' means assigning equal weights of 1/2 (equiprobable) to the two terms in \n",
        "        the Quantum Helstrom observable. 'weighted' means assigning weights equal to the \n",
        "        proportion of the number of rows in each class to the two terms in the Quantum Helstrom \n",
        "        observable. When using \"Pretty Good Measurement\", specify class_wgt = None. Default set \n",
        "        to None.        \n",
        "    n_splits : int, default = 1\n",
        "        The number of subset splits performed on the input dataset row-wise and on the number \n",
        "        of eigenvalues/eigenvectors of the Quantum Helstrom observable for optimal speed \n",
        "        performance. If 1 is given, no splits are performed. For optimal speed, recommend \n",
        "        using small values as close to 1 as possible. If memory blow-out occurs, increase \n",
        "        n_splits.\n",
        "    dtype : torch.float32 or torch.float64, default = torch.float64\n",
        "        The float datatype used for the elements in the Pytorch tensor dataset. Datatype has to\n",
        "        be of float to ensure calculations are done in float rather than integer. To achieve\n",
        "        higher n_copies without memory blow-out issues, reduce float precision, which may or may   \n",
        "        not affect accuracy in a significant way.\n",
        "    \n",
        "    Attributes\n",
        "    ----------\n",
        "    classes_ : ndarray, shape (n_classes,)\n",
        "        Sorted binary classes. Stored in CPU.\n",
        "    qcentroids_ : tensor, size (n_classes, (n_features + 1)**n_copies, (n_features + 1)**n_copies)\n",
        "        Quantum Centroids for each class. Stored in GPU.\n",
        "    pgms_ : tensor, shape (n_classes, (n_features + 1)**n_copies, (n_features + 1)**n_copies)\n",
        "        Pretty Good Measurement. Stored in GPU.\n",
        "    hels_obs_ : tensor, size ((n_features + 1)**n_copies, (n_features + 1)**n_copies)\n",
        "        Quantum Helstrom observable. Stored in GPU.\n",
        "    proj_sums_ : tensor, size (n_classes, (n_features + 1)**n_copies, (n_features + 1)**n_copies)\n",
        "        Sum of the projectors of the Quantum Helstrom observable's eigenvectors, which has\n",
        "        corresponding positive and negative eigenvalues respectively. Stored in GPU.\n",
        "    hels_bound_ : float\n",
        "        Helstrom bound is the upper bound of the probability that one can correctly \n",
        "        discriminate whether a quantum density is of which of the two binary quantum density \n",
        "        pattern. Stored in CPU.         \n",
        "    \"\"\"   \n",
        "    # Initialize model hyperparameters\n",
        "    def __init__(self, \n",
        "                 rescale = 1,\n",
        "                 encoding = 'amplit',\n",
        "                 n_copies = 1,  \n",
        "                 measure = 'pgm',\n",
        "                 class_wgt = None, \n",
        "                 n_splits = 1,\n",
        "                 dtype = torch.float64):\n",
        "        self.rescale = rescale\n",
        "        self.encoding = encoding\n",
        "        self.n_copies = n_copies\n",
        "        self.measure = measure\n",
        "        self.class_wgt = class_wgt\n",
        "        self.n_splits = n_splits\n",
        "        self.dtype = dtype\n",
        "        \n",
        "        # Raise error if dtype is not torch.float32 or torch.float64\n",
        "        if self.dtype not in [torch.float32, torch.float64]:\n",
        "            raise ValueError('dtype should be torch.float32 or torch.float64 only')\n",
        "        \n",
        "    \n",
        "    # Function for kronecker tensor product for PyTorch tensors, set as global function\n",
        "    global kronecker\n",
        "    def kronecker(A, B):\n",
        "        return torch.einsum('nab,ncd->nacbd', A, B).view(A.size(0), \n",
        "                                                         A.size(1)*B.size(1), \n",
        "                                                         A.size(2)*B.size(2))\n",
        "    \n",
        "    \n",
        "    # Function for fit\n",
        "    def fit(self, X, y):\n",
        "        \"\"\"Perform PGM-HQC classification with the amplitude and inverse of the standard \n",
        "        stereographic projection encoding methods, with the option to rescale the dataset prior \n",
        "        to encoding.\n",
        "                \n",
        "        Parameters\n",
        "        ----------\n",
        "        X : array-like, shape (n_samples, n_features)\n",
        "            The training input samples. An array of int or float.\n",
        "        y : array-like, shape (n_samples,)\n",
        "            The training input binary target values. An array of str, int or float.\n",
        "            \n",
        "        Returns\n",
        "        -------\n",
        "        self : object\n",
        "            Returns self.\n",
        "        \"\"\"\n",
        "        # Check data in X and y as required by scikit-learn v0.25\n",
        "        X, y = self._validate_data(X, y, reset = True)\n",
        "        \n",
        "        # Ensure target array y is of non-regression type  \n",
        "        # Added as required by sklearn check_estimator\n",
        "        check_classification_targets(y)\n",
        "            \n",
        "        # Store classes and encode y into class indexes\n",
        "        self.classes_, y_class_index = np.unique(y, return_inverse = True)\n",
        "        \n",
        "        # Number of classes, set as global variable\n",
        "        global num_classes\n",
        "        num_classes = len(self.classes_)\n",
        "        \n",
        "        # Raise error when there are more than 2 classes and Helstrom measurement is specified\n",
        "        if num_classes > 2 and self.measure == 'hels':\n",
        "            raise ValueError('Helstrom measurement can be applied for binary classification only')\n",
        "        else:                    \n",
        "            # Cast array X into a floating point tensor to ensure all following calculations below  \n",
        "            # are done in float rather than integer, and send tensor X from CPU to GPU\n",
        "            X = torch.tensor(X, dtype = self.dtype).cuda()\n",
        "        \n",
        "            # Rescale X\n",
        "            X = self.rescale*X\n",
        "        \n",
        "            # Calculate sum of squares of each row (sample) in X\n",
        "            X_sq_sum = (X**2).sum(dim = 1)\n",
        "        \n",
        "            # Number of rows in X\n",
        "            m = X.shape[0]\n",
        "        \n",
        "            # Number of columns in X\n",
        "            n = X.shape[1]\n",
        "        \n",
        "            # Calculate X' using amplitude or inverse of the standard stereographic projection \n",
        "            # encoding method\n",
        "            if self.encoding == 'amplit':\n",
        "                X_prime = normalize(torch.cat([X, torch.ones(m, dtype = self.dtype) \\\n",
        "                                               .reshape(-1, 1).cuda()], dim = 1), p = 2, dim = 1)\n",
        "            elif self.encoding == 'stereo':\n",
        "                X_prime = (1/(X_sq_sum + 1)).reshape(-1, 1) \\\n",
        "                          *(torch.cat((2*X, (X_sq_sum - 1).reshape(-1, 1)), dim = 1))\n",
        "            else:\n",
        "                raise ValueError('encoding should be \"amplit\" or \"stereo\"')\n",
        "        \n",
        "            # Number of columns in X', set as global variable\n",
        "            global n_prime\n",
        "            n_prime = n + 1\n",
        "        \n",
        "            # Function to calculate number of rows (samples) and Quantum Centroids for each class \n",
        "            def qcentroids_terms_func(i):\n",
        "                # Cast array y_class_index into a tensor and send from CPU to GPU\n",
        "                # Determine rows (samples) in X' belonging to either class\n",
        "                X_prime_class = X_prime[torch.CharTensor(y_class_index).cuda() == i]\n",
        "                                    \n",
        "                # Split X' belonging to either class into n_splits subsets, row-wise\n",
        "                # Send tensors from GPU to CPU and cast tensors into arrays, use np.array_split()\n",
        "                # because the equivalent torch.chunk() doesn't behave similarly to np.array_split()\n",
        "                X_prime_class_split_arr = np.array_split(X_prime_class.cpu().numpy(),\n",
        "                                                         indices_or_sections = self.n_splits,\n",
        "                                                         axis = 0)\n",
        "            \n",
        "                # Cast arrays back to tensors and send back from CPU to GPU\n",
        "                X_prime_class_split = [torch.tensor(a, dtype = self.dtype).cuda() \n",
        "                                       for a in X_prime_class_split_arr]\n",
        "            \n",
        "                # Function to calculate sum of quantum densities belonging to each class, \n",
        "                # per subset split\n",
        "                def X_prime_class_split_func(j):\n",
        "                    # Counter for j-th split of X'\n",
        "                    X_prime_class_split_jth = X_prime_class_split[j]\n",
        "                \n",
        "                    # Number of rows (samples) in j-th split of X'\n",
        "                    m_class_split = X_prime_class_split_jth.shape[0]\n",
        "                \n",
        "                    # Encode vectors into quantum densities\n",
        "                    density_chunk = torch.matmul(X_prime_class_split_jth.view(m_class_split, \n",
        "                                                                              n_prime, 1),\n",
        "                                                 X_prime_class_split_jth.view(m_class_split, \n",
        "                                                                              1, n_prime))\n",
        "                \n",
        "                    # Calculate n-fold Kronecker tensor product\n",
        "                    if self.n_copies == 1:\n",
        "                        density_chunk = density_chunk\n",
        "                    else:\n",
        "                        density_chunk_copy = density_chunk\n",
        "                        for b in range(self.n_copies - 1):\n",
        "                            density_chunk = kronecker(density_chunk, density_chunk_copy)\n",
        "                    \n",
        "                    # Calculate sum of quantum densities\n",
        "                    density_chunk_sum = density_chunk.sum(dim = 0)\n",
        "                    return density_chunk_sum\n",
        "\n",
        "                # Number of rows/columns in density matrix, set as global variable\n",
        "                global density_nrow_ncol\n",
        "                density_nrow_ncol = n_prime**self.n_copies\n",
        "            \n",
        "                # Initialize tensor density_class_sum\n",
        "                density_class_sum = torch.zeros([density_nrow_ncol, density_nrow_ncol], \n",
        "                                                dtype = self.dtype).cuda()\n",
        "                for j in range(self.n_splits):\n",
        "                    # Calculate sum of quantum densities belonging to each class\n",
        "                    density_class_sum = density_class_sum + X_prime_class_split_func(j)\n",
        "            \n",
        "                # Number of rows (samples) in X' belonging to each class\n",
        "                m_class = X_prime_class.shape[0]\n",
        "            \n",
        "                # Function to calculate Quantum Centroid belonging to each class\n",
        "                def qcentroid_func():\n",
        "                    # Calculate Quantum Centroid belonging to each class\n",
        "                    # Added ZeroDivisionError as required by sklearn check_estimator\n",
        "                    try:\n",
        "                        qcentroid = (1/m_class)*density_class_sum\n",
        "                    except ZeroDivisionError:\n",
        "                        qcentroid = 0 \n",
        "                    return qcentroid\n",
        "            \n",
        "                # Calculate Quantum Centroid belonging to each class\n",
        "                qcentroid_class = qcentroid_func()\n",
        "                return m_class, qcentroid_class\n",
        "            \n",
        "            # Calculate number of rows (samples) and Quantum Centroids for each class \n",
        "            qcentroids_terms = [qcentroids_terms_func(i) for i in range(num_classes)]\n",
        "            \n",
        "            # Determine Quantum Centroids\n",
        "            self.qcentroids_ = torch.stack([qcentroids_terms[z][1] for z in range(num_classes)], dim = 0)\n",
        "            \n",
        "            # When Pretty Good Measurement is specified\n",
        "            if self.measure == 'pgm':\n",
        "                if self.class_wgt == None:\n",
        "                    # Calculate R\n",
        "                    R = self.qcentroids_.sum(dim = 0)\n",
        "                    \n",
        "                    # Calculate square root of pseudo inverse of R\n",
        "                    # Change datatype of R to float64 as the square root of a matrix calculation is highly \n",
        "                    # senstive to numerical precision/rounding\n",
        "                    # Calculate pseudo inverse of R, send tensor from GPU to CPU and cast into an array\n",
        "                    # Use scipy.linalg.sqrtm() to calculate square root of the pseudo inverse of R because \n",
        "                    # there is no equivalent function in PyTorch which behaves numerically similarly \n",
        "                    # Remove complex part of the matrix created due to numerical precision/rounding issues\n",
        "                    # in machine language\n",
        "                    # Cast array back into a tensor and send back from CPU to GPU\n",
        "                    sqrt_pinv_R = torch.tensor(np.real(linalg.sqrtm(torch.pinverse(torch.as_tensor(R, dtype = \\\n",
        "                                              torch.float64)).cpu().numpy())), dtype = self.dtype).cuda()\n",
        "                    \n",
        "                    # Calculate kernel of R\n",
        "                    # Change datatype of R to float64 as the kernel of a matrix calculation is highly\n",
        "                    # senstive to numerical precision/rounding\n",
        "                    # Send tensor from GPU to CPU and cast into an array, use scipy.linalg.null_space()\n",
        "                    # to calculate kernel because there is no equivalent function in PyTorch which\n",
        "                    # behaves numerically similarly\n",
        "                    # Cast array back into a tensor and send back from CPU to GPU\n",
        "                    ker_R = torch.tensor(linalg.null_space(torch.as_tensor(R, dtype = torch.float64).cpu() \\\n",
        "                                         .numpy()), dtype = self.dtype).cuda()\n",
        "                    \n",
        "                    # Calculate projector of kernel of R\n",
        "                    proj_ker_R = torch.matmul(ker_R, ker_R.T)\n",
        "                    \n",
        "                    # Function to calculate Pretty Good Measurement\n",
        "                    def pgm_func(a):\n",
        "                        return torch.matmul(torch.matmul(sqrt_pinv_R, self.qcentroids_[a]), sqrt_pinv_R) \\\n",
        "                               + (1/num_classes)*proj_ker_R\n",
        "                                               \n",
        "                    # Calculate Pretty Good Measurement\n",
        "                    self.pgms_ = torch.stack([pgm_func(a) for a in range(num_classes)], dim = 0)\n",
        "                else:\n",
        "                    raise ValueError('when using \"pgm\" measure, class_wgt should be None')\n",
        "            # When Helstrom measurement is specified\n",
        "            elif self.measure == 'hels':\n",
        "                # Calculate quantum Helstrom observable\n",
        "                if self.class_wgt == 'equi':\n",
        "                    self.hels_obs_ = 0.5*(self.qcentroids_[0] - self.qcentroids_[1])\n",
        "                elif self.class_wgt == 'weighted':\n",
        "                    self.hels_obs_ = (qcentroids_terms[0][0]/m)*self.qcentroids_[0] \\\n",
        "                                     - (qcentroids_terms[1][0]/m)*self.qcentroids_[1]\n",
        "                else:\n",
        "                    raise ValueError('when using \"hels\" measure, class_wgt should be \"equi\" or \"weighted\"')\n",
        "                \n",
        "                # Number of rows/columns in density matrix, set as global variable\n",
        "                global density_nrow_ncol\n",
        "                density_nrow_ncol = self.hels_obs_.shape[0]\n",
        "                \n",
        "                # Calculate eigenvalues w and unit eigenvectors v of the quantum Helstrom observable\n",
        "                w, v = torch.symeig(self.hels_obs_, eigenvectors = True)\n",
        "                \n",
        "                # Length of w\n",
        "                len_w = len(w)\n",
        "                \n",
        "                # Initialize tensor eigval_class\n",
        "                eigval_class = torch.empty_like(w, dtype = self.dtype).cuda()\n",
        "                for d in range(len_w):\n",
        "                    # Create a tensor of 0s and 1s to indicate positive and negative eigenvalues\n",
        "                    # respectively\n",
        "                    if w[d] > 0:\n",
        "                        eigval_class[d] = 0\n",
        "                    else:\n",
        "                        eigval_class[d] = 1\n",
        "                        \n",
        "                # Transpose matrix v containing eigenvectors to row-wise\n",
        "                eigvec = v.T\n",
        "                \n",
        "                # Function to calculate sum of the projectors corresponding to positive and negative\n",
        "                # eigenvalues respectively\n",
        "                def sum_proj_func(e):\n",
        "                    # Split eigenvectors belonging to positive or negative eigenvalues into n_splits subsets\n",
        "                    # Send tensors from GPU to CPU and cast tensors into arrays, use np.array_split()\n",
        "                    # because the equivalent torch.chunk() doesn't behave similarly to np.array_split()\n",
        "                    eigvec_class_split_arr_full = np.array_split(eigvec.cpu().numpy()[eigval_class.cpu() == e],\n",
        "                                                                 indices_or_sections = self.n_splits,\n",
        "                                                                 axis = 0)\n",
        "                    \n",
        "                    # Remove empty rows in eigvec_class_split_arr_full\n",
        "                    eigvec_class_split_arr = [f for f in eigvec_class_split_arr_full if f.shape[0] > 0]\n",
        "                    \n",
        "                    # Cast arrays back to tensors and send back from CPU to GPU\n",
        "                    eigvec_class_split = [torch.tensor(g, dtype = self.dtype).cuda()\n",
        "                                          for g in eigvec_class_split_arr]\n",
        "                    \n",
        "                    # Function to calculate sum of the projectors corresponding to positive and negative\n",
        "                    # eigenvalues respectively, per subset split\n",
        "                    def eigvec_class_split_func(h):\n",
        "                        # Counter for h-th split of eigvec\n",
        "                        eigvec_class_split_hth = eigvec_class_split[h]\n",
        "                        \n",
        "                        # Number of rows (samples) in h-th split of eigvec\n",
        "                        m_eigvec_class_split = eigvec_class_split_hth.shape[0]\n",
        "                        \n",
        "                        # Calculate projectors corresponding to positive and negative eigenvalues\n",
        "                        # respectively, per subset split\n",
        "                        proj_split = torch.matmul(eigvec_class_split_hth.view(m_eigvec_class_split,\n",
        "                                                                              density_nrow_ncol, 1),\n",
        "                                                  eigvec_class_split_hth.view(m_eigvec_class_split,\n",
        "                                                                              1, density_nrow_ncol))\n",
        "                        \n",
        "                        # Calculate sum of projectors\n",
        "                        proj_split_sum = proj_split.sum(dim = 0)\n",
        "                        return proj_split_sum\n",
        "                    \n",
        "                    # Determine length of eigvec_class_split_arr\n",
        "                    eigvec_class_split_arr_len = len(eigvec_class_split_arr)\n",
        "                    \n",
        "                    # Initialize tensor proj_class_sum\n",
        "                    proj_class_sum = torch.zeros([density_nrow_ncol, density_nrow_ncol],\n",
        "                                                 dtype = self.dtype).cuda()\n",
        "                    for h in range(eigvec_class_split_arr_len):\n",
        "                        # Calculate sum of the projectors corresponding to positive and negative eigenvalues\n",
        "                        # respectively\n",
        "                        proj_class_sum = proj_class_sum + eigvec_class_split_func(h)\n",
        "                    return proj_class_sum\n",
        "                \n",
        "                # Calculate sum of the projectors corresponding to positive and negative eigenvalues\n",
        "                # respectively\n",
        "                self.proj_sums_ = torch.stack([sum_proj_func(0), sum_proj_func(1)], dim = 0)\n",
        "                \n",
        "                # Calculate Helstrom bound\n",
        "                self.hels_bound_ = (qcentroids_terms[0][0]/m)*torch.einsum('ij,ji->', self.qcentroids_[0],\n",
        "                                                                           self.proj_sums_[0]).item() \\\n",
        "                                   + (qcentroids_terms[1][0]/m)*torch.einsum('ij,ji->', self.qcentroids_[1],\n",
        "                                                                             self.proj_sums_[1]).item()\n",
        "            # When Pretty Good Measurement or Helstrom measurement is misspecified\n",
        "            else:\n",
        "                raise ValueError('measure should be \"pgm\" or \"hels\"')\n",
        "        return self\n",
        "\n",
        "           \n",
        "    # Function for predict_proba\n",
        "    def predict_proba(self, X):\n",
        "        \"\"\"Performs PMG-HQC classification on X and returns the trace of the dot product of the \n",
        "        densities and the POV (positive operator-valued) measure, i.e. the class probabilities.\n",
        "        \n",
        "        Parameters\n",
        "        ----------\n",
        "        X : array-like, shape (n_samples, n_features)\n",
        "            The input samples. An array of int or float.       \n",
        "            \n",
        "        Returns\n",
        "        -------\n",
        "        trace_matrix : tensor, size (n_samples, n_classes)\n",
        "            Each column corresponds to the trace of the dot product of the densities and the POV \n",
        "            (positive operator-valued) measure for each class, i.e. each column corresponds to the \n",
        "            class probabilities. A tensor of float. Stored in GPU.\n",
        "        \"\"\"\n",
        "        # Send tensors self.pgms_ and self.proj_sums_ from GPU to CPU and cast into an array, and\n",
        "        # check if fit had been called\n",
        "        if self.measure == 'pgm':\n",
        "            self.pgms_arr_ = self.pgms_.cpu().numpy()\n",
        "            check_is_fitted(self, ['pgms_arr_'])\n",
        "        else:\n",
        "            self.proj_sums_arr_ = self.proj_sums_.cpu().numpy()\n",
        "            check_is_fitted(self, ['proj_sums_arr_'])\n",
        "               \n",
        "        # Check data in X as required by scikit-learn v0.25\n",
        "        X = self._validate_data(X, reset = False)\n",
        "                 \n",
        "        # Cast array X into a floating point tensor to ensure all following calculations below  \n",
        "        # are done in float rather than integer, and send tensor X from CPU to GPU\n",
        "        X = torch.tensor(X, dtype = self.dtype).cuda()\n",
        "        \n",
        "        # Rescale X\n",
        "        X = self.rescale*X        \n",
        "        \n",
        "        # Calculate sum of squares of each row (sample) in X\n",
        "        X_sq_sum = (X**2).sum(dim = 1)\n",
        "        \n",
        "        # Number of rows in X\n",
        "        m = X.shape[0]\n",
        "        \n",
        "        # Number of columns in X\n",
        "        n = X.shape[1]\n",
        "\n",
        "        # Calculate X' using amplitude or inverse of the standard stereographic projection \n",
        "        # encoding method\n",
        "        if self.encoding == 'amplit':\n",
        "            X_prime = normalize(torch.cat([X, torch.ones(m, dtype = self.dtype) \\\n",
        "                                           .reshape(-1, 1).cuda()], dim = 1), p = 2, dim = 1)\n",
        "        elif self.encoding == 'stereo':\n",
        "            X_prime = (1/(X_sq_sum + 1)).reshape(-1, 1) \\\n",
        "                      *(torch.cat((2*X, (X_sq_sum - 1).reshape(-1, 1)), dim = 1))\n",
        "        else:\n",
        "            raise ValueError('encoding should be \"amplit\" or \"stereo\"')\n",
        "                       \n",
        "        # Function to calculate trace values for each class\n",
        "        def trace_func(i):\n",
        "            # Split X' into n_splits subsets, row-wise\n",
        "            # Send tensors from GPU to CPU and cast tensors into arrays, use np.array_split()\n",
        "            # because the equivalent torch.chunk() doesn't behave similarly to np.array_split()\n",
        "            X_prime_split_arr_full = np.array_split(X_prime.cpu().numpy(),\n",
        "                                                    indices_or_sections = self.n_splits,\n",
        "                                                    axis = 0)\n",
        "            \n",
        "            # Remove empty rows in X_prime_split_arr_full\n",
        "            X_prime_split_arr = [a for a in X_prime_split_arr_full if a.shape[0] > 0]\n",
        "\n",
        "            # Cast arrays back to tensors and send back from CPU to GPU\n",
        "            X_prime_split = [torch.tensor(q, dtype = self.dtype).cuda() for q in X_prime_split_arr]\n",
        "            \n",
        "            # Function to calculate trace values for each class, per subset split\n",
        "            def trace_split_func(j):\n",
        "                # Counter for j-th split X'\n",
        "                X_prime_split_jth = X_prime_split[j]\n",
        "                \n",
        "                # Number of rows (samples) in j-th split X'\n",
        "                X_prime_split_m = X_prime_split_jth.shape[0]\n",
        "                \n",
        "                # Encode vectors into quantum densities\n",
        "                density_chunk = torch.matmul(X_prime_split_jth.view(X_prime_split_m, n_prime, 1),\n",
        "                                             X_prime_split_jth.view(X_prime_split_m, 1, n_prime))\n",
        "                \n",
        "                # Calculate n-fold Kronecker tensor product\n",
        "                if self.n_copies == 1:\n",
        "                    density_chunk = density_chunk\n",
        "                else:\n",
        "                    density_chunk_copy = density_chunk\n",
        "                    for b in range(self.n_copies - 1):\n",
        "                        density_chunk = kronecker(density_chunk, density_chunk_copy)\n",
        "                        \n",
        "                # When Pretty Good Measurement is specified\n",
        "                if self.measure == 'pgm':\n",
        "                    # Calculate trace of the dot product of density of each row and Pretty Good\n",
        "                    # Measurement\n",
        "                    trace_class_split = torch.einsum('bij,ji->b', density_chunk, self.pgms_[i])\n",
        "                # When Helstrom measurement is specified\n",
        "                else:\n",
        "                    # Calculate trace of the dot product of density of each row and sum of \n",
        "                    # projectors with corresponding positive and negative eigenvalues respectively\n",
        "                    trace_class_split = torch.einsum('bij,ji->b', density_chunk, self.proj_sums_[i])\n",
        "                return trace_class_split\n",
        "            \n",
        "            # Determine length of X_prime_split_arr\n",
        "            X_prime_split_arr_len = len(X_prime_split_arr)\n",
        "\n",
        "            # Initialize tensor trace_class\n",
        "            trace_class = torch.empty([0], dtype = self.dtype).cuda()\n",
        "            for j in range(X_prime_split_arr_len):\n",
        "                # Calculate trace values for each class, per subset split\n",
        "                trace_class = torch.cat([trace_class, trace_split_func(j)], dim = 0)\n",
        "            return trace_class\n",
        "        \n",
        "        # Calculate trace values for each class\n",
        "        trace_matrix = torch.stack([trace_func(i) for i in range(num_classes)], dim = 1)\n",
        "        return trace_matrix\n",
        "                \n",
        "    \n",
        "    # Function for predict\n",
        "    def predict(self, X):\n",
        "        \"\"\"Performs PGM-HQC classification on X and returns the classes.\n",
        "        \n",
        "        Parameters\n",
        "        ----------\n",
        "        X : array-like, shape (n_samples, n_features)\n",
        "            The input samples. An array of int or float.\n",
        "            \n",
        "        Returns\n",
        "        -------\n",
        "        self.classes_[predict_trace_index] : array-like, shape (n_samples,)\n",
        "            The predicted binary classes. An array of str, int or float.\n",
        "        \"\"\"\n",
        "        # Determine column index with the higher trace value in trace_matrix\n",
        "        # If both columns have the same trace value, returns column index 1, which is different \n",
        "        # to np.argmax() which returns column index 0\n",
        "        predict_trace_index = torch.argmax(self.predict_proba(X), axis = 1)\n",
        "        # Returns the predicted binary classes\n",
        "        return self.classes_[predict_trace_index.cpu().numpy()]"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UB2dg2zPWgju"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mkrUhDg5WgkA"
      },
      "source": [
        "# appendicitis dataset (7 features, 106 rows)\n",
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv('appendicitis.tsv',delimiter='\\t')\n",
        "X = df.drop('target', axis=1).values\n",
        "y = df['target'].values\n",
        "\n",
        "from sklearn import model_selection\n",
        "X_train, X_test, y_train, y_test = model_selection.train_test_split(X, y, test_size=0.2, random_state=4)"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PizRcXdiWgkN",
        "outputId": "05458385-e194-4668-9d71-b5461175a2ac",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Check F1 score and Helstrom bound values for various rescale and n_copies values\n",
        "model = PGMHQC_gpu_dtype(rescale=0.5, n_copies=3, encoding='stereo', measure='hels', class_wgt='weighted', n_splits=1, dtype=torch.float32).fit(X_train, y_train)\n",
        "y_hat = model.predict(X_test)\n",
        "\n",
        "from sklearn import metrics\n",
        "metrics.f1_score(y_test, y_hat, average='weighted'), model.hels_bound_"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.7520661157024794, 0.8772541781266531)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l2l_WACvWgkk"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fBRcX1lyWgky"
      },
      "source": [
        "# banana dataset (2 features, 5300 rows)\n",
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv('banana.tsv', sep='\\t')\n",
        "X = df.drop('target', axis=1).values\n",
        "y = df['target'].values\n",
        "\n",
        "from sklearn import model_selection\n",
        "X_train, X_test, y_train, y_test = model_selection.train_test_split(X, y, test_size=0.2, random_state=4)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vb8GrFW0Wgk_",
        "outputId": "ff3de174-6bff-4493-d6a8-25ebad3783fa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Check F1 score and Helstrom bound values for various rescale and n_copies values\n",
        "model = PGMHQC_gpu_dtype(rescale=0.5, n_copies=4, encoding='stereo', measure='hels', class_wgt='weighted', n_splits=1, dtype=torch.float32).fit(X_train, y_train)\n",
        "y_hat = model.predict(X_test)\n",
        "\n",
        "from sklearn import metrics\n",
        "metrics.f1_score(y_test, y_hat, average='weighted'), model.hels_bound_"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.858978398722441, 0.7732936040410455)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7xrzaQkyWglR"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R8WD6wlIWglc"
      },
      "source": [
        "# iris dataset (5 features, 150 rows)\n",
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv('iris.tsv', sep='\\t')\n",
        "X = df.drop('target', axis=1).values\n",
        "y = df['target'].values"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LC9h58SwWgl2",
        "outputId": "d90d57b2-c746-4cd6-e3d6-1e232970d503",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Check if class imbalance\n",
        "df['target'].value_counts()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2    50\n",
              "1    50\n",
              "0    50\n",
              "Name: target, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S-nExohJWgmO",
        "outputId": "d2fae6b8-3e8b-48b2-dce8-ab22c044b8b2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from sklearn import model_selection\n",
        "X_train, X_test, y_train, y_test = model_selection.train_test_split(X, y, test_size=0.2, random_state=4)\n",
        "\n",
        "# Check trace values sum to 1 for first 5 rows\n",
        "model = PGMHQC_gpu_dtype(rescale=0.5, n_copies=1, encoding='stereo', measure='pgm', class_wgt=None, n_splits=1, dtype=torch.float32).fit(X_train, y_train)\n",
        "torch.cat([model.predict_proba(X_test), torch.sum(model.predict_proba(X_test), dim=1).reshape(-1,1)], dim=1)[0:5,:]"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.2714, 0.3513, 0.3773, 1.0000],\n",
              "        [0.2244, 0.3658, 0.4098, 1.0000],\n",
              "        [0.5722, 0.2603, 0.1675, 1.0000],\n",
              "        [0.2334, 0.3523, 0.4143, 1.0000],\n",
              "        [0.2178, 0.3588, 0.4234, 1.0000]], device='cuda:0')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "898ubyWnWgmc",
        "outputId": "dd2393a7-1641-4308-82c6-92701e01022b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Check F1 score for various n_copies values\n",
        "model = PGMHQC_gpu_dtype(rescale=0.5, n_copies=1, encoding='stereo', measure='pgm', class_wgt=None, n_splits=1, dtype=torch.float32).fit(X_train, y_train)\n",
        "y_hat = model.predict(X_test)\n",
        "\n",
        "from sklearn import metrics\n",
        "metrics.f1_score(y_test, y_hat, average='weighted')"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8940170940170941"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ycxxC68wWgmq",
        "outputId": "35a5a433-9ed6-471c-99fe-b716e3f5a28f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Check F1 score for various n_copies values\n",
        "model = PGMHQC_gpu_dtype(rescale=0.5, n_copies=2, encoding='stereo', measure='pgm', class_wgt=None, n_splits=1, dtype=torch.float32).fit(X_train, y_train)\n",
        "y_hat = model.predict(X_test)\n",
        "\n",
        "from sklearn import metrics\n",
        "metrics.f1_score(y_test, y_hat, average='weighted')"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8940170940170941"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N4-pemXHWgnE",
        "outputId": "429937d0-1d61-45ba-f901-14822b161df4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Check F1 score for various n_copies values\n",
        "model = PGMHQC_gpu_dtype(rescale=0.5, n_copies=3, encoding='stereo', measure='pgm', class_wgt=None, n_splits=1, dtype=torch.float32).fit(X_train, y_train)\n",
        "y_hat = model.predict(X_test)\n",
        "\n",
        "from sklearn import metrics\n",
        "metrics.f1_score(y_test, y_hat, average='weighted')"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8940170940170941"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6q_Qdg-9WgnQ",
        "outputId": "a824ab8a-c852-4fde-cce4-d6fa96de6ae3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Check F1 score for various n_copies values\n",
        "model = PGMHQC_gpu_dtype(rescale=0.5, n_copies=4, encoding='stereo', measure='pgm', class_wgt=None, n_splits=1, dtype=torch.float32).fit(X_train, y_train)\n",
        "y_hat = model.predict(X_test)\n",
        "\n",
        "from sklearn import metrics\n",
        "metrics.f1_score(y_test, y_hat, average='weighted')"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8940170940170941"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KtL0AvFJWgno",
        "outputId": "1b85cecf-74f8-4651-9921-79e6311e9932",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Check F1 score for various n_copies values\n",
        "model = PGMHQC_gpu_dtype(rescale=0.5, n_copies=5, encoding='stereo', measure='pgm', class_wgt=None, n_splits=1, dtype=torch.float32).fit(X_train, y_train)\n",
        "y_hat = model.predict(X_test)\n",
        "\n",
        "from sklearn import metrics\n",
        "metrics.f1_score(y_test, y_hat, average='weighted')"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8940170940170941"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AcwVrvb8Wgn2"
      },
      "source": [
        "# Testing using scikit-learn's GridSearchCV\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "# Create rescale hyperparamter list [0.1, 0.5, 1, 1.5,...,10.0]\n",
        "rescale_list1 = [0.1]\n",
        "rescale_list2 = np.linspace(0.5, 10, 20).tolist()\n",
        "rescale_list1.extend(rescale_list2)\n",
        "\n",
        "param_grid = {'rescale':rescale_list1, 'n_copies':[1, 2, 3, 4], 'encoding':['amplit', 'stereo'], 'measure':['pgm'], 'class_wgt':[None]}\n",
        "models = GridSearchCV(PGMHQC_gpu_dtype(n_splits=1, dtype=torch.float32), param_grid, scoring='f1_weighted').fit(X_train, y_train)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xPrF5OC9WgoU",
        "outputId": "b747d663-08bc-4687-8418-972dcf29f027",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Best F1 score\n",
        "best_model = models.best_estimator_\n",
        "y_hat = best_model.predict(X_test)\n",
        "\n",
        "from sklearn import metrics\n",
        "metrics.f1_score(y_test, y_hat, average='weighted')"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1.0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7-zjELrzWgoo",
        "outputId": "85ea4d2f-1227-4fb2-a16f-a2d165cd6f6d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Best hyperparameter combination\n",
        "models.best_params_"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'class_wgt': None,\n",
              " 'encoding': 'stereo',\n",
              " 'measure': 'pgm',\n",
              " 'n_copies': 4,\n",
              " 'rescale': 0.1}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V-EcGpP4Wgo0"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YfUGySUYWgpA"
      },
      "source": [
        "# balance-scale dataset (5 features, 625 rows)\n",
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv('balance-scale.tsv', sep='\\t')\n",
        "X = df.drop('target', axis=1).values\n",
        "y = df['target'].values"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cJW2XvHGWgpe",
        "outputId": "cf312d8c-4988-4af3-b46f-6cff9253bbd1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Check if class imbalance\n",
        "df['target'].value_counts()"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2    288\n",
              "1    288\n",
              "0     49\n",
              "Name: target, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gXZVf50lWgpu",
        "outputId": "d350ed2a-fc30-4f08-d936-44030ee325f9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from sklearn import model_selection\n",
        "X_train, X_test, y_train, y_test = model_selection.train_test_split(X, y, test_size=0.2, random_state=4)\n",
        "\n",
        "# Check trace values sum to 1 for first 5 rows\n",
        "model = PGMHQC_gpu_dtype(rescale=1, n_copies=2, encoding='stereo', measure='pgm', class_wgt=None, n_splits=1, dtype=torch.float32).fit(X_train, y_train)\n",
        "torch.cat([model.predict_proba(X_test), torch.sum(model.predict_proba(X_test), dim=1).reshape(-1,1)], dim=1)[0:5,:]"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.3146, 0.3284, 0.3570, 1.0000],\n",
              "        [0.3154, 0.3281, 0.3566, 1.0000],\n",
              "        [0.3152, 0.3429, 0.3419, 1.0000],\n",
              "        [0.3229, 0.3163, 0.3607, 0.9999],\n",
              "        [0.3167, 0.3418, 0.3415, 1.0000]], device='cuda:0')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BP3ltHOwWgp8",
        "outputId": "d1f0988d-d533-4117-e6ed-43a233bbeedc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Check F1 score for various n_copies values\n",
        "model = PGMHQC_gpu_dtype(rescale=1, n_copies=1, encoding='stereo', measure='pgm', class_wgt=None, n_splits=1, dtype=torch.float32).fit(X_train, y_train)\n",
        "y_hat = model.predict(X_test)\n",
        "\n",
        "from sklearn import metrics\n",
        "metrics.f1_score(y_test, y_hat, average='weighted')"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8367315008833559"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y6KDcNygWgqj",
        "outputId": "4f6b5f91-fa6f-4524-d00c-7d77468f0aae",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Check F1 score for various n_copies values\n",
        "model = PGMHQC_gpu_dtype(rescale=1, n_copies=2, encoding='stereo', measure='pgm', class_wgt=None, n_splits=1, dtype=torch.float32).fit(X_train, y_train)\n",
        "y_hat = model.predict(X_test)\n",
        "\n",
        "from sklearn import metrics\n",
        "metrics.f1_score(y_test, y_hat, average='weighted')"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8447445962196664"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3L4jnNlFWgqt",
        "outputId": "1b9343a0-4625-414e-a1c5-18e31df4b851",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Check F1 score for various n_copies values\n",
        "model = PGMHQC_gpu_dtype(rescale=1, n_copies=3, encoding='stereo', measure='pgm', class_wgt=None, n_splits=1, dtype=torch.float32).fit(X_train, y_train)\n",
        "y_hat = model.predict(X_test)\n",
        "\n",
        "from sklearn import metrics\n",
        "metrics.f1_score(y_test, y_hat, average='weighted')"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8605017010473518"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pp2IKkYtWgrs",
        "outputId": "ecdbc889-c461-4409-d59d-e67660984d40",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "### SCORE IS SLIGHTLY DIFFERENT BETWEEN FLOAT32 AND FLOAT64 (0.8676190476190477 vs. 0.8682253173918403) ###\n",
        "# Check F1 score for various n_copies values\n",
        "model = PGMHQC_gpu_dtype(rescale=1, n_copies=4, encoding='stereo', measure='pgm', class_wgt=None, n_splits=1, dtype=torch.float32).fit(X_train, y_train)\n",
        "y_hat = model.predict(X_test)\n",
        "\n",
        "from sklearn import metrics\n",
        "metrics.f1_score(y_test, y_hat, average='weighted')"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8676190476190477"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pVHpR8LKWgr4",
        "outputId": "d5f2097f-9c15-4e81-d0cc-415381824984",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Check F1 score for various n_copies values\n",
        "model = PGMHQC_gpu_dtype(rescale=1, n_copies=5, encoding='stereo', measure='pgm', class_wgt=None, n_splits=50, dtype=torch.float32).fit(X_train, y_train)\n",
        "y_hat = model.predict(X_test)\n",
        "\n",
        "from sklearn import metrics\n",
        "metrics.f1_score(y_test, y_hat, average='weighted')"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8678604178430266"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TefpoXjmWgsA"
      },
      "source": [
        "# Testing using scikit-learn's GridSearchCV\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "# Create rescale hyperparamter list [0.1, 0.5, 1, 1.5,...,10.0]\n",
        "rescale_list1 = [0.1]\n",
        "rescale_list2 = np.linspace(0.5, 10, 20).tolist()\n",
        "rescale_list1.extend(rescale_list2)\n",
        "\n",
        "param_grid = {'rescale':rescale_list1, 'n_copies':[1, 2, 3, 4], 'encoding':['amplit', 'stereo'], 'measure':['pgm'], 'class_wgt':[None]}\n",
        "models = GridSearchCV(PGMHQC_gpu_dtype(n_splits=1, dtype=torch.float32), param_grid, scoring='f1_weighted').fit(X_train, y_train)"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dPg7dxprWgsO",
        "outputId": "4b3a2f06-ea4a-4776-ffff-97a4a70524b3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Best F1 score\n",
        "best_model = models.best_estimator_\n",
        "y_hat = best_model.predict(X_test)\n",
        "\n",
        "from sklearn import metrics\n",
        "metrics.f1_score(y_test, y_hat, average='weighted')"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9106013291733841"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "njzJZhG4WgsX",
        "outputId": "024eeffa-48d3-4260-82b0-4b628bb2a1ba",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Best hyperparameter combination\n",
        "models.best_params_"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'class_wgt': None,\n",
              " 'encoding': 'amplit',\n",
              " 'measure': 'pgm',\n",
              " 'n_copies': 2,\n",
              " 'rescale': 2.0}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mqKy0QoDWgso"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YYtnAVSwWgs2"
      },
      "source": [
        "# new-thyroid dataset (6 features, 215 rows)\n",
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv('new-thyroid.tsv', sep='\\t')\n",
        "X = df.drop('target', axis=1).values\n",
        "y = df['target'].values"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NLAbg8bXWgtE",
        "outputId": "6ca271b4-2ef7-4c1e-d935-3cb51526e7b6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Check if class imbalance\n",
        "df['target'].value_counts()"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1    150\n",
              "2     35\n",
              "3     30\n",
              "Name: target, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DaMSnZsDWgtM",
        "outputId": "370bc05f-0532-4942-f0a5-e79ab0192ec0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from sklearn import model_selection\n",
        "X_train, X_test, y_train, y_test = model_selection.train_test_split(X, y, test_size=0.2, random_state=4)\n",
        "\n",
        "# Check trace values sum to 1 for first 5 rows\n",
        "model = PGMHQC_gpu_dtype(rescale=1.5, n_copies=3, encoding='stereo', measure='pgm', class_wgt=None, n_splits=1, dtype=torch.float32).fit(X_train, y_train)\n",
        "torch.cat([model.predict_proba(X_test), torch.sum(model.predict_proba(X_test), dim=1).reshape(-1,1)], dim=1)[0:5,:]"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.3336, 0.3326, 0.3336, 0.9998],\n",
              "        [0.3313, 0.3300, 0.3386, 0.9998],\n",
              "        [0.3337, 0.3324, 0.3337, 0.9998],\n",
              "        [0.3338, 0.3325, 0.3336, 0.9998],\n",
              "        [0.3338, 0.3328, 0.3331, 0.9998]], device='cuda:0')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8ZmBz6ilWgtq",
        "outputId": "d76a9ada-7527-43cc-8993-cbbe123b6c31",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Check F1 score for various n_copies values\n",
        "model = PGMHQC_gpu_dtype(rescale=1.5, n_copies=1, encoding='stereo', measure='pgm', class_wgt=None, n_splits=1, dtype=torch.float32).fit(X_train, y_train)\n",
        "y_hat = model.predict(X_test)\n",
        "\n",
        "from sklearn import metrics\n",
        "metrics.f1_score(y_test, y_hat, average='weighted')"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8549863029667192"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k0zmghdjWgt0",
        "outputId": "525b3e08-34ee-4ba3-82af-41770f0ab9f0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Check F1 score for various n_copies values\n",
        "model = PGMHQC_gpu_dtype(rescale=1.5, n_copies=2, encoding='stereo', measure='pgm', class_wgt=None, n_splits=1, dtype=torch.float32).fit(X_train, y_train)\n",
        "y_hat = model.predict(X_test)\n",
        "\n",
        "from sklearn import metrics\n",
        "metrics.f1_score(y_test, y_hat, average='weighted')"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8549863029667192"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x2-y_eLYWguB",
        "outputId": "c0c3f92c-6f45-4a2a-abfe-bac221ba10d8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "### SCORE IS SLIGHTLY DIFFERENT BETWEEN FLOAT32 AND FLOAT64 (0.8314708547266686 vs. 0.8549863029667192) ###\n",
        "# Check F1 score for various n_copies values\n",
        "model = PGMHQC_gpu_dtype(rescale=1.5, n_copies=3, encoding='stereo', measure='pgm', class_wgt=None, n_splits=1, dtype=torch.float32).fit(X_train, y_train)\n",
        "y_hat = model.predict(X_test)\n",
        "\n",
        "from sklearn import metrics\n",
        "metrics.f1_score(y_test, y_hat, average='weighted')"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8314708547266686"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yys-sUwkWguK",
        "outputId": "c9391781-7614-456b-b7d4-4507e2318f2a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Check F1 score for various n_copies values\n",
        "model = PGMHQC_gpu_dtype(rescale=1.5, n_copies=4, encoding='stereo', measure='pgm', class_wgt=None, n_splits=1, dtype=torch.float32).fit(X_train, y_train)\n",
        "y_hat = model.predict(X_test)\n",
        "\n",
        "from sklearn import metrics\n",
        "metrics.f1_score(y_test, y_hat, average='weighted')"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8549863029667192"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UzPAHJD4Wgub",
        "outputId": "7c4a6dc3-b0ba-4a27-c8bf-f9e21d567331",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Check F1 score for various n_copies values\n",
        "# 150+35+30 = 215\n",
        "model = PGMHQC_gpu_dtype(rescale=1.5, n_copies=5, encoding='stereo', measure='pgm', class_wgt=None, n_splits=100, dtype=torch.float32).fit(X_train, y_train)\n",
        "y_hat = model.predict(X_test)\n",
        "\n",
        "from sklearn import metrics\n",
        "metrics.f1_score(y_test, y_hat, average='weighted')"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8549863029667192"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fYDF5waDWguk"
      },
      "source": [
        "# Testing using scikit-learn's GridSearchCV\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "# Create rescale hyperparamter list [0.1, 0.5, 1, 1.5,...,10.0]\n",
        "rescale_list1 = [0.1]\n",
        "rescale_list2 = np.linspace(0.5, 10, 20).tolist()\n",
        "rescale_list1.extend(rescale_list2)\n",
        "\n",
        "# 150+35+30 = 215\n",
        "param_grid = {'rescale':rescale_list1, 'n_copies':[1, 2, 3, 4], 'encoding':['amplit', 'stereo'], 'measure':['pgm'], 'class_wgt':[None]}\n",
        "models = GridSearchCV(PGMHQC_gpu_dtype(n_splits=1, dtype=torch.float32), param_grid, scoring='f1_weighted').fit(X_train, y_train)"
      ],
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DyZHsQFFWgvS",
        "outputId": "ec9df386-ef69-45b0-cbaa-59d63b4ac5e9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Best F1 score\n",
        "best_model = models.best_estimator_\n",
        "y_hat = best_model.predict(X_test)\n",
        "\n",
        "from sklearn import metrics\n",
        "metrics.f1_score(y_test, y_hat, average='weighted')"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9516311369509043"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RNYt1QrUWgva",
        "outputId": "5e9c00c0-178a-4eb7-ea56-79fedfb9145a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Best hyperparameter combination\n",
        "models.best_params_"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'class_wgt': None,\n",
              " 'encoding': 'amplit',\n",
              " 'measure': 'pgm',\n",
              " 'n_copies': 2,\n",
              " 'rescale': 0.1}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q91xy2t-prtV"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}