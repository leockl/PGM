{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2SjbUqx76_TV"
   },
   "outputs": [],
   "source": [
    "### This is a GPU implementation for the HQC classifier using Scikit-learn's methods, but with PyTorch as the backend. ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "s9I9rWGT6_Td"
   },
   "outputs": [],
   "source": [
    "# I have implemented the code below in such a way that you would only need to input X and y as numpy arrays and the\n",
    "# output y_hat would also be a numpy array (rather than PyTorch tensors). This would make it easier to use the package\n",
    "# below with minimal knowledge of PyTorch tensors.\n",
    "\n",
    "# Take note of the parameter n_splits, where the implementation of n_splits now is different to the one in the CPU case.\n",
    "# Please read the description of n_splits below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "PuJShFTnWgjb"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from sklearn.utils.validation import check_is_fitted\n",
    "from sklearn.utils.multiclass import check_classification_targets\n",
    "import torch\n",
    "from torch.nn.functional import normalize\n",
    "from scipy import linalg\n",
    "\n",
    "class PGMHQC_gpu_dtype(BaseEstimator, ClassifierMixin):\n",
    "    \"\"\"The Pretty Good Measurement (PGM) - Helstrom Quantum Centroid (HQC) classifier is a \n",
    "    quantum-inspired supervised classification approach for data with multiple classes.\n",
    "                         \n",
    "    Parameters\n",
    "    ----------\n",
    "    rescale : int or float, default = 1\n",
    "        The dataset rescaling factor. A parameter used for rescaling the dataset. \n",
    "    encoding : str, default = 'amplit'\n",
    "        The encoding method used to encode vectors into quantum densities. Possible values:\n",
    "        'amplit', 'stereo'. 'amplit' means using the amplitude encoding method. 'stereo' means \n",
    "        using the inverse of the standard stereographic projection encoding method. Default set \n",
    "        to 'amplit'.\n",
    "    n_copies : int, default = 1\n",
    "        The number of copies to take for each quantum density. This is equivalent to taking \n",
    "        the n-fold Kronecker tensor product for each quantum density.\n",
    "    measure : str, default = 'pgm'\n",
    "        The measurement used to distinguish between quantum states. Possible values: 'pgm', \n",
    "        'hels'. The value 'pgm' stands for \"Pretty Good Measurement\", 'hels' stands for \n",
    "        \"Helstrom measurement\" (applicable only for binary classification). Default set to \n",
    "        'pgm'. \n",
    "    class_wgt : str, default = None        \n",
    "        Applicable only when \"Helstrom measurement\" is selected. This is the class weights \n",
    "        assigned to the Quantum Helstrom observable terms. Possible values: 'equi', 'weighted', \n",
    "        None. 'equi' means assigning equal weights of 1/2 (equiprobable) to the two terms in \n",
    "        the Quantum Helstrom observable. 'weighted' means assigning weights equal to the \n",
    "        proportion of the number of rows in each class to the two terms in the Quantum Helstrom \n",
    "        observable. When using \"Pretty Good Measurement\", specify class_wgt = None. Default set \n",
    "        to None.        \n",
    "    n_splits : int, default = 1\n",
    "        The number of subset splits performed on the input dataset row-wise and on the number \n",
    "        of eigenvalues/eigenvectors of the Quantum Helstrom observable for optimal speed \n",
    "        performance. If 1 is given, no splits are performed. For optimal speed, recommend \n",
    "        using small values as close to 1 as possible. If memory blow-out occurs, increase \n",
    "        n_splits.\n",
    "    dtype : torch.float32 or torch.float64, default = torch.float64\n",
    "        The float datatype used for the elements in the Pytorch tensor dataset. Datatype has to\n",
    "        be of float to ensure calculations are done in float rather than integer. To achieve\n",
    "        higher n_copies without memory blow-out issues, reduce float precision, which may or may   \n",
    "        not affect accuracy in a significant way.\n",
    "    \n",
    "    Attributes\n",
    "    ----------\n",
    "    classes_ : ndarray, shape (n_classes,)\n",
    "        Sorted binary classes. Stored in CPU.\n",
    "    qcentroids_ : tensor, size (n_classes, (n_features + 1)**n_copies, (n_features + 1)**n_copies)\n",
    "        Quantum Centroids for each class. Stored in GPU.\n",
    "    pgms_ : tensor, shape (n_classes, (n_features + 1)**n_copies, (n_features + 1)**n_copies)\n",
    "        Pretty Good Measurement. Stored in GPU.\n",
    "    hels_obs_ : tensor, size ((n_features + 1)**n_copies, (n_features + 1)**n_copies)\n",
    "        Quantum Helstrom observable. Stored in GPU.\n",
    "    proj_sums_ : tensor, size (n_classes, (n_features + 1)**n_copies, (n_features + 1)**n_copies)\n",
    "        Sum of the projectors of the Quantum Helstrom observable's eigenvectors, which has\n",
    "        corresponding positive and negative eigenvalues respectively. Stored in GPU.\n",
    "    hels_bound_ : float\n",
    "        Helstrom bound is the upper bound of the probability that one can correctly \n",
    "        discriminate whether a quantum density is of which of the two binary quantum density \n",
    "        pattern. Stored in CPU.         \n",
    "    \"\"\"   \n",
    "    # Initialize model hyperparameters\n",
    "    def __init__(self, \n",
    "                 rescale = 1,\n",
    "                 encoding = 'amplit',\n",
    "                 n_copies = 1,  \n",
    "                 measure = 'pgm',\n",
    "                 class_wgt = None, \n",
    "                 n_splits = 1,\n",
    "                 dtype = torch.float64):\n",
    "        self.rescale = rescale\n",
    "        self.encoding = encoding\n",
    "        self.n_copies = n_copies\n",
    "        self.measure = measure\n",
    "        self.class_wgt = class_wgt\n",
    "        self.n_splits = n_splits\n",
    "        self.dtype = dtype\n",
    "        \n",
    "        # Raise error if dtype is not torch.float32 or torch.float64\n",
    "        if self.dtype not in [torch.float32, torch.float64]:\n",
    "            raise ValueError('dtype should be torch.float32 or torch.float64 only')\n",
    "        \n",
    "    \n",
    "    # Function for kronecker tensor product for PyTorch tensors, set as global function\n",
    "    global kronecker\n",
    "    def kronecker(A, B):\n",
    "        return torch.einsum('nab,ncd->nacbd', A, B).view(A.size(0), \n",
    "                                                         A.size(1)*B.size(1), \n",
    "                                                         A.size(2)*B.size(2))\n",
    "    \n",
    "    \n",
    "    # Function for fit\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"Perform PGM-HQC classification with the amplitude and inverse of the standard \n",
    "        stereographic projection encoding methods, with the option to rescale the dataset prior \n",
    "        to encoding.\n",
    "                \n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like, shape (n_samples, n_features)\n",
    "            The training input samples. An array of int or float.\n",
    "        y : array-like, shape (n_samples,)\n",
    "            The training input binary target values. An array of str, int or float.\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        self : object\n",
    "            Returns self.\n",
    "        \"\"\"\n",
    "        # Check data in X and y as required by scikit-learn v0.25\n",
    "        X, y = self._validate_data(X, y, reset = True)\n",
    "        \n",
    "        # Ensure target array y is of non-regression type  \n",
    "        # Added as required by sklearn check_estimator\n",
    "        check_classification_targets(y)\n",
    "            \n",
    "        # Store classes and encode y into class indexes\n",
    "        self.classes_, y_class_index = np.unique(y, return_inverse = True)\n",
    "        \n",
    "        # Number of classes, set as global variable\n",
    "        global num_classes\n",
    "        num_classes = len(self.classes_)\n",
    "        \n",
    "        # Raise error when there are more than 2 classes and Helstrom measurement is specified\n",
    "        if num_classes > 2 and self.measure == 'hels':\n",
    "            raise ValueError('Helstrom measurement can be applied for binary classification only')\n",
    "        else:                    \n",
    "            # Cast array X into a floating point tensor to ensure all following calculations below  \n",
    "            # are done in float rather than integer, and send tensor X from CPU to GPU\n",
    "            X = torch.tensor(X, dtype = self.dtype).cuda()\n",
    "        \n",
    "            # Rescale X\n",
    "            X = self.rescale*X\n",
    "        \n",
    "            # Calculate sum of squares of each row (sample) in X\n",
    "            X_sq_sum = (X**2).sum(dim = 1)\n",
    "        \n",
    "            # Number of rows in X\n",
    "            m = X.shape[0]\n",
    "        \n",
    "            # Number of columns in X\n",
    "            n = X.shape[1]\n",
    "        \n",
    "            # Calculate X' using amplitude or inverse of the standard stereographic projection \n",
    "            # encoding method\n",
    "            if self.encoding == 'amplit':\n",
    "                X_prime = normalize(torch.cat([X, torch.ones(m, dtype = self.dtype) \\\n",
    "                                               .reshape(-1, 1).cuda()], dim = 1), p = 2, dim = 1)\n",
    "            elif self.encoding == 'stereo':\n",
    "                X_prime = (1/(X_sq_sum + 1)).reshape(-1, 1) \\\n",
    "                          *(torch.cat((2*X, (X_sq_sum - 1).reshape(-1, 1)), dim = 1))\n",
    "            else:\n",
    "                raise ValueError('encoding should be \"amplit\" or \"stereo\"')\n",
    "        \n",
    "            # Number of columns in X', set as global variable\n",
    "            global n_prime\n",
    "            n_prime = n + 1\n",
    "        \n",
    "            # Function to calculate number of rows (samples) and Quantum Centroids for each class \n",
    "            def qcentroids_terms_func(i):\n",
    "                # Cast array y_class_index into a tensor and send from CPU to GPU\n",
    "                # Determine rows (samples) in X' belonging to either class\n",
    "                X_prime_class = X_prime[torch.CharTensor(y_class_index).cuda() == i]\n",
    "                                    \n",
    "                # Split X' belonging to either class into n_splits subsets, row-wise\n",
    "                # Send tensors from GPU to CPU and cast tensors into arrays, use np.array_split()\n",
    "                # because the equivalent torch.chunk() doesn't behave similarly to np.array_split()\n",
    "                X_prime_class_split_arr = np.array_split(X_prime_class.cpu().numpy(),\n",
    "                                                         indices_or_sections = self.n_splits,\n",
    "                                                         axis = 0)\n",
    "            \n",
    "                # Cast arrays back to tensors and send back from CPU to GPU\n",
    "                X_prime_class_split = [torch.tensor(a, dtype = self.dtype).cuda() \n",
    "                                       for a in X_prime_class_split_arr]\n",
    "            \n",
    "                # Function to calculate sum of quantum densities belonging to each class, \n",
    "                # per subset split\n",
    "                def X_prime_class_split_func(j):\n",
    "                    # Counter for j-th split of X'\n",
    "                    X_prime_class_split_jth = X_prime_class_split[j]\n",
    "                \n",
    "                    # Number of rows (samples) in j-th split of X'\n",
    "                    m_class_split = X_prime_class_split_jth.shape[0]\n",
    "                \n",
    "                    # Encode vectors into quantum densities\n",
    "                    density_chunk = torch.matmul(X_prime_class_split_jth.view(m_class_split, \n",
    "                                                                              n_prime, 1),\n",
    "                                                 X_prime_class_split_jth.view(m_class_split, \n",
    "                                                                              1, n_prime))\n",
    "                \n",
    "                    # Calculate n-fold Kronecker tensor product\n",
    "                    if self.n_copies == 1:\n",
    "                        density_chunk = density_chunk\n",
    "                    else:\n",
    "                        density_chunk_copy = density_chunk\n",
    "                        for b in range(self.n_copies - 1):\n",
    "                            density_chunk = kronecker(density_chunk, density_chunk_copy)\n",
    "                    \n",
    "                    # Calculate sum of quantum densities\n",
    "                    density_chunk_sum = density_chunk.sum(dim = 0)\n",
    "                    return density_chunk_sum\n",
    "\n",
    "                # Number of rows/columns in density matrix, set as global variable\n",
    "                global density_nrow_ncol\n",
    "                density_nrow_ncol = n_prime**self.n_copies\n",
    "            \n",
    "                # Initialize tensor density_class_sum\n",
    "                density_class_sum = torch.zeros([density_nrow_ncol, density_nrow_ncol], \n",
    "                                                dtype = self.dtype).cuda()\n",
    "                for j in range(self.n_splits):\n",
    "                    # Calculate sum of quantum densities belonging to each class\n",
    "                    density_class_sum = density_class_sum + X_prime_class_split_func(j)\n",
    "            \n",
    "                # Number of rows (samples) in X' belonging to each class\n",
    "                m_class = X_prime_class.shape[0]\n",
    "            \n",
    "                # Function to calculate Quantum Centroid belonging to each class\n",
    "                def qcentroid_func():\n",
    "                    # Calculate Quantum Centroid belonging to each class\n",
    "                    # Added ZeroDivisionError as required by sklearn check_estimator\n",
    "                    try:\n",
    "                        qcentroid = (1/m_class)*density_class_sum\n",
    "                    except ZeroDivisionError:\n",
    "                        qcentroid = 0 \n",
    "                    return qcentroid\n",
    "            \n",
    "                # Calculate Quantum Centroid belonging to each class\n",
    "                qcentroid_class = qcentroid_func()\n",
    "                return m_class, qcentroid_class\n",
    "            \n",
    "            # Calculate number of rows (samples) and Quantum Centroids for each class \n",
    "            qcentroids_terms = [qcentroids_terms_func(i) for i in range(num_classes)]\n",
    "            \n",
    "            # Determine Quantum Centroids\n",
    "            self.qcentroids_ = torch.stack([qcentroids_terms[z][1] for z in range(num_classes)], dim = 0)\n",
    "            \n",
    "            # When Pretty Good Measurement is specified\n",
    "            if self.measure == 'pgm':\n",
    "                if self.class_wgt == None:\n",
    "                    # Calculate R\n",
    "                    R = self.qcentroids_.sum(dim = 0)\n",
    "                    \n",
    "                    # Calculate square root of pseudo inverse of R\n",
    "                    # Change datatype of R to float64 as the square root of a matrix calculation is highly \n",
    "                    # senstive to numerical precision/rounding\n",
    "                    # Calculate pseudo inverse of R, send tensor from GPU to CPU and cast into an array\n",
    "                    # Use scipy.linalg.sqrtm() to calculate square root of the pseudo inverse of R because \n",
    "                    # there is no equivalent function in PyTorch which behaves numerically similarly \n",
    "                    # Remove complex part of the matrix created due to numerical precision/rounding issues\n",
    "                    # in machine language\n",
    "                    # Cast array back into a tensor and send back from CPU to GPU\n",
    "                    sqrt_pinv_R = torch.tensor(np.real(linalg.sqrtm(torch.pinverse(torch.as_tensor(R, dtype = \\\n",
    "                                              torch.float64)).cpu().numpy())), dtype = self.dtype).cuda()\n",
    "                    \n",
    "                    # Calculate kernel of R\n",
    "                    # Change datatype of R to float64 as the kernel of a matrix calculation is highly\n",
    "                    # senstive to numerical precision/rounding\n",
    "                    # Send tensor from GPU to CPU and cast into an array, use scipy.linalg.null_space()\n",
    "                    # to calculate kernel because there is no equivalent function in PyTorch which\n",
    "                    # behaves numerically similarly\n",
    "                    # Cast array back into a tensor and send back from CPU to GPU\n",
    "                    ker_R = torch.tensor(linalg.null_space(torch.as_tensor(R, dtype = torch.float64).cpu() \\\n",
    "                                         .numpy()), dtype = self.dtype).cuda()\n",
    "                    \n",
    "                    # Calculate projector of kernel of R\n",
    "                    proj_ker_R = torch.matmul(ker_R, ker_R.T)\n",
    "                    \n",
    "                    # Function to calculate Pretty Good Measurement\n",
    "                    def pgm_func(a):\n",
    "                        return torch.matmul(torch.matmul(sqrt_pinv_R, self.qcentroids_[a]), sqrt_pinv_R) \\\n",
    "                               + (1/num_classes)*proj_ker_R\n",
    "                                               \n",
    "                    # Calculate Pretty Good Measurement\n",
    "                    self.pgms_ = torch.stack([pgm_func(a) for a in range(num_classes)], dim = 0)\n",
    "                else:\n",
    "                    raise ValueError('when using \"pgm\" measure, class_wgt should be None')\n",
    "            # When Helstrom measurement is specified\n",
    "            elif self.measure == 'hels':\n",
    "                # Calculate quantum Helstrom observable\n",
    "                if self.class_wgt == 'equi':\n",
    "                    self.hels_obs_ = 0.5*(self.qcentroids_[0] - self.qcentroids_[1])\n",
    "                elif self.class_wgt == 'weighted':\n",
    "                    self.hels_obs_ = (qcentroids_terms[0][0]/m)*self.qcentroids_[0] \\\n",
    "                                     - (qcentroids_terms[1][0]/m)*self.qcentroids_[1]\n",
    "                else:\n",
    "                    raise ValueError('when using \"hels\" measure, class_wgt should be \"equi\" or \"weighted\"')\n",
    "                \n",
    "                # Number of rows/columns in density matrix, set as global variable\n",
    "                global density_nrow_ncol\n",
    "                density_nrow_ncol = self.hels_obs_.shape[0]\n",
    "                \n",
    "                # Calculate eigenvalues w and unit eigenvectors v of the quantum Helstrom observable\n",
    "                w, v = torch.symeig(self.hels_obs_, eigenvectors = True)\n",
    "                \n",
    "                # Length of w\n",
    "                len_w = len(w)\n",
    "                \n",
    "                # Initialize tensor eigval_class\n",
    "                eigval_class = torch.empty_like(w, dtype = self.dtype).cuda()\n",
    "                for d in range(len_w):\n",
    "                    # Create a tensor of 0s and 1s to indicate positive and negative eigenvalues\n",
    "                    # respectively\n",
    "                    if w[d] > 0:\n",
    "                        eigval_class[d] = 0\n",
    "                    else:\n",
    "                        eigval_class[d] = 1\n",
    "                        \n",
    "                # Transpose matrix v containing eigenvectors to row-wise\n",
    "                eigvec = v.T\n",
    "                \n",
    "                # Function to calculate sum of the projectors corresponding to positive and negative\n",
    "                # eigenvalues respectively\n",
    "                def sum_proj_func(e):\n",
    "                    # Split eigenvectors belonging to positive or negative eigenvalues into n_splits subsets\n",
    "                    # Send tensors from GPU to CPU and cast tensors into arrays, use np.array_split()\n",
    "                    # because the equivalent torch.chunk() doesn't behave similarly to np.array_split()\n",
    "                    eigvec_class_split_arr_full = np.array_split(eigvec.cpu().numpy()[eigval_class.cpu() == e],\n",
    "                                                                 indices_or_sections = self.n_splits,\n",
    "                                                                 axis = 0)\n",
    "                    \n",
    "                    # Remove empty rows in eigvec_class_split_arr_full\n",
    "                    eigvec_class_split_arr = [f for f in eigvec_class_split_arr_full if f.shape[0] > 0]\n",
    "                    \n",
    "                    # Cast arrays back to tensors and send back from CPU to GPU\n",
    "                    eigvec_class_split = [torch.tensor(g, dtype = self.dtype).cuda()\n",
    "                                          for g in eigvec_class_split_arr]\n",
    "                    \n",
    "                    # Function to calculate sum of the projectors corresponding to positive and negative\n",
    "                    # eigenvalues respectively, per subset split\n",
    "                    def eigvec_class_split_func(h):\n",
    "                        # Counter for h-th split of eigvec\n",
    "                        eigvec_class_split_hth = eigvec_class_split[h]\n",
    "                        \n",
    "                        # Number of rows (samples) in h-th split of eigvec\n",
    "                        m_eigvec_class_split = eigvec_class_split_hth.shape[0]\n",
    "                        \n",
    "                        # Calculate projectors corresponding to positive and negative eigenvalues\n",
    "                        # respectively, per subset split\n",
    "                        proj_split = torch.matmul(eigvec_class_split_hth.view(m_eigvec_class_split,\n",
    "                                                                              density_nrow_ncol, 1),\n",
    "                                                  eigvec_class_split_hth.view(m_eigvec_class_split,\n",
    "                                                                              1, density_nrow_ncol))\n",
    "                        \n",
    "                        # Calculate sum of projectors\n",
    "                        proj_split_sum = proj_split.sum(dim = 0)\n",
    "                        return proj_split_sum\n",
    "                    \n",
    "                    # Determine length of eigvec_class_split_arr\n",
    "                    eigvec_class_split_arr_len = len(eigvec_class_split_arr)\n",
    "                    \n",
    "                    # Initialize tensor proj_class_sum\n",
    "                    proj_class_sum = torch.zeros([density_nrow_ncol, density_nrow_ncol],\n",
    "                                                 dtype = self.dtype).cuda()\n",
    "                    for h in range(eigvec_class_split_arr_len):\n",
    "                        # Calculate sum of the projectors corresponding to positive and negative eigenvalues\n",
    "                        # respectively\n",
    "                        proj_class_sum = proj_class_sum + eigvec_class_split_func(h)\n",
    "                    return proj_class_sum\n",
    "                \n",
    "                # Calculate sum of the projectors corresponding to positive and negative eigenvalues\n",
    "                # respectively\n",
    "                self.proj_sums_ = torch.stack([sum_proj_func(0), sum_proj_func(1)], dim = 0)\n",
    "                \n",
    "                # Calculate Helstrom bound\n",
    "                self.hels_bound_ = (qcentroids_terms[0][0]/m)*torch.einsum('ij,ji->', self.qcentroids_[0],\n",
    "                                                                           self.proj_sums_[0]).item() \\\n",
    "                                   + (qcentroids_terms[1][0]/m)*torch.einsum('ij,ji->', self.qcentroids_[1],\n",
    "                                                                             self.proj_sums_[1]).item()\n",
    "            # When Pretty Good Measurement or Helstrom measurement is misspecified\n",
    "            else:\n",
    "                raise ValueError('measure should be \"pgm\" or \"hels\"')\n",
    "        return self\n",
    "\n",
    "           \n",
    "    # Function for predict_proba\n",
    "    def predict_proba(self, X):\n",
    "        \"\"\"Performs PMG-HQC classification on X and returns the trace of the dot product of the \n",
    "        densities and the POV (positive operator-valued) measure, i.e. the class probabilities.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like, shape (n_samples, n_features)\n",
    "            The input samples. An array of int or float.       \n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        trace_matrix : array-like, shape (n_samples, n_classes)\n",
    "            Each column corresponds to the trace of the dot product of the densities and the POV \n",
    "            (positive operator-valued) measure for each class, i.e. each column corresponds to the \n",
    "            class probabilities. An array of float.\n",
    "        \"\"\"\n",
    "        # Send tensors self.pgms_ and self.proj_sums_ from GPU to CPU and cast into an array, and\n",
    "        # check if fit had been called\n",
    "        if self.measure == 'pgm':\n",
    "            self.pgms_arr_ = self.pgms_.cpu().numpy()\n",
    "            check_is_fitted(self, ['pgms_arr_'])\n",
    "        else:\n",
    "            self.proj_sums_arr_ = self.proj_sums_.cpu().numpy()\n",
    "            check_is_fitted(self, ['proj_sums_arr_'])\n",
    "               \n",
    "        # Check data in X as required by scikit-learn v0.25\n",
    "        X = self._validate_data(X, reset = False)\n",
    "                 \n",
    "        # Cast array X into a floating point tensor to ensure all following calculations below  \n",
    "        # are done in float rather than integer, and send tensor X from CPU to GPU\n",
    "        X = torch.tensor(X, dtype = self.dtype).cuda()\n",
    "        \n",
    "        # Rescale X\n",
    "        X = self.rescale*X        \n",
    "        \n",
    "        # Calculate sum of squares of each row (sample) in X\n",
    "        X_sq_sum = (X**2).sum(dim = 1)\n",
    "        \n",
    "        # Number of rows in X\n",
    "        m = X.shape[0]\n",
    "        \n",
    "        # Number of columns in X\n",
    "        n = X.shape[1]\n",
    "\n",
    "        # Calculate X' using amplitude or inverse of the standard stereographic projection \n",
    "        # encoding method\n",
    "        if self.encoding == 'amplit':\n",
    "            X_prime = normalize(torch.cat([X, torch.ones(m, dtype = self.dtype) \\\n",
    "                                           .reshape(-1, 1).cuda()], dim = 1), p = 2, dim = 1)\n",
    "        elif self.encoding == 'stereo':\n",
    "            X_prime = (1/(X_sq_sum + 1)).reshape(-1, 1) \\\n",
    "                      *(torch.cat((2*X, (X_sq_sum - 1).reshape(-1, 1)), dim = 1))\n",
    "        else:\n",
    "            raise ValueError('encoding should be \"amplit\" or \"stereo\"')\n",
    "                       \n",
    "        # Function to calculate trace values for each class\n",
    "        def trace_func(i):\n",
    "            # Split X' into n_splits subsets, row-wise\n",
    "            # Send tensors from GPU to CPU and cast tensors into arrays, use np.array_split()\n",
    "            # because the equivalent torch.chunk() doesn't behave similarly to np.array_split()\n",
    "            X_prime_split_arr_full = np.array_split(X_prime.cpu().numpy(),\n",
    "                                                    indices_or_sections = self.n_splits,\n",
    "                                                    axis = 0)\n",
    "            \n",
    "            # Remove empty rows in X_prime_split_arr_full\n",
    "            X_prime_split_arr = [a for a in X_prime_split_arr_full if a.shape[0] > 0]\n",
    "\n",
    "            # Cast arrays back to tensors and send back from CPU to GPU\n",
    "            X_prime_split = [torch.tensor(q, dtype = self.dtype).cuda() for q in X_prime_split_arr]\n",
    "            \n",
    "            # Function to calculate trace values for each class, per subset split\n",
    "            def trace_split_func(j):\n",
    "                # Counter for j-th split X'\n",
    "                X_prime_split_jth = X_prime_split[j]\n",
    "                \n",
    "                # Number of rows (samples) in j-th split X'\n",
    "                X_prime_split_m = X_prime_split_jth.shape[0]\n",
    "                \n",
    "                # Encode vectors into quantum densities\n",
    "                density_chunk = torch.matmul(X_prime_split_jth.view(X_prime_split_m, n_prime, 1),\n",
    "                                             X_prime_split_jth.view(X_prime_split_m, 1, n_prime))\n",
    "                \n",
    "                # Calculate n-fold Kronecker tensor product\n",
    "                if self.n_copies == 1:\n",
    "                    density_chunk = density_chunk\n",
    "                else:\n",
    "                    density_chunk_copy = density_chunk\n",
    "                    for b in range(self.n_copies - 1):\n",
    "                        density_chunk = kronecker(density_chunk, density_chunk_copy)\n",
    "                        \n",
    "                # When Pretty Good Measurement is specified\n",
    "                if self.measure == 'pgm':\n",
    "                    # Calculate trace of the dot product of density of each row and Pretty Good\n",
    "                    # Measurement\n",
    "                    trace_class_split = torch.einsum('bij,ji->b', density_chunk, self.pgms_[i])\n",
    "                # When Helstrom measurement is specified\n",
    "                else:\n",
    "                    # Calculate trace of the dot product of density of each row and sum of \n",
    "                    # projectors with corresponding positive and negative eigenvalues respectively\n",
    "                    trace_class_split = torch.einsum('bij,ji->b', density_chunk, self.proj_sums_[i])\n",
    "                return trace_class_split\n",
    "            \n",
    "            # Determine length of X_prime_split_arr\n",
    "            X_prime_split_arr_len = len(X_prime_split_arr)\n",
    "\n",
    "            # Initialize tensor trace_class\n",
    "            trace_class = torch.empty([0], dtype = self.dtype).cuda()\n",
    "            for j in range(X_prime_split_arr_len):\n",
    "                # Calculate trace values for each class, per subset split\n",
    "                trace_class = torch.cat([trace_class, trace_split_func(j)], dim = 0)\n",
    "            return trace_class\n",
    "        \n",
    "        # Calculate trace values for each class, send from GPU to CPU and cast into an array\n",
    "        trace_matrix = torch.stack([trace_func(i) for i in range(num_classes)], dim = 1).cpu().numpy()\n",
    "        return trace_matrix\n",
    "                \n",
    "    \n",
    "    # Function for predict\n",
    "    def predict(self, X):\n",
    "        \"\"\"Performs PGM-HQC classification on X and returns the classes.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like, shape (n_samples, n_features)\n",
    "            The input samples. An array of int or float.\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        self.classes_[predict_trace_index] : array-like, shape (n_samples,)\n",
    "            The predicted binary classes. An array of str, int or float.\n",
    "        \"\"\"\n",
    "        # Determine column index with the higher trace value in trace_matrix\n",
    "        # Cast predict_proba(X) from an array into a tensor and send from CPU to GPU\n",
    "        # If both columns have the same trace value, returns column index 1, which is different \n",
    "        # to np.argmax() which returns column index 0\n",
    "        predict_trace_index = torch.argmax(torch.tensor(self.predict_proba(X), \n",
    "                                                        dtype = self.dtype).cuda(), axis = 1)\n",
    "        # Returns the predicted binary classes, send tensor from GPU to CPU and cast tensor\n",
    "        # into an array\n",
    "        return self.classes_[predict_trace_index.cpu().numpy()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UB2dg2zPWgju"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "id": "mkrUhDg5WgkA"
   },
   "outputs": [],
   "source": [
    "# appendicitis dataset (7 features, 106 rows)\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('appendicitis.tsv',delimiter='\\t')\n",
    "X = df.drop('target', axis=1).values\n",
    "y = df['target'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vugqLw0CMcOA",
    "outputId": "24d206e0-85c0-4cea-b45d-8133d29fb740"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    80.188679\n",
       "1    19.811321\n",
       "Name: target, dtype: float64"
      ]
     },
     "execution_count": 65,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check if class imbalance\n",
    "df['target'].value_counts(normalize=True)*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "id": "yMyOB_wOMpue"
   },
   "outputs": [],
   "source": [
    "from sklearn import model_selection\n",
    "X_train, X_test, y_train, y_test = model_selection.train_test_split(X, y, test_size=0.2, random_state=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PizRcXdiWgkN",
    "outputId": "b7fb0886-59cc-4026-c0df-fd1d982a4de9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.7520661157024794, 0.8772541532913843)"
      ]
     },
     "execution_count": 67,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check F1 score and Helstrom bound values for various rescale and n_copies values\n",
    "model = PGMHQC_gpu_dtype(rescale=0.5, n_copies=3, encoding='stereo', measure='hels', class_wgt='weighted', n_splits=1, dtype=torch.float32).fit(X_train, y_train)\n",
    "y_hat = model.predict(X_test)\n",
    "\n",
    "from sklearn import metrics\n",
    "metrics.f1_score(y_test, y_hat, average='weighted'), model.hels_bound_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "id": "UnBkJCP3hktL"
   },
   "outputs": [],
   "source": [
    "# Testing using scikit-learn's GridSearchCV\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Create rescale hyperparamter list [0.1, 0.5, 1, 1.5,...,10.0]\n",
    "rescale_list1 = [0.1]\n",
    "rescale_list2 = np.linspace(0.5, 10, 20).tolist()\n",
    "rescale_list1.extend(rescale_list2)\n",
    "\n",
    "param_grid = {'rescale':rescale_list1, 'n_copies':[1, 2, 3, 4], 'encoding':['amplit', 'stereo'], 'measure':['hels'], 'class_wgt':['equi', 'weighted']}\n",
    "models = GridSearchCV(PGMHQC_gpu_dtype(n_splits=30, dtype=torch.float32), param_grid, scoring='f1_weighted').fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "S41M8lCYhmVD",
    "outputId": "218aa883-296b-4856-aa21-e0deaa7e6e9f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7676517031355741"
      ]
     },
     "execution_count": 70,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Best F1 score\n",
    "best_model = models.best_estimator_\n",
    "y_hat = best_model.predict(X_test)\n",
    "\n",
    "from sklearn import metrics\n",
    "metrics.f1_score(y_test, y_hat, average='weighted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eEJPGYtihsaQ",
    "outputId": "f130a359-dfc9-4e2c-b72a-3785e5ce35a6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'class_wgt': 'equi',\n",
       " 'encoding': 'amplit',\n",
       " 'measure': 'hels',\n",
       " 'n_copies': 1,\n",
       " 'rescale': 9.0}"
      ]
     },
     "execution_count": 71,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Best hyperparameter combination\n",
    "models.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OodE6n_Y2Dc1",
    "outputId": "1436bb10-a6b7-4b5e-b0c8-b2a13ea222d4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[86.66666667, 13.33333333],\n",
       "       [42.85714286, 57.14285714]])"
      ]
     },
     "execution_count": 72,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "confusion_matrix(y_test, y_hat, normalize='true')*100\n",
    "# 86.66666667% from 1st class correctly predicted\n",
    "# 57.14285714% from 2nd class correctly predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "l2l_WACvWgkk"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "id": "fBRcX1lyWgky"
   },
   "outputs": [],
   "source": [
    "# banana dataset (2 features, 5300 rows)\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('banana.tsv', sep='\\t')\n",
    "X = df.drop('target', axis=1).values\n",
    "y = df['target'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wcgbphhSNTkf",
    "outputId": "8ca9e8c9-9cb9-4029-b26c-cf70880e2443"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1.0    55.169811\n",
       " 1.0    44.830189\n",
       "Name: target, dtype: float64"
      ]
     },
     "execution_count": 74,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check if class imbalance\n",
    "df['target'].value_counts(normalize=True)*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "id": "UeEpmqSHNToZ"
   },
   "outputs": [],
   "source": [
    "from sklearn import model_selection\n",
    "X_train, X_test, y_train, y_test = model_selection.train_test_split(X, y, test_size=0.2, random_state=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vb8GrFW0Wgk_",
    "outputId": "a635223d-71ff-4681-9744-19e940ecf105"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.858978398722441, 0.7732936040410455)"
      ]
     },
     "execution_count": 20,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check F1 score and Helstrom bound values for various rescale and n_copies values\n",
    "model = PGMHQC_gpu_dtype(rescale=0.5, n_copies=4, encoding='stereo', measure='hels', class_wgt='weighted', n_splits=1, dtype=torch.float32).fit(X_train, y_train)\n",
    "y_hat = model.predict(X_test)\n",
    "\n",
    "from sklearn import metrics\n",
    "metrics.f1_score(y_test, y_hat, average='weighted'), model.hels_bound_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "id": "94HRAyPirUjt"
   },
   "outputs": [],
   "source": [
    "# Testing using scikit-learn's GridSearchCV\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Create rescale hyperparamter list [0.1, 0.5, 1, 1.5,...,10.0]\n",
    "rescale_list1 = [0.1]\n",
    "rescale_list2 = np.linspace(0.5, 10, 20).tolist()\n",
    "rescale_list1.extend(rescale_list2)\n",
    "\n",
    "param_grid = {'rescale':rescale_list1, 'n_copies':[1, 2, 3, 4], 'encoding':['amplit', 'stereo'], 'measure':['hels'], 'class_wgt':['equi', 'weighted']}\n",
    "models = GridSearchCV(PGMHQC_gpu_dtype(n_splits=1, dtype=torch.float32), param_grid, scoring='f1_weighted').fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yMWcaEgCrUZ2",
    "outputId": "9b7d4e83-b97e-4f71-bf63-0359d93e99df"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8718953078333712"
      ]
     },
     "execution_count": 79,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Best F1 score\n",
    "best_model = models.best_estimator_\n",
    "y_hat = best_model.predict(X_test)\n",
    "\n",
    "from sklearn import metrics\n",
    "metrics.f1_score(y_test, y_hat, average='weighted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NsmLYHKWrb9E",
    "outputId": "70d37796-4b46-416b-8361-b8958c165856"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'class_wgt': 'equi',\n",
       " 'encoding': 'stereo',\n",
       " 'measure': 'hels',\n",
       " 'n_copies': 4,\n",
       " 'rescale': 0.5}"
      ]
     },
     "execution_count": 80,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Best hyperparameter combination\n",
    "models.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TApkp-mo4DTO",
    "outputId": "8237cde1-fc36-4d58-925e-40bfe5a93627"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[94.92385787,  5.07614213],\n",
       "       [22.17484009, 77.82515991]])"
      ]
     },
     "execution_count": 82,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "confusion_matrix(y_test, y_hat, normalize='true')*100\n",
    "# 94.92385787% from 1st class correctly predicted\n",
    "# 77.82515991% from 2nd class correctly predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7xrzaQkyWglR"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "R8WD6wlIWglc"
   },
   "outputs": [],
   "source": [
    "# iris dataset (5 features, 150 rows)\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('iris.tsv', sep='\\t')\n",
    "X = df.drop('target', axis=1).values\n",
    "y = df['target'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LC9h58SwWgl2",
    "outputId": "52a34107-7f6c-42ed-c4ba-7f5b047e4989"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2    33.333333\n",
       "1    33.333333\n",
       "0    33.333333\n",
       "Name: target, dtype: float64"
      ]
     },
     "execution_count": 36,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check if class imbalance\n",
    "df['target'].value_counts(normalize=True)*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "S-nExohJWgmO",
    "outputId": "3ad93ca8-9534-48f9-fcf6-e54d840b0be5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.2714, 0.3513, 0.3773, 1.0000],\n",
       "        [0.2244, 0.3658, 0.4098, 1.0000],\n",
       "        [0.5722, 0.2603, 0.1675, 1.0000],\n",
       "        [0.2334, 0.3523, 0.4143, 1.0000],\n",
       "        [0.2178, 0.3588, 0.4234, 1.0000]], device='cuda:0')"
      ]
     },
     "execution_count": 24,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import model_selection\n",
    "X_train, X_test, y_train, y_test = model_selection.train_test_split(X, y, test_size=0.2, random_state=4)\n",
    "\n",
    "# Check trace values sum to 1 for first 5 rows\n",
    "model = PGMHQC_gpu_dtype(rescale=0.5, n_copies=1, encoding='stereo', measure='pgm', class_wgt=None, n_splits=1, dtype=torch.float32).fit(X_train, y_train)\n",
    "torch.cat([model.predict_proba(X_test), torch.sum(model.predict_proba(X_test), dim=1).reshape(-1,1)], dim=1)[0:5,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "898ubyWnWgmc",
    "outputId": "6383cf27-4b76-48d9-9499-ba723ed5767a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8940170940170941"
      ]
     },
     "execution_count": 25,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check F1 score for various n_copies values\n",
    "model = PGMHQC_gpu_dtype(rescale=0.5, n_copies=1, encoding='stereo', measure='pgm', class_wgt=None, n_splits=1, dtype=torch.float32).fit(X_train, y_train)\n",
    "y_hat = model.predict(X_test)\n",
    "\n",
    "from sklearn import metrics\n",
    "metrics.f1_score(y_test, y_hat, average='weighted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ycxxC68wWgmq",
    "outputId": "5990de33-83d8-4183-dbec-2fc506f3a895"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8940170940170941"
      ]
     },
     "execution_count": 26,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check F1 score for various n_copies values\n",
    "model = PGMHQC_gpu_dtype(rescale=0.5, n_copies=2, encoding='stereo', measure='pgm', class_wgt=None, n_splits=1, dtype=torch.float32).fit(X_train, y_train)\n",
    "y_hat = model.predict(X_test)\n",
    "\n",
    "from sklearn import metrics\n",
    "metrics.f1_score(y_test, y_hat, average='weighted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "N4-pemXHWgnE",
    "outputId": "5a8870a7-1508-46db-837a-7e0f36c37706"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8940170940170941"
      ]
     },
     "execution_count": 27,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check F1 score for various n_copies values\n",
    "model = PGMHQC_gpu_dtype(rescale=0.5, n_copies=3, encoding='stereo', measure='pgm', class_wgt=None, n_splits=1, dtype=torch.float32).fit(X_train, y_train)\n",
    "y_hat = model.predict(X_test)\n",
    "\n",
    "from sklearn import metrics\n",
    "metrics.f1_score(y_test, y_hat, average='weighted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6q_Qdg-9WgnQ",
    "outputId": "09e91c85-191f-41e9-9b7e-307ef656c287"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8940170940170941"
      ]
     },
     "execution_count": 28,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check F1 score for various n_copies values\n",
    "model = PGMHQC_gpu_dtype(rescale=0.5, n_copies=4, encoding='stereo', measure='pgm', class_wgt=None, n_splits=1, dtype=torch.float32).fit(X_train, y_train)\n",
    "y_hat = model.predict(X_test)\n",
    "\n",
    "from sklearn import metrics\n",
    "metrics.f1_score(y_test, y_hat, average='weighted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KtL0AvFJWgno",
    "outputId": "5ac65fd3-2087-4ddc-e457-edfe0f644a3d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8940170940170941"
      ]
     },
     "execution_count": 29,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check F1 score for various n_copies values\n",
    "model = PGMHQC_gpu_dtype(rescale=0.5, n_copies=5, encoding='stereo', measure='pgm', class_wgt=None, n_splits=1, dtype=torch.float32).fit(X_train, y_train)\n",
    "y_hat = model.predict(X_test)\n",
    "\n",
    "from sklearn import metrics\n",
    "metrics.f1_score(y_test, y_hat, average='weighted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "AcwVrvb8Wgn2"
   },
   "outputs": [],
   "source": [
    "# Testing using scikit-learn's GridSearchCV\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Create rescale hyperparamter list [0.1, 0.5, 1, 1.5,...,10.0]\n",
    "rescale_list1 = [0.1]\n",
    "rescale_list2 = np.linspace(0.5, 10, 20).tolist()\n",
    "rescale_list1.extend(rescale_list2)\n",
    "\n",
    "param_grid = {'rescale':rescale_list1, 'n_copies':[1, 2, 3, 4], 'encoding':['amplit', 'stereo'], 'measure':['pgm'], 'class_wgt':[None]}\n",
    "models = GridSearchCV(PGMHQC_gpu_dtype(n_splits=1, dtype=torch.float32), param_grid, scoring='f1_weighted').fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xPrF5OC9WgoU",
    "outputId": "1182551a-e8ca-476f-a5a7-70e9468b7414"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 31,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Best F1 score\n",
    "best_model = models.best_estimator_\n",
    "y_hat = best_model.predict(X_test)\n",
    "\n",
    "from sklearn import metrics\n",
    "metrics.f1_score(y_test, y_hat, average='weighted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7-zjELrzWgoo",
    "outputId": "b4b084ef-16a8-46da-eca3-c086f67555ac"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'class_wgt': None,\n",
       " 'encoding': 'stereo',\n",
       " 'measure': 'pgm',\n",
       " 'n_copies': 4,\n",
       " 'rescale': 0.1}"
      ]
     },
     "execution_count": 32,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Best hyperparameter combination\n",
    "models.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EimYw0QoQVha",
    "outputId": "d37a98cc-1283-49bb-b9c1-0d4733f42efb"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[100.,   0.,   0.],\n",
       "       [  0., 100.,   0.],\n",
       "       [  0.,   0., 100.]])"
      ]
     },
     "execution_count": 33,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "confusion_matrix(y_test, y_hat, normalize='true')*100\n",
    "# 100% from 1st class correctly predicted\n",
    "# 100% from 2nd class correctly predicted\n",
    "# 100% from 3rd class correctly predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "V-EcGpP4Wgo0"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "YfUGySUYWgpA"
   },
   "outputs": [],
   "source": [
    "# balance-scale dataset (5 features, 625 rows)\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('balance-scale.tsv', sep='\\t')\n",
    "X = df.drop('target', axis=1).values\n",
    "y = df['target'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cJW2XvHGWgpe",
    "outputId": "508ae742-4d73-4a65-dca3-588323013c9b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2    46.08\n",
       "1    46.08\n",
       "0     7.84\n",
       "Name: target, dtype: float64"
      ]
     },
     "execution_count": 38,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check if class imbalance\n",
    "df['target'].value_counts(normalize=True)*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gXZVf50lWgpu",
    "outputId": "080bc818-e0a5-46f2-8f7f-354f361e6cad"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.3146, 0.3285, 0.3570, 1.0001],\n",
       "        [0.3155, 0.3281, 0.3566, 1.0001],\n",
       "        [0.3153, 0.3429, 0.3419, 1.0000],\n",
       "        [0.3230, 0.3163, 0.3607, 1.0000],\n",
       "        [0.3167, 0.3418, 0.3415, 1.0000]], device='cuda:0')"
      ]
     },
     "execution_count": 39,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import model_selection\n",
    "X_train, X_test, y_train, y_test = model_selection.train_test_split(X, y, test_size=0.2, random_state=4)\n",
    "\n",
    "# Check trace values sum to 1 for first 5 rows\n",
    "model = PGMHQC_gpu_dtype(rescale=1, n_copies=2, encoding='stereo', measure='pgm', class_wgt=None, n_splits=1, dtype=torch.float32).fit(X_train, y_train)\n",
    "torch.cat([model.predict_proba(X_test), torch.sum(model.predict_proba(X_test), dim=1).reshape(-1,1)], dim=1)[0:5,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BP3ltHOwWgp8",
    "outputId": "6af064a0-6829-432a-ae19-2b3efa2d391e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8367315008833559"
      ]
     },
     "execution_count": 40,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check F1 score for various n_copies values\n",
    "model = PGMHQC_gpu_dtype(rescale=1, n_copies=1, encoding='stereo', measure='pgm', class_wgt=None, n_splits=1, dtype=torch.float32).fit(X_train, y_train)\n",
    "y_hat = model.predict(X_test)\n",
    "\n",
    "from sklearn import metrics\n",
    "metrics.f1_score(y_test, y_hat, average='weighted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "y6KDcNygWgqj",
    "outputId": "434f1958-19b4-4f6a-c9cc-77df859e3704"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8447445962196664"
      ]
     },
     "execution_count": 41,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check F1 score for various n_copies values\n",
    "model = PGMHQC_gpu_dtype(rescale=1, n_copies=2, encoding='stereo', measure='pgm', class_wgt=None, n_splits=1, dtype=torch.float32).fit(X_train, y_train)\n",
    "y_hat = model.predict(X_test)\n",
    "\n",
    "from sklearn import metrics\n",
    "metrics.f1_score(y_test, y_hat, average='weighted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3L4jnNlFWgqt",
    "outputId": "12258d8c-5383-4bfc-a2a7-3a7b323dbf5a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8605017010473518"
      ]
     },
     "execution_count": 42,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check F1 score for various n_copies values\n",
    "model = PGMHQC_gpu_dtype(rescale=1, n_copies=3, encoding='stereo', measure='pgm', class_wgt=None, n_splits=1, dtype=torch.float32).fit(X_train, y_train)\n",
    "y_hat = model.predict(X_test)\n",
    "\n",
    "from sklearn import metrics\n",
    "metrics.f1_score(y_test, y_hat, average='weighted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pp2IKkYtWgrs",
    "outputId": "dbe1b481-07f4-45ce-a136-2709735c3127"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8678604178430266"
      ]
     },
     "execution_count": 43,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### SCORE IS SLIGHTLY DIFFERENT BETWEEN FLOAT32 AND FLOAT64 (0.8676190476190477 vs. 0.8682253173918403, respectively) ###\n",
    "# Check F1 score for various n_copies values\n",
    "model = PGMHQC_gpu_dtype(rescale=1, n_copies=4, encoding='stereo', measure='pgm', class_wgt=None, n_splits=1, dtype=torch.float32).fit(X_train, y_train)\n",
    "y_hat = model.predict(X_test)\n",
    "\n",
    "from sklearn import metrics\n",
    "metrics.f1_score(y_test, y_hat, average='weighted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pVHpR8LKWgr4",
    "outputId": "f9ac7ba5-0816-4bb9-e3f9-9e15dde3728c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8678604178430266"
      ]
     },
     "execution_count": 44,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check F1 score for various n_copies values\n",
    "model = PGMHQC_gpu_dtype(rescale=1, n_copies=5, encoding='stereo', measure='pgm', class_wgt=None, n_splits=50, dtype=torch.float32).fit(X_train, y_train)\n",
    "y_hat = model.predict(X_test)\n",
    "\n",
    "from sklearn import metrics\n",
    "metrics.f1_score(y_test, y_hat, average='weighted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "id": "TefpoXjmWgsA"
   },
   "outputs": [],
   "source": [
    "# Testing using scikit-learn's GridSearchCV\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Create rescale hyperparamter list [0.1, 0.5, 1, 1.5,...,10.0]\n",
    "rescale_list1 = [0.1]\n",
    "rescale_list2 = np.linspace(0.5, 10, 20).tolist()\n",
    "rescale_list1.extend(rescale_list2)\n",
    "\n",
    "param_grid = {'rescale':rescale_list1, 'n_copies':[1, 2, 3, 4], 'encoding':['amplit', 'stereo'], 'measure':['pgm'], 'class_wgt':[None]}\n",
    "models = GridSearchCV(PGMHQC_gpu_dtype(n_splits=1, dtype=torch.float32), param_grid, scoring='f1_weighted').fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dPg7dxprWgsO",
    "outputId": "265e885f-66bf-4692-b170-9568d0e98168"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9106013291733841"
      ]
     },
     "execution_count": 46,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Best F1 score\n",
    "best_model = models.best_estimator_\n",
    "y_hat = best_model.predict(X_test)\n",
    "\n",
    "from sklearn import metrics\n",
    "metrics.f1_score(y_test, y_hat, average='weighted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "njzJZhG4WgsX",
    "outputId": "78a2cdcb-061e-4736-9206-6cda30d2177b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'class_wgt': None,\n",
       " 'encoding': 'amplit',\n",
       " 'measure': 'pgm',\n",
       " 'n_copies': 2,\n",
       " 'rescale': 2.0}"
      ]
     },
     "execution_count": 47,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Best hyperparameter combination\n",
    "models.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "miNaWClfSYsP",
    "outputId": "b1795116-3a38-4a8f-ceb8-763c5856eac3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[88.88888889,  0.        , 11.11111111],\n",
       "       [10.34482759, 87.93103448,  1.72413793],\n",
       "       [ 8.62068966,  0.        , 91.37931034]])"
      ]
     },
     "execution_count": 48,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "confusion_matrix(y_test, y_hat, normalize='true')*100\n",
    "# 88.88888889% from 1st class correctly predicted\n",
    "# 87.93103448% from 2nd class correctly predicted\n",
    "# 91.37931034% from 3rd class correctly predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mqKy0QoDWgso"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "id": "YYtnAVSwWgs2"
   },
   "outputs": [],
   "source": [
    "# new-thyroid dataset (6 features, 215 rows)\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('new-thyroid.tsv', sep='\\t')\n",
    "X = df.drop('target', axis=1).values\n",
    "y = df['target'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NLAbg8bXWgtE",
    "outputId": "1109e775-d7c2-4b74-e69f-095acc3e17d9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    69.767442\n",
       "2    16.279070\n",
       "3    13.953488\n",
       "Name: target, dtype: float64"
      ]
     },
     "execution_count": 50,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check if class imbalance\n",
    "df['target'].value_counts(normalize=True)*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DaMSnZsDWgtM",
    "outputId": "776dc637-10c7-4afe-e842-729292831774"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.3336, 0.3327, 0.3335, 0.9999],\n",
       "        [0.3313, 0.3301, 0.3385, 0.9999],\n",
       "        [0.3338, 0.3325, 0.3336, 0.9999],\n",
       "        [0.3338, 0.3326, 0.3335, 0.9999],\n",
       "        [0.3339, 0.3329, 0.3331, 0.9999]], device='cuda:0')"
      ]
     },
     "execution_count": 51,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import model_selection\n",
    "X_train, X_test, y_train, y_test = model_selection.train_test_split(X, y, test_size=0.2, random_state=4)\n",
    "\n",
    "# Check trace values sum to 1 for first 5 rows\n",
    "model = PGMHQC_gpu_dtype(rescale=1.5, n_copies=3, encoding='stereo', measure='pgm', class_wgt=None, n_splits=1, dtype=torch.float32).fit(X_train, y_train)\n",
    "torch.cat([model.predict_proba(X_test), torch.sum(model.predict_proba(X_test), dim=1).reshape(-1,1)], dim=1)[0:5,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8ZmBz6ilWgtq",
    "outputId": "afb34145-2fb7-4135-d1d9-6078f2d373e6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8549863029667192"
      ]
     },
     "execution_count": 52,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check F1 score for various n_copies values\n",
    "model = PGMHQC_gpu_dtype(rescale=1.5, n_copies=1, encoding='stereo', measure='pgm', class_wgt=None, n_splits=1, dtype=torch.float32).fit(X_train, y_train)\n",
    "y_hat = model.predict(X_test)\n",
    "\n",
    "from sklearn import metrics\n",
    "metrics.f1_score(y_test, y_hat, average='weighted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "k0zmghdjWgt0",
    "outputId": "d1270fcb-67fd-495b-920e-3fb8fac64b72"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8549863029667192"
      ]
     },
     "execution_count": 53,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check F1 score for various n_copies values\n",
    "model = PGMHQC_gpu_dtype(rescale=1.5, n_copies=2, encoding='stereo', measure='pgm', class_wgt=None, n_splits=1, dtype=torch.float32).fit(X_train, y_train)\n",
    "y_hat = model.predict(X_test)\n",
    "\n",
    "from sklearn import metrics\n",
    "metrics.f1_score(y_test, y_hat, average='weighted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "x2-y_eLYWguB",
    "outputId": "8daf31a7-cda5-4531-b3b9-8b1602ea46fc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8549863029667192"
      ]
     },
     "execution_count": 57,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### SCORE IS SLIGHTLY DIFFERENT BETWEEN FLOAT32 AND FLOAT64 (0.8314708547266686 vs. 0.8549863029667192, respectively) ###\n",
    "# Check F1 score for various n_copies values\n",
    "model = PGMHQC_gpu_dtype(rescale=1.5, n_copies=3, encoding='stereo', measure='pgm', class_wgt=None, n_splits=1, dtype=torch.float32).fit(X_train, y_train)\n",
    "y_hat = model.predict(X_test)\n",
    "\n",
    "from sklearn import metrics\n",
    "metrics.f1_score(y_test, y_hat, average='weighted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Yys-sUwkWguK",
    "outputId": "2c439b33-a119-4149-92ca-25b7af39dad9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8549863029667192"
      ]
     },
     "execution_count": 58,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check F1 score for various n_copies values\n",
    "model = PGMHQC_gpu_dtype(rescale=1.5, n_copies=4, encoding='stereo', measure='pgm', class_wgt=None, n_splits=1, dtype=torch.float32).fit(X_train, y_train)\n",
    "y_hat = model.predict(X_test)\n",
    "\n",
    "from sklearn import metrics\n",
    "metrics.f1_score(y_test, y_hat, average='weighted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UzPAHJD4Wgub",
    "outputId": "ed12dc07-606e-4ff5-d7cc-a4946298eff4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8549863029667192"
      ]
     },
     "execution_count": 59,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check F1 score for various n_copies values\n",
    "# 150+35+30 = 215\n",
    "model = PGMHQC_gpu_dtype(rescale=1.5, n_copies=5, encoding='stereo', measure='pgm', class_wgt=None, n_splits=100, dtype=torch.float32).fit(X_train, y_train)\n",
    "y_hat = model.predict(X_test)\n",
    "\n",
    "from sklearn import metrics\n",
    "metrics.f1_score(y_test, y_hat, average='weighted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "id": "fYDF5waDWguk"
   },
   "outputs": [],
   "source": [
    "# Testing using scikit-learn's GridSearchCV\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Create rescale hyperparamter list [0.1, 0.5, 1, 1.5,...,10.0]\n",
    "rescale_list1 = [0.1]\n",
    "rescale_list2 = np.linspace(0.5, 10, 20).tolist()\n",
    "rescale_list1.extend(rescale_list2)\n",
    "\n",
    "# 150+35+30 = 215\n",
    "param_grid = {'rescale':rescale_list1, 'n_copies':[1, 2, 3, 4], 'encoding':['amplit', 'stereo'], 'measure':['pgm'], 'class_wgt':[None]}\n",
    "models = GridSearchCV(PGMHQC_gpu_dtype(n_splits=1, dtype=torch.float32), param_grid, scoring='f1_weighted').fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DyZHsQFFWgvS",
    "outputId": "272f744f-130e-4605-e636-754d963e02d3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9516311369509043"
      ]
     },
     "execution_count": 61,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Best F1 score\n",
    "best_model = models.best_estimator_\n",
    "y_hat = best_model.predict(X_test)\n",
    "\n",
    "from sklearn import metrics\n",
    "metrics.f1_score(y_test, y_hat, average='weighted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RNYt1QrUWgva",
    "outputId": "c25e2847-670c-4762-d3f6-d23c3334b8ad"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'class_wgt': None,\n",
       " 'encoding': 'amplit',\n",
       " 'measure': 'pgm',\n",
       " 'n_copies': 2,\n",
       " 'rescale': 0.1}"
      ]
     },
     "execution_count": 62,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Best hyperparameter combination\n",
    "models.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "q91xy2t-prtV",
    "outputId": "336daa6b-997f-48c3-f202-ee16a80918d3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[100.,   0.,   0.],\n",
       "       [  0., 100.,   0.],\n",
       "       [ 20.,   0.,  80.]])"
      ]
     },
     "execution_count": 63,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "confusion_matrix(y_test, y_hat, normalize='true')*100\n",
    "# 100% from 1st class correctly predicted\n",
    "# 100% from 2nd class correctly predicted\n",
    "# 80% from 3rd class correctly predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jziu-wfLe9sO"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "PGM-HQC-gpu-dtype.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
