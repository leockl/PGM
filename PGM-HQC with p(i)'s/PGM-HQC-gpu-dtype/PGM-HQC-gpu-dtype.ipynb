{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Z79t6LylqcKa",
    "outputId": "ce375dd1-49fa-4d37-9494-8f18bd97ccfe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting scikit-learn\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/5c/a1/273def87037a7fb010512bbc5901c31cfddfca8080bc63b42b26e3cc55b3/scikit_learn-0.23.2-cp36-cp36m-manylinux1_x86_64.whl (6.8MB)\n",
      "\u001b[K     |████████████████████████████████| 6.8MB 15.4MB/s \n",
      "\u001b[?25hRequirement already satisfied, skipping upgrade: numpy>=1.13.3 in /usr/local/lib/python3.6/dist-packages (from scikit-learn) (1.18.5)\n",
      "Requirement already satisfied, skipping upgrade: scipy>=0.19.1 in /usr/local/lib/python3.6/dist-packages (from scikit-learn) (1.4.1)\n",
      "Collecting threadpoolctl>=2.0.0\n",
      "  Downloading https://files.pythonhosted.org/packages/f7/12/ec3f2e203afa394a149911729357aa48affc59c20e2c1c8297a60f33f133/threadpoolctl-2.1.0-py3-none-any.whl\n",
      "Requirement already satisfied, skipping upgrade: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn) (0.17.0)\n",
      "Installing collected packages: threadpoolctl, scikit-learn\n",
      "  Found existing installation: scikit-learn 0.22.2.post1\n",
      "    Uninstalling scikit-learn-0.22.2.post1:\n",
      "      Successfully uninstalled scikit-learn-0.22.2.post1\n",
      "Successfully installed scikit-learn-0.23.2 threadpoolctl-2.1.0\n"
     ]
    }
   ],
   "source": [
    "# Update scikit-learn to lastest version in Google Colab\n",
    "!pip install --upgrade scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UH-DU39Cq9vy"
   },
   "outputs": [],
   "source": [
    "### This is a GPU implementation for the HQC classifier using Scikit-learn's methods, but with PyTorch as the backend. ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4UJ8KeaCq9z3"
   },
   "outputs": [],
   "source": [
    "# I have implemented the code below in such a way that you would only need to input X and y as numpy arrays and the\n",
    "# output y_hat would also be a numpy array (rather than PyTorch tensors). This would make it easier to use the package\n",
    "# below with minimal knowledge of PyTorch tensors.\n",
    "\n",
    "# Take note of the parameter n_splits, where the implementation of n_splits now is different to the one in the CPU case.\n",
    "# Please read the description of n_splits below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "WwamW89CZLFE"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from sklearn.utils.validation import check_is_fitted\n",
    "from sklearn.utils.multiclass import check_classification_targets\n",
    "import torch\n",
    "from torch.nn.functional import normalize\n",
    "from scipy import linalg\n",
    "\n",
    "class PGMHQC_gpu_dtype(BaseEstimator, ClassifierMixin):\n",
    "    \"\"\"The Pretty Good Measurement (PGM) - Helstrom Quantum Centroid (HQC) classifier is a \n",
    "    quantum-inspired supervised classification approach for data with multiple classes.\n",
    "                         \n",
    "    Parameters\n",
    "    ----------\n",
    "    rescale : int or float, default = 1\n",
    "        The dataset rescaling factor. A parameter used for rescaling the dataset. \n",
    "    encoding : str, default = 'amplit'\n",
    "        The encoding method used to encode vectors into quantum densities. Possible values:\n",
    "        'amplit', 'stereo'. 'amplit' means using the amplitude encoding method. 'stereo' means \n",
    "        using the inverse of the standard stereographic projection encoding method. Default set \n",
    "        to 'amplit'.\n",
    "    n_copies : int, default = 1\n",
    "        The number of copies to take for each quantum density. This is equivalent to taking \n",
    "        the n-fold Kronecker tensor product for each quantum density.\n",
    "    measure : str, default = 'pgm'\n",
    "        The measurement used to distinguish between quantum states. Possible values: 'pgm', \n",
    "        'hels'. The value 'pgm' stands for \"Pretty Good Measurement\", 'hels' stands for \n",
    "        \"Helstrom measurement\" (applicable only for binary classification). Default set to \n",
    "        'pgm'. \n",
    "    class_weight : str, default = None        \n",
    "        Weights associated with classes. This is the class weights assigned to the quantum \n",
    "        centroids in the Pretty Good Measurement or Helstrom observable. Possible values: None,\n",
    "        'balanced'. If None given, all classes are supposed to have weight one. The 'balanced' \n",
    "        mode uses the values of y to automatically adjust weights inversely proportional to class\n",
    "        frequencies in the input data as n_samples / (n_classes * np.bincount(y)). Default set\n",
    "        to None.       \n",
    "    n_splits : int, default = 1\n",
    "        The number of subset splits performed on the input dataset row-wise and on the number \n",
    "        of eigenvalues/eigenvectors of the Quantum Helstrom observable for optimal speed \n",
    "        performance. If 1 is given, no splits are performed. For optimal speed, recommend \n",
    "        using small values as close to 1 as possible. If memory blow-out occurs, increase \n",
    "        n_splits.\n",
    "    dtype : torch.float32 or torch.float64, default = torch.float64\n",
    "        The float datatype used for the elements in the Pytorch tensor dataset. Datatype has to\n",
    "        be of float to ensure calculations are done in float rather than integer. To achieve\n",
    "        higher n_copies without memory blow-out issues, reduce float precision, which may or may   \n",
    "        not affect accuracy in a significant way.\n",
    "    \n",
    "    Attributes\n",
    "    ----------\n",
    "    classes_ : ndarray, shape (n_classes,)\n",
    "        Sorted classes.\n",
    "    qcentroids_ : ndarray, shape (n_classes, (n_features + 1)**n_copies, (n_features + 1)**n_copies)\n",
    "        Quantum Centroids for each class.\n",
    "    pgms_ : list, shape (n_classes, (n_features + 1)**n_copies, (n_features + 1)**n_copies)\n",
    "        Values for the Pretty Good Measurements. Only applicable when Pretty Good Measurement is \n",
    "        selected.\n",
    "    pgm_bound_ : float\n",
    "        Pretty Good Measurement bound is the upper bound on the probability that one can correctly\n",
    "        discriminate whether a quantum density is of which of the (multiclass) N quantum density \n",
    "        patterns. Only applicable when Pretty Good Measurement is selected.\n",
    "    proj_sums_ : list, shape (n_classes, (n_features + 1)**n_copies, (n_features + 1)**n_copies)\n",
    "        Sum of the projectors of the Quantum Helstrom observable's unit eigenvectors, which has\n",
    "        corresponding positive and negative eigenvalues respectively. Only applicable when Helstrom\n",
    "        Measurement is selected.\n",
    "    hels_bound_ : float\n",
    "        Helstrom bound is the upper bound on the probability that one can correctly \n",
    "        discriminate whether a quantum density is of which of the two binary quantum density \n",
    "        pattern. Only applicable when Helstrom Measurement is selected.         \n",
    "    \"\"\"   \n",
    "    # Initialize model hyperparameters\n",
    "    def __init__(self, \n",
    "                 rescale = 1,\n",
    "                 encoding = 'amplit',\n",
    "                 n_copies = 1,  \n",
    "                 measure = 'pgm',\n",
    "                 class_weight = None, \n",
    "                 n_splits = 1,\n",
    "                 dtype = torch.float64):\n",
    "        self.rescale = rescale\n",
    "        self.encoding = encoding\n",
    "        self.n_copies = n_copies\n",
    "        self.measure = measure\n",
    "        self.class_weight = class_weight\n",
    "        self.n_splits = n_splits\n",
    "        self.dtype = dtype\n",
    "        \n",
    "        # Raise error if dtype is not torch.float32 or torch.float64\n",
    "        if self.dtype not in [torch.float32, torch.float64]:\n",
    "            raise ValueError('dtype should be torch.float32 or torch.float64 only')\n",
    "        \n",
    "\n",
    "    # Function for X_prime, set as global function\n",
    "    global X_prime_func\n",
    "    def X_prime_func(self, X, m):\n",
    "        # Cast array X into a floating point tensor to ensure all following calculations below  \n",
    "        # are done in float rather than integer, and send tensor X from CPU to GPU\n",
    "        X = torch.tensor(X, dtype = self.dtype).cuda()\n",
    "        \n",
    "        # Rescale X\n",
    "        X = self.rescale*X\n",
    "        \n",
    "        # Calculate sum of squares of each row (sample) in X\n",
    "        X_sq_sum = (X**2).sum(dim = 1)\n",
    "        \n",
    "        # Calculate X' using amplitude or inverse of the standard stereographic projection \n",
    "        # encoding method\n",
    "        if self.encoding == 'amplit':\n",
    "            X_prime = normalize(torch.cat([X, torch.ones(m, dtype = self.dtype) \\\n",
    "                                           .reshape(-1, 1).cuda()], dim = 1), p = 2, dim = 1)\n",
    "        elif self.encoding == 'stereo':\n",
    "            X_prime = (1 / (X_sq_sum + 1)).reshape(-1, 1)*(torch.cat((2*X, (X_sq_sum - 1) \\\n",
    "                                                                      .reshape(-1, 1)), dim = 1))\n",
    "        else:\n",
    "            raise ValueError('encoding should be \"amplit\" or \"stereo\"')\n",
    "        return X_prime\n",
    "        \n",
    "        \n",
    "    # Function for kronecker tensor product for PyTorch tensors, set as global function\n",
    "    global kronecker\n",
    "    def kronecker(A, B):\n",
    "        return torch.einsum('nab,ncd->nacbd', A, B).view(A.size(0), \n",
    "                                                         A.size(1)*B.size(1), \n",
    "                                                         A.size(2)*B.size(2))\n",
    "    \n",
    "\n",
    "    # Set np.einsum subscripts (between unnested and nested objects) as a constant, set as global\n",
    "    # variable\n",
    "    global einsum_unnest, einsum_nest\n",
    "    einsum_unnest = 'ij,ji->'\n",
    "    einsum_nest = 'bij,ji->b'\n",
    "    \n",
    "    \n",
    "    # Function for fit\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"Perform PGM-HQC classification with the amplitude and inverse of the standard \n",
    "        stereographic projection encoding methods, with the option to rescale the dataset prior \n",
    "        to encoding.\n",
    "                \n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like, shape (n_samples, n_features)\n",
    "            The training input samples. An array of int or float.\n",
    "        y : array-like, shape (n_samples,)\n",
    "            The training input binary target values. An array of str, int or float.\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        self : object\n",
    "            Returns self.\n",
    "        \"\"\"\n",
    "        # Check data in X and y as required by scikit-learn v0.25\n",
    "        X, y = self._validate_data(X, y, reset = True)\n",
    "        \n",
    "        # Ensure target array y is of non-regression type  \n",
    "        # Added as required by sklearn check_estimator\n",
    "        check_classification_targets(y)\n",
    "            \n",
    "        # Store classes and encode y into class indexes\n",
    "        self.classes_, y_class_index = np.unique(y, return_inverse = True)\n",
    "        \n",
    "        # Number of classes, set as global variable\n",
    "        global num_classes\n",
    "        num_classes = len(self.classes_)\n",
    "        \n",
    "        # Raise error when there are more than 2 classes and Helstrom measurement is specified\n",
    "        if num_classes > 2 and self.measure == 'hels':\n",
    "            raise ValueError('Helstrom measurement can be applied for binary classification only')\n",
    "        else:\n",
    "            # Number of rows and columns in X\n",
    "            m, n = X.shape[0], X.shape[1]\n",
    "            \n",
    "            # Calculate X_prime\n",
    "            X_prime = X_prime_func(self, X, m)\n",
    "                   \n",
    "            # Number of columns in X', set as global variable\n",
    "            global n_prime\n",
    "            n_prime = n + 1\n",
    "        \n",
    "            # Function to calculate number of rows (samples) and Quantum Centroids for each class \n",
    "            def qcentroids_terms_func(i):\n",
    "                # Cast array y_class_index into a tensor and send from CPU to GPU\n",
    "                # Determine rows (samples) in X' belonging to either class\n",
    "                X_prime_class = X_prime[torch.CharTensor(y_class_index).cuda() == i]\n",
    "                                    \n",
    "                # Split X' belonging to either class into n_splits subsets, row-wise\n",
    "                # Send tensors from GPU to CPU and cast tensors into arrays, use np.array_split()\n",
    "                # because the equivalent torch.chunk() doesn't behave similarly to np.array_split()\n",
    "                X_prime_class_split_arr = np.array_split(X_prime_class.cpu().numpy(),\n",
    "                                                         indices_or_sections = self.n_splits,\n",
    "                                                         axis = 0)\n",
    "            \n",
    "                # Cast arrays back to tensors and send back from CPU to GPU\n",
    "                X_prime_class_split = [torch.tensor(a, dtype = self.dtype).cuda() \n",
    "                                       for a in X_prime_class_split_arr]\n",
    "            \n",
    "                # Function to calculate sum of quantum densities belonging to each class, \n",
    "                # per subset split\n",
    "                def X_prime_class_split_func(j):\n",
    "                    # Counter for j-th split of X'\n",
    "                    X_prime_class_split_jth = X_prime_class_split[j]\n",
    "                \n",
    "                    # Number of rows (samples) in j-th split of X'\n",
    "                    m_class_split = X_prime_class_split_jth.shape[0]\n",
    "                \n",
    "                    # Encode vectors into quantum densities\n",
    "                    density_chunk = torch.matmul(X_prime_class_split_jth.view(m_class_split, \n",
    "                                                                              n_prime, 1),\n",
    "                                                 X_prime_class_split_jth.view(m_class_split, \n",
    "                                                                              1, n_prime))\n",
    "                \n",
    "                    # Calculate n-fold Kronecker tensor product\n",
    "                    if self.n_copies == 1:\n",
    "                        density_chunk = density_chunk\n",
    "                    else:\n",
    "                        density_chunk_copy = density_chunk\n",
    "                        for _ in range(self.n_copies - 1):\n",
    "                            density_chunk = kronecker(density_chunk, density_chunk_copy)\n",
    "                    \n",
    "                    # Calculate sum of quantum densities\n",
    "                    density_chunk_sum = density_chunk.sum(dim = 0)\n",
    "                    return density_chunk_sum\n",
    "\n",
    "                # Number of rows/columns in density matrix, set as global variable\n",
    "                global density_nrow_ncol\n",
    "                density_nrow_ncol = n_prime**self.n_copies\n",
    "            \n",
    "                # Initialize tensor density_class_sum\n",
    "                density_class_sum = torch.zeros([density_nrow_ncol, density_nrow_ncol], \n",
    "                                                dtype = self.dtype).cuda()\n",
    "                for j in range(self.n_splits):\n",
    "                    # Calculate sum of quantum densities belonging to each class\n",
    "                    density_class_sum = density_class_sum + X_prime_class_split_func(j)\n",
    "            \n",
    "                # Number of rows (samples) in X' belonging to each class\n",
    "                m_class = X_prime_class.shape[0]\n",
    "            \n",
    "                # Function to calculate Quantum Centroid belonging to each class\n",
    "                def qcentroid_func():\n",
    "                    # Calculate Quantum Centroid belonging to each class\n",
    "                    # Added ZeroDivisionError as required by sklearn check_estimator\n",
    "                    try:\n",
    "                        qcentroid = (1/m_class)*density_class_sum\n",
    "                    except ZeroDivisionError:\n",
    "                        qcentroid = 0 \n",
    "                    return qcentroid\n",
    "            \n",
    "                # Calculate Quantum Centroid belonging to each class\n",
    "                qcentroid_class = qcentroid_func()\n",
    "                return m_class, qcentroid_class\n",
    "            \n",
    "            # Calculate number of rows (samples) and Quantum Centroids for each class \n",
    "            qcentroids_terms = [qcentroids_terms_func(i) for i in range(num_classes)]\n",
    "\n",
    "            # Determine Quantum Centroids\n",
    "            self.qcentroids_ = torch.stack([qcentroids_terms[z][1] for z in range(num_classes)], dim = 0)\n",
    "\n",
    "            # Calculate class weight\n",
    "            if self.class_weight == None:\n",
    "                class_weight_terms = torch.tensor([qcentroids_terms[y][0] for y in range(num_classes)], \\\n",
    "                                                  dtype = self.dtype)/m\n",
    "            elif self.class_weight == 'balanced':\n",
    "                class_weight_terms = torch.tensor([0.5 for k in range(num_classes)], dtype = self.dtype)\n",
    "            else:\n",
    "                raise ValueError('class_weight should be None or \"balanced\"')\n",
    "            \n",
    "            # When Pretty Good Measurement is specified\n",
    "            if self.measure == 'pgm':\n",
    "                # Function to calculate R\n",
    "                def R_func(a):\n",
    "                    return class_weight_terms[a]*self.qcentroids_[a]\n",
    "\n",
    "                # Calculate R\n",
    "                R = torch.stack([R_func(a) for a in range(num_classes)], dim = 0).sum(dim = 0)\n",
    "              \n",
    "                # Calculate square root of pseudo inverse of R\n",
    "                # Change datatype of R to float64 as the square root of a matrix calculation is highly \n",
    "                # senstive to numerical precision/rounding\n",
    "                # Calculate pseudo inverse of R, send tensor from GPU to CPU and cast into an array\n",
    "                # Use scipy.linalg.sqrtm() to calculate square root of the pseudo inverse of R because \n",
    "                # there is no equivalent function in PyTorch which behaves numerically similarly \n",
    "                # Remove complex part of the matrix created due to numerical precision/rounding issues\n",
    "                # in machine language\n",
    "                # Cast array back into a tensor and send back from CPU to GPU\n",
    "                sqrt_pinv_R = torch.tensor(np.real(linalg.sqrtm(torch.pinverse(torch.as_tensor(R, dtype = \\\n",
    "                                           torch.float64)).cpu().numpy())), dtype = self.dtype).cuda()\n",
    "                    \n",
    "                # Calculate kernel of R\n",
    "                # Change datatype of R to float64 as the kernel of a matrix calculation is highly\n",
    "                # senstive to numerical precision/rounding\n",
    "                # Send tensor from GPU to CPU and cast into an array, use scipy.linalg.null_space()\n",
    "                # to calculate kernel because there is no equivalent function in PyTorch which\n",
    "                # behaves numerically similarly\n",
    "                # Cast array back into a tensor and send back from CPU to GPU\n",
    "                ker_R = torch.tensor(linalg.null_space(torch.as_tensor(R, dtype = torch.float64).cpu() \\\n",
    "                                     .numpy()), dtype = self.dtype).cuda()\n",
    "                    \n",
    "                # Calculate projector of kernel of R\n",
    "                proj_ker_R = torch.matmul(ker_R, ker_R.T)\n",
    "                    \n",
    "                # Function to calculate Pretty Good Measurement\n",
    "                def pgm_func(b):\n",
    "                    return torch.matmul(torch.matmul(sqrt_pinv_R, class_weight_terms[b]*self.qcentroids_[b]), \n",
    "                                        sqrt_pinv_R) + (1/num_classes)*proj_ker_R\n",
    "                                               \n",
    "                # Calculate Pretty Good Measurement\n",
    "                self.pgms_ = torch.stack([pgm_func(b) for b in range(num_classes)], dim = 0)\n",
    "\n",
    "                # Function to calculate PGM bound\n",
    "                def pgm_bound_func(c):\n",
    "                    return class_weight_terms[c]*torch.einsum(einsum_unnest, self.qcentroids_[c], self.pgms_[c])\n",
    "\n",
    "                # Calculate PGM bound\n",
    "                self.pgm_bound_ = torch.stack([pgm_bound_func(c) for c in range(num_classes)], dim = 0) \\\n",
    "                                             .sum(dim = 0).item()\n",
    "            # When Helstrom measurement is specified\n",
    "            elif self.measure == 'hels':\n",
    "                # Calculate quantum Helstrom observable\n",
    "                hels_obs = class_weight_terms[0]*self.qcentroids_[0] \\\n",
    "                           - class_weight_terms[1]*self.qcentroids_[1]\n",
    "                \n",
    "                # Number of rows/columns in density matrix, set as global variable\n",
    "                global density_nrow_ncol\n",
    "                density_nrow_ncol = hels_obs.shape[0]\n",
    "                \n",
    "                # Calculate eigenvalues w and unit eigenvectors v of the quantum Helstrom observable\n",
    "                w, v = torch.symeig(hels_obs, eigenvectors = True)\n",
    "                \n",
    "                # Length of w\n",
    "                len_w = len(w)\n",
    "                \n",
    "                # Initialize tensor eigval_class\n",
    "                eigval_class = torch.empty_like(w, dtype = self.dtype).cuda()\n",
    "                for d in range(len_w):\n",
    "                    # Create a tensor of 0s and 1s to indicate positive and negative eigenvalues\n",
    "                    # respectively\n",
    "                    if w[d] > 0:\n",
    "                        eigval_class[d] = 0\n",
    "                    else:\n",
    "                        eigval_class[d] = 1\n",
    "                        \n",
    "                # Transpose matrix v containing eigenvectors to row-wise\n",
    "                eigvec = v.T\n",
    "                \n",
    "                # Function to calculate sum of the projectors corresponding to positive and negative\n",
    "                # eigenvalues respectively\n",
    "                def sum_proj_func(e):\n",
    "                    # Split eigenvectors belonging to positive or negative eigenvalues into n_splits subsets\n",
    "                    # Send tensors from GPU to CPU and cast tensors into arrays, use np.array_split()\n",
    "                    # because the equivalent torch.chunk() doesn't behave similarly to np.array_split()\n",
    "                    eigvec_class_split_arr_full = np.array_split(eigvec.cpu().numpy()[eigval_class.cpu() == e],\n",
    "                                                                 indices_or_sections = self.n_splits,\n",
    "                                                                 axis = 0)\n",
    "                    \n",
    "                    # Remove empty rows in eigvec_class_split_arr_full\n",
    "                    eigvec_class_split_arr = [f for f in eigvec_class_split_arr_full if f.shape[0] > 0]\n",
    "                    \n",
    "                    # Cast arrays back to tensors and send back from CPU to GPU\n",
    "                    eigvec_class_split = [torch.tensor(g, dtype = self.dtype).cuda()\n",
    "                                          for g in eigvec_class_split_arr]\n",
    "                    \n",
    "                    # Function to calculate sum of the projectors corresponding to positive and negative\n",
    "                    # eigenvalues respectively, per subset split\n",
    "                    def eigvec_class_split_func(h):\n",
    "                        # Counter for h-th split of eigvec\n",
    "                        eigvec_class_split_hth = eigvec_class_split[h]\n",
    "                        \n",
    "                        # Number of rows (samples) in h-th split of eigvec\n",
    "                        m_eigvec_class_split = eigvec_class_split_hth.shape[0]\n",
    "                        \n",
    "                        # Calculate projectors corresponding to positive and negative eigenvalues\n",
    "                        # respectively, per subset split\n",
    "                        proj_split = torch.matmul(eigvec_class_split_hth.view(m_eigvec_class_split,\n",
    "                                                                              density_nrow_ncol, 1),\n",
    "                                                  eigvec_class_split_hth.view(m_eigvec_class_split,\n",
    "                                                                              1, density_nrow_ncol))\n",
    "                        \n",
    "                        # Calculate sum of projectors\n",
    "                        proj_split_sum = proj_split.sum(dim = 0)\n",
    "                        return proj_split_sum\n",
    "                    \n",
    "                    # Determine length of eigvec_class_split_arr\n",
    "                    eigvec_class_split_arr_len = len(eigvec_class_split_arr)\n",
    "                    \n",
    "                    # Initialize tensor proj_class_sum\n",
    "                    proj_class_sum = torch.zeros([density_nrow_ncol, density_nrow_ncol],\n",
    "                                                 dtype = self.dtype).cuda()\n",
    "                    for h in range(eigvec_class_split_arr_len):\n",
    "                        # Calculate sum of the projectors corresponding to positive and negative eigenvalues\n",
    "                        # respectively\n",
    "                        proj_class_sum = proj_class_sum + eigvec_class_split_func(h)\n",
    "                    return proj_class_sum\n",
    "                \n",
    "                # Calculate sum of the projectors corresponding to positive and negative eigenvalues\n",
    "                # respectively\n",
    "                self.proj_sums_ = torch.stack([sum_proj_func(0), sum_proj_func(1)], dim = 0)\n",
    "                \n",
    "                # Calculate Helstrom bound\n",
    "                self.hels_bound_ = (class_weight_terms[0]*torch.einsum(einsum_unnest, self.qcentroids_[0],\n",
    "                                                                      self.proj_sums_[0])).item() \\\n",
    "                                   + (class_weight_terms[1]*torch.einsum(einsum_unnest, self.qcentroids_[1],\n",
    "                                                                        self.proj_sums_[1])).item()\n",
    "            # When Pretty Good Measurement or Helstrom measurement is misspecified\n",
    "            else:\n",
    "                raise ValueError('measure should be \"pgm\" or \"hels\"')\n",
    "        return self\n",
    "\n",
    "           \n",
    "    # Function for predict_proba\n",
    "    def predict_proba(self, X):\n",
    "        \"\"\"Performs PMG-HQC classification on X and returns the trace of the dot product of the \n",
    "        densities and the POV (positive operator-valued) measure, i.e. the class probabilities.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like, shape (n_samples, n_features)\n",
    "            The input samples. An array of int or float.       \n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        trace_matrix : array-like, shape (n_samples, n_classes)\n",
    "            Each column corresponds to the trace of the dot product of the densities and the POV \n",
    "            (positive operator-valued) measure for each class, i.e. each column corresponds to the \n",
    "            class probabilities. An array of float.\n",
    "        \"\"\"\n",
    "        # Send tensors self.pgms_ and self.proj_sums_ from GPU to CPU and cast into an array, and\n",
    "        # check if fit had been called\n",
    "        if self.measure == 'pgm':\n",
    "            self.pgms_arr_ = self.pgms_.cpu().numpy()\n",
    "            check_is_fitted(self, ['pgms_arr_'])\n",
    "        else:\n",
    "            self.proj_sums_arr_ = self.proj_sums_.cpu().numpy()\n",
    "            check_is_fitted(self, ['proj_sums_arr_'])\n",
    "               \n",
    "        # Check data in X as required by scikit-learn v0.25\n",
    "        X = self._validate_data(X, reset = False)\n",
    "        \n",
    "        # Number of rows in X\n",
    "        m = X.shape[0]\n",
    "        \n",
    "        # Calculate X_prime\n",
    "        X_prime = X_prime_func(self, X, m)\n",
    "                       \n",
    "        # Function to calculate trace values for each class\n",
    "        def trace_func(i):\n",
    "            # Split X' into n_splits subsets, row-wise\n",
    "            # Send tensors from GPU to CPU and cast tensors into arrays, use np.array_split()\n",
    "            # because the equivalent torch.chunk() doesn't behave similarly to np.array_split()\n",
    "            X_prime_split_arr_full = np.array_split(X_prime.cpu().numpy(),\n",
    "                                                    indices_or_sections = self.n_splits,\n",
    "                                                    axis = 0)\n",
    "            \n",
    "            # Remove empty rows in X_prime_split_arr_full\n",
    "            X_prime_split_arr = [a for a in X_prime_split_arr_full if a.shape[0] > 0]\n",
    "\n",
    "            # Cast arrays back to tensors and send back from CPU to GPU\n",
    "            X_prime_split = [torch.tensor(q, dtype = self.dtype).cuda() for q in X_prime_split_arr]\n",
    "            \n",
    "            # Function to calculate trace values for each class, per subset split\n",
    "            def trace_split_func(j):\n",
    "                # Counter for j-th split X'\n",
    "                X_prime_split_jth = X_prime_split[j]\n",
    "                \n",
    "                # Number of rows (samples) in j-th split X'\n",
    "                X_prime_split_m = X_prime_split_jth.shape[0]\n",
    "                \n",
    "                # Encode vectors into quantum densities\n",
    "                density_chunk = torch.matmul(X_prime_split_jth.view(X_prime_split_m, n_prime, 1),\n",
    "                                             X_prime_split_jth.view(X_prime_split_m, 1, n_prime))\n",
    "                \n",
    "                # Calculate n-fold Kronecker tensor product\n",
    "                if self.n_copies == 1:\n",
    "                    density_chunk = density_chunk\n",
    "                else:\n",
    "                    density_chunk_copy = density_chunk\n",
    "                    for _ in range(self.n_copies - 1):\n",
    "                        density_chunk = kronecker(density_chunk, density_chunk_copy)\n",
    "                        \n",
    "                # When Pretty Good Measurement is specified\n",
    "                if self.measure == 'pgm':\n",
    "                    # Calculate trace of the dot product of density of each row and Pretty Good\n",
    "                    # Measurement\n",
    "                    trace_class_split = torch.einsum(einsum_nest, density_chunk, self.pgms_[i])\n",
    "                # When Helstrom measurement is specified\n",
    "                else:\n",
    "                    # Calculate trace of the dot product of density of each row and sum of \n",
    "                    # projectors with corresponding positive and negative eigenvalues respectively\n",
    "                    trace_class_split = torch.einsum(einsum_nest, density_chunk, self.proj_sums_[i])\n",
    "                return trace_class_split\n",
    "            \n",
    "            # Determine length of X_prime_split_arr\n",
    "            X_prime_split_arr_len = len(X_prime_split_arr)\n",
    "\n",
    "            # Initialize tensor trace_class\n",
    "            trace_class = torch.empty([0], dtype = self.dtype).cuda()\n",
    "            for j in range(X_prime_split_arr_len):\n",
    "                # Calculate trace values for each class, per subset split\n",
    "                trace_class = torch.cat([trace_class, trace_split_func(j)], dim = 0)\n",
    "            return trace_class\n",
    "        \n",
    "        # Calculate trace values for each class, send from GPU to CPU and cast into an array\n",
    "        trace_matrix = torch.stack([trace_func(i) for i in range(num_classes)], dim = 1).cpu().numpy()\n",
    "        return trace_matrix\n",
    "                \n",
    "    \n",
    "    # Function for predict\n",
    "    def predict(self, X):\n",
    "        \"\"\"Performs PGM-HQC classification on X and returns the classes.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like, shape (n_samples, n_features)\n",
    "            The input samples. An array of int or float.\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        self.classes_[predict_trace_index] : array-like, shape (n_samples,)\n",
    "            The predicted binary classes. An array of str, int or float.\n",
    "        \"\"\"\n",
    "        # Determine column index with the higher trace value in trace_matrix\n",
    "        # Cast predict_proba(X) from an array into a tensor and send from CPU to GPU\n",
    "        # If both columns have the same trace value, returns column index 1, which is different \n",
    "        # to np.argmax() which returns column index 0\n",
    "        predict_trace_index = torch.argmax(torch.tensor(self.predict_proba(X), \n",
    "                                                        dtype = self.dtype).cuda(), axis = 1)\n",
    "        # Returns the predicted binary classes, send tensor from GPU to CPU and cast tensor\n",
    "        # into an array\n",
    "        return self.classes_[predict_trace_index.cpu().numpy()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KMdJ-Nhpq96Z"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "Y7raB5K8q989"
   },
   "outputs": [],
   "source": [
    "# appendicitis dataset (7 features, 106 rows)\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('appendicitis.tsv',delimiter='\\t')\n",
    "X = df.drop('target', axis=1).values\n",
    "y = df['target'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qBdb2VDFq-Au",
    "outputId": "5b42bd57-3714-46a6-8211-bdc8996beee0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    80.188679\n",
       "1    19.811321\n",
       "Name: target, dtype: float64"
      ]
     },
     "execution_count": 4,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check if class imbalance\n",
    "df['target'].value_counts(normalize=True)*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "kay9HcGuq-DE"
   },
   "outputs": [],
   "source": [
    "from sklearn import model_selection\n",
    "X_train, X_test, y_train, y_test = model_selection.train_test_split(X, y, test_size=0.2, random_state=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Wu5LcY6Iq-Hs",
    "outputId": "e7f93c45-a375-4f4e-ec28-43116d5bed5c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.7520661157024794, 0.8772541433572769)"
      ]
     },
     "execution_count": 6,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check F1 score and Helstrom bound values for various rescale and n_copies values\n",
    "model = PGMHQC_gpu_dtype(rescale=0.5, n_copies=3, encoding='stereo', measure='hels', class_weight=None, n_splits=1, dtype=torch.float32).fit(X_train, y_train)\n",
    "y_hat = model.predict(X_test)\n",
    "\n",
    "from sklearn import metrics\n",
    "metrics.f1_score(y_test, y_hat, average='weighted'), model.hels_bound_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "4e393v2Aq-MH"
   },
   "outputs": [],
   "source": [
    "# Testing using scikit-learn's GridSearchCV\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Create rescale hyperparamter list [0.1, 0.5, 1, 1.5,...,10.0]\n",
    "rescale_list1 = [0.1]\n",
    "rescale_list2 = np.linspace(0.5, 10, 20).tolist()\n",
    "rescale_list1.extend(rescale_list2)\n",
    "\n",
    "param_grid = {'rescale':rescale_list1, 'n_copies':[1, 2, 3, 4], 'encoding':['amplit', 'stereo'], 'measure':['hels'], 'class_weight':[None, 'balanced']}\n",
    "models = GridSearchCV(PGMHQC_gpu_dtype(n_splits=30, dtype=torch.float32), param_grid, scoring='f1_weighted').fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7vX43oy3rO_m",
    "outputId": "84bf1d41-57c9-46e0-fe5c-23c5e5282ae0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7676517031355741"
      ]
     },
     "execution_count": 8,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Best F1 score\n",
    "best_model = models.best_estimator_\n",
    "y_hat = best_model.predict(X_test)\n",
    "\n",
    "from sklearn import metrics\n",
    "metrics.f1_score(y_test, y_hat, average='weighted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1-qwZXDqrPEV",
    "outputId": "0f0ae6ce-f177-46c6-b8bf-43ca51da76d2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'class_weight': 'balanced',\n",
       " 'encoding': 'amplit',\n",
       " 'measure': 'hels',\n",
       " 'n_copies': 1,\n",
       " 'rescale': 9.0}"
      ]
     },
     "execution_count": 9,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Best hyperparameter combination\n",
    "models.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UkTX8RxRrPOk",
    "outputId": "c28065ed-5ade-4f4c-f81d-f27b592478f3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[86.66666667, 13.33333333],\n",
       "       [42.85714286, 57.14285714]])"
      ]
     },
     "execution_count": 10,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "confusion_matrix(y_test, y_hat, normalize='true')*100\n",
    "# 86.66666667% from 1st class correctly predicted\n",
    "# 57.14285714% from 2nd class correctly predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "s86_9-4mrPUs"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "IOc-w6mKZaL-"
   },
   "outputs": [],
   "source": [
    "# banana dataset (2 features, 5300 rows)\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('banana.tsv', sep='\\t')\n",
    "X = df.drop('target', axis=1).values\n",
    "y = df['target'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fnqul6ZlZaUq",
    "outputId": "839309ea-9c20-486c-ad5e-f4100d933909"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1.0    55.169811\n",
       " 1.0    44.830189\n",
       "Name: target, dtype: float64"
      ]
     },
     "execution_count": 21,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check if class imbalance\n",
    "df['target'].value_counts(normalize=True)*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "aAkCtrxcZaYG"
   },
   "outputs": [],
   "source": [
    "from sklearn import model_selection\n",
    "X_train, X_test, y_train, y_test = model_selection.train_test_split(X, y, test_size=0.2, random_state=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ppyehEoDZaar",
    "outputId": "b5d4a77f-66f3-4a80-8a72-e47df8a3738f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.858978398722441, 0.7732936143875122)"
      ]
     },
     "execution_count": 23,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check F1 score and Helstrom bound values for various rescale and n_copies values\n",
    "model = PGMHQC_gpu_dtype(rescale=0.5, n_copies=4, encoding='stereo', measure='hels', class_weight=None, n_splits=1, dtype=torch.float32).fit(X_train, y_train)\n",
    "y_hat = model.predict(X_test)\n",
    "\n",
    "from sklearn import metrics\n",
    "metrics.f1_score(y_test, y_hat, average='weighted'), model.hels_bound_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "tG2ucJ62ZadX"
   },
   "outputs": [],
   "source": [
    "# Testing using scikit-learn's GridSearchCV\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Create rescale hyperparamter list [0.1, 0.5, 1, 1.5,...,10.0]\n",
    "rescale_list1 = [0.1]\n",
    "rescale_list2 = np.linspace(0.5, 10, 20).tolist()\n",
    "rescale_list1.extend(rescale_list2)\n",
    "\n",
    "param_grid = {'rescale':rescale_list1, 'n_copies':[1, 2, 3, 4], 'encoding':['amplit', 'stereo'], 'measure':['hels'], 'class_weight':[None, 'balanced']}\n",
    "models = GridSearchCV(PGMHQC_gpu_dtype(n_splits=1, dtype=torch.float32), param_grid, scoring='f1_weighted').fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aLiG1KE9Zafv",
    "outputId": "758e956e-ca2d-43d3-e8e2-560dce361d8a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8718953078333712"
      ]
     },
     "execution_count": 25,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Best F1 score\n",
    "best_model = models.best_estimator_\n",
    "y_hat = best_model.predict(X_test)\n",
    "\n",
    "from sklearn import metrics\n",
    "metrics.f1_score(y_test, y_hat, average='weighted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cMkDP-jTZoYI",
    "outputId": "03ccb1a1-3dbc-426b-ad19-5f2130344380"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'class_weight': 'balanced',\n",
       " 'encoding': 'stereo',\n",
       " 'measure': 'hels',\n",
       " 'n_copies': 4,\n",
       " 'rescale': 0.5}"
      ]
     },
     "execution_count": 26,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Best hyperparameter combination\n",
    "models.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Q1qEngZnZobU",
    "outputId": "b235a262-8274-43c4-fecb-a7ba9a0912f8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[94.92385787,  5.07614213],\n",
       "       [22.17484009, 77.82515991]])"
      ]
     },
     "execution_count": 27,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "confusion_matrix(y_test, y_hat, normalize='true')*100\n",
    "# 94.92385787% from 1st class correctly predicted\n",
    "# 77.82515991% from 2nd class correctly predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZxIh0MLDZogw"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "SxwBVa8g3O3I"
   },
   "outputs": [],
   "source": [
    "# iris dataset (5 features, 150 rows)\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('iris.tsv', sep='\\t')\n",
    "X = df.drop('target', axis=1).values\n",
    "y = df['target'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5NgQEqmS3PD2",
    "outputId": "f3bbecdd-b9ae-4ff2-bf4d-d99e64d61700"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2    33.333333\n",
       "1    33.333333\n",
       "0    33.333333\n",
       "Name: target, dtype: float64"
      ]
     },
     "execution_count": 29,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check if class imbalance\n",
    "df['target'].value_counts(normalize=True)*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ILSNDYAz3PHY",
    "outputId": "5b2341d5-0051-49b5-e71e-5f4072839c10"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.2707228 , 0.36924452, 0.36003342, 1.0000007 ],\n",
       "       [0.22387266, 0.38475874, 0.39136937, 1.0000007 ],\n",
       "       [0.5708144 , 0.2715043 , 0.15768205, 1.0000007 ],\n",
       "       [0.2328984 , 0.370999  , 0.39610338, 1.0000007 ],\n",
       "       [0.21734022, 0.3778105 , 0.40485004, 1.0000007 ]], dtype=float32)"
      ]
     },
     "execution_count": 33,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import model_selection\n",
    "X_train, X_test, y_train, y_test = model_selection.train_test_split(X, y, test_size=0.2, random_state=4)\n",
    "\n",
    "# Check trace values sum to 1 for first 5 rows\n",
    "model = PGMHQC_gpu_dtype(rescale=0.5, n_copies=1, encoding='stereo', measure='pgm', class_weight=None, n_splits=1, dtype=torch.float32).fit(X_train, y_train)\n",
    "np.concatenate([model.predict_proba(X_test), np.sum(model.predict_proba(X_test), axis=1).reshape(-1,1)], axis=1)[0:5,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Yryk3aaX3PLv",
    "outputId": "aa1a42b5-b503-43c1-831d-dc916c5c1d6b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8666666666666667"
      ]
     },
     "execution_count": 34,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check F1 score for various n_copies values\n",
    "model = PGMHQC_gpu_dtype(rescale=0.5, n_copies=1, encoding='stereo', measure='pgm', class_weight=None, n_splits=1, dtype=torch.float32).fit(X_train, y_train)\n",
    "y_hat = model.predict(X_test)\n",
    "\n",
    "from sklearn import metrics\n",
    "metrics.f1_score(y_test, y_hat, average='weighted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "F_v-FSOJ3POJ",
    "outputId": "f81cc6f3-c55d-40fc-c8b5-5b0a922a7e03"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9007518796992481"
      ]
     },
     "execution_count": 35,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check F1 score for various n_copies values\n",
    "model = PGMHQC_gpu_dtype(rescale=0.5, n_copies=2, encoding='stereo', measure='pgm', class_weight=None, n_splits=1, dtype=torch.float32).fit(X_train, y_train)\n",
    "y_hat = model.predict(X_test)\n",
    "\n",
    "from sklearn import metrics\n",
    "metrics.f1_score(y_test, y_hat, average='weighted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9exWYZH73PTu",
    "outputId": "f5ca5bbc-bf19-4c7c-f994-2dd33be90901"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9340067340067341"
      ]
     },
     "execution_count": 36,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check F1 score for various n_copies values\n",
    "model = PGMHQC_gpu_dtype(rescale=0.5, n_copies=3, encoding='stereo', measure='pgm', class_weight=None, n_splits=1, dtype=torch.float32).fit(X_train, y_train)\n",
    "y_hat = model.predict(X_test)\n",
    "\n",
    "from sklearn import metrics\n",
    "metrics.f1_score(y_test, y_hat, average='weighted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Fhm1HRFS3PWv",
    "outputId": "1de68f57-1565-45a9-b471-7e92bde5fd7a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9340067340067341"
      ]
     },
     "execution_count": 37,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check F1 score for various n_copies values\n",
    "model = PGMHQC_gpu_dtype(rescale=0.5, n_copies=4, encoding='stereo', measure='pgm', class_weight=None, n_splits=1, dtype=torch.float32).fit(X_train, y_train)\n",
    "y_hat = model.predict(X_test)\n",
    "\n",
    "from sklearn import metrics\n",
    "metrics.f1_score(y_test, y_hat, average='weighted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Y_UCAjxY3PZG",
    "outputId": "2623d763-edeb-4aab-d3be-b49116184ede"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9340067340067341"
      ]
     },
     "execution_count": 38,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check F1 score for various n_copies values\n",
    "model = PGMHQC_gpu_dtype(rescale=0.5, n_copies=5, encoding='stereo', measure='pgm', class_weight=None, n_splits=1, dtype=torch.float32).fit(X_train, y_train)\n",
    "y_hat = model.predict(X_test)\n",
    "\n",
    "from sklearn import metrics\n",
    "metrics.f1_score(y_test, y_hat, average='weighted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "jBxditFXaBe8"
   },
   "outputs": [],
   "source": [
    "# Testing using scikit-learn's GridSearchCV\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Create rescale hyperparamter list [0.1, 0.5, 1, 1.5,...,10.0]\n",
    "rescale_list1 = [0.1]\n",
    "rescale_list2 = np.linspace(0.5, 10, 20).tolist()\n",
    "rescale_list1.extend(rescale_list2)\n",
    "\n",
    "param_grid = {'rescale':rescale_list1, 'n_copies':[1, 2, 3, 4], 'encoding':['amplit', 'stereo'], 'measure':['pgm'], 'class_weight':[None, 'balanced']}\n",
    "models = GridSearchCV(PGMHQC_gpu_dtype(n_splits=1, dtype=torch.float32), param_grid, scoring='f1_weighted').fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Qppun1WsaBiz",
    "outputId": "770af8fd-345a-48f8-97a8-d1d8441a08a1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 40,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Best F1 score\n",
    "best_model = models.best_estimator_\n",
    "y_hat = best_model.predict(X_test)\n",
    "\n",
    "from sklearn import metrics\n",
    "metrics.f1_score(y_test, y_hat, average='weighted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WB5gCp03aBv2",
    "outputId": "10acec7e-4f61-4f1c-9962-59b9b3a91d8d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'class_weight': None,\n",
       " 'encoding': 'amplit',\n",
       " 'measure': 'pgm',\n",
       " 'n_copies': 4,\n",
       " 'rescale': 0.5}"
      ]
     },
     "execution_count": 41,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Best hyperparameter combination\n",
    "models.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "M_2zBnNSaBye",
    "outputId": "771328d9-133f-470d-f38d-12dcfe29c810"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[100.,   0.,   0.],\n",
       "       [  0., 100.,   0.],\n",
       "       [  0.,   0., 100.]])"
      ]
     },
     "execution_count": 42,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "confusion_matrix(y_test, y_hat, normalize='true')*100\n",
    "# 100% from 1st class correctly predicted\n",
    "# 100% from 2nd class correctly predicted\n",
    "# 100% from 3rd class correctly predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rT613dCzaMk2"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "id": "LAYV5qJqaMus"
   },
   "outputs": [],
   "source": [
    "# balance-scale dataset (5 features, 625 rows)\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('balance-scale.tsv', sep='\\t')\n",
    "X = df.drop('target', axis=1).values\n",
    "y = df['target'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cAy9ltnDaMxk",
    "outputId": "be941e01-89db-4d71-b740-b7a1f84e01c1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2    46.08\n",
       "1    46.08\n",
       "0     7.84\n",
       "Name: target, dtype: float64"
      ]
     },
     "execution_count": 44,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check if class imbalance\n",
    "df['target'].value_counts(normalize=True)*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sKPQZVG8aM07",
    "outputId": "c4d25c84-04bf-4b33-b479-686de4690807"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.07412766, 0.44647816, 0.47940743, 1.0000132 ],\n",
       "       [0.0741693 , 0.44637394, 0.47945842, 1.0000017 ],\n",
       "       [0.07444036, 0.46400514, 0.46159616, 1.0000416 ],\n",
       "       [0.07733456, 0.43572628, 0.4870389 , 1.0000998 ],\n",
       "       [0.07502761, 0.463611  , 0.46141928, 1.0000579 ]], dtype=float32)"
      ]
     },
     "execution_count": 45,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import model_selection\n",
    "X_train, X_test, y_train, y_test = model_selection.train_test_split(X, y, test_size=0.2, random_state=4)\n",
    "\n",
    "# Check trace values sum to 1 for first 5 rows\n",
    "model = PGMHQC_gpu_dtype(rescale=1, n_copies=2, encoding='stereo', measure='pgm', class_weight=None, n_splits=1, dtype=torch.float32).fit(X_train, y_train)\n",
    "np.concatenate([model.predict_proba(X_test), np.sum(model.predict_proba(X_test), axis=1).reshape(-1,1)], axis=1)[0:5,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FHTzOSNfaM5s",
    "outputId": "06d2848a-0899-4834-8bd6-4b5f089fef6e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8320926002480364"
      ]
     },
     "execution_count": 46,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check F1 score for various n_copies values\n",
    "model = PGMHQC_gpu_dtype(rescale=1, n_copies=1, encoding='stereo', measure='pgm', class_weight=None, n_splits=1, dtype=torch.float32).fit(X_train, y_train)\n",
    "y_hat = model.predict(X_test)\n",
    "\n",
    "from sklearn import metrics\n",
    "metrics.f1_score(y_test, y_hat, average='weighted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sTkly02laM8w",
    "outputId": "ad9d8d6a-12b9-40b7-c36b-a18af7399f74"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8555660455486543"
      ]
     },
     "execution_count": 47,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check F1 score for various n_copies values\n",
    "model = PGMHQC_gpu_dtype(rescale=1, n_copies=2, encoding='stereo', measure='pgm', class_weight=None, n_splits=1, dtype=torch.float32).fit(X_train, y_train)\n",
    "y_hat = model.predict(X_test)\n",
    "\n",
    "from sklearn import metrics\n",
    "metrics.f1_score(y_test, y_hat, average='weighted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3gBfyHIFaM_7",
    "outputId": "2eb35070-c511-4b3b-c97e-25bc5029d114"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8555660455486543"
      ]
     },
     "execution_count": 48,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check F1 score for various n_copies values\n",
    "model = PGMHQC_gpu_dtype(rescale=1, n_copies=3, encoding='stereo', measure='pgm', class_weight=None, n_splits=1, dtype=torch.float32).fit(X_train, y_train)\n",
    "y_hat = model.predict(X_test)\n",
    "\n",
    "from sklearn import metrics\n",
    "metrics.f1_score(y_test, y_hat, average='weighted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FqqM9iWEaNC_",
    "outputId": "7a070d77-9145-4d9f-c0fd-5ada7177baaa"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8631679999999999"
      ]
     },
     "execution_count": 49,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### SCORE IS SLIGHTLY DIFFERENT BETWEEN FLOAT32 AND FLOAT64 (0.8676190476190477 vs. 0.8682253173918403, respectively) ###\n",
    "# Check F1 score for various n_copies values\n",
    "model = PGMHQC_gpu_dtype(rescale=1, n_copies=4, encoding='stereo', measure='pgm', class_weight=None, n_splits=1, dtype=torch.float32).fit(X_train, y_train)\n",
    "y_hat = model.predict(X_test)\n",
    "\n",
    "from sklearn import metrics\n",
    "metrics.f1_score(y_test, y_hat, average='weighted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6KdyPgO-aNep",
    "outputId": "f1681bee-e9a3-4088-ef7b-886d67893210"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8707515853322305"
      ]
     },
     "execution_count": 50,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check F1 score for various n_copies values\n",
    "model = PGMHQC_gpu_dtype(rescale=1, n_copies=5, encoding='stereo', measure='pgm', class_weight=None, n_splits=50, dtype=torch.float32).fit(X_train, y_train)\n",
    "y_hat = model.predict(X_test)\n",
    "\n",
    "from sklearn import metrics\n",
    "metrics.f1_score(y_test, y_hat, average='weighted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "id": "0P7atHlkaNis"
   },
   "outputs": [],
   "source": [
    "# Testing using scikit-learn's GridSearchCV\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Create rescale hyperparamter list [0.1, 0.5, 1, 1.5,...,10.0]\n",
    "rescale_list1 = [0.1]\n",
    "rescale_list2 = np.linspace(0.5, 10, 20).tolist()\n",
    "rescale_list1.extend(rescale_list2)\n",
    "\n",
    "param_grid = {'rescale':rescale_list1, 'n_copies':[1, 2, 3, 4], 'encoding':['amplit', 'stereo'], 'measure':['pgm'], 'class_weight':[None, 'balanced']}\n",
    "models = GridSearchCV(PGMHQC_gpu_dtype(n_splits=1, dtype=torch.float32), param_grid, scoring='f1_weighted').fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "P4Tk3crUaNlW",
    "outputId": "9825a266-1810-4787-c537-2379d6ccb754"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9106013291733841"
      ]
     },
     "execution_count": 52,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Best F1 score\n",
    "best_model = models.best_estimator_\n",
    "y_hat = best_model.predict(X_test)\n",
    "\n",
    "from sklearn import metrics\n",
    "metrics.f1_score(y_test, y_hat, average='weighted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JaJ-L8rHak5s",
    "outputId": "e6266a9a-c316-425a-f9c8-37589d7991b7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'class_weight': 'balanced',\n",
       " 'encoding': 'amplit',\n",
       " 'measure': 'pgm',\n",
       " 'n_copies': 2,\n",
       " 'rescale': 2.0}"
      ]
     },
     "execution_count": 53,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Best hyperparameter combination\n",
    "models.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hs7VcnpvalBu",
    "outputId": "fbc325d0-ae4f-4d5f-e99d-7dac184d07f4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[88.88888889,  0.        , 11.11111111],\n",
       "       [10.34482759, 87.93103448,  1.72413793],\n",
       "       [ 8.62068966,  0.        , 91.37931034]])"
      ]
     },
     "execution_count": 54,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "confusion_matrix(y_test, y_hat, normalize='true')*100\n",
    "# 88.88888889% from 1st class correctly predicted\n",
    "# 87.93103448% from 2nd class correctly predicted\n",
    "# 91.37931034% from 3rd class correctly predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jfTBny1UalE0"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "id": "TOcGhR1HalIk"
   },
   "outputs": [],
   "source": [
    "# new-thyroid dataset (6 features, 215 rows)\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('new-thyroid.tsv', sep='\\t')\n",
    "X = df.drop('target', axis=1).values\n",
    "y = df['target'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cxJFlVHFapS7",
    "outputId": "317c8ed6-0bb3-46f9-9b61-189dcd94c95a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    69.767442\n",
       "2    16.279070\n",
       "3    13.953488\n",
       "Name: target, dtype: float64"
      ]
     },
     "execution_count": 56,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check if class imbalance\n",
    "df['target'].value_counts(normalize=True)*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WqoMIQaOapWS",
    "outputId": "6caa39b1-7032-4930-cafb-0d8cf5c745b5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.69215024, 0.19132827, 0.11649358, 0.9999721 ],\n",
       "       [0.6900071 , 0.18992247, 0.12004522, 0.9999748 ],\n",
       "       [0.6923071 , 0.19116598, 0.11649961, 0.9999727 ],\n",
       "       [0.6923986 , 0.1911569 , 0.11641742, 0.99997294],\n",
       "       [0.69234943, 0.19143161, 0.11618779, 0.9999689 ]], dtype=float32)"
      ]
     },
     "execution_count": 57,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import model_selection\n",
    "X_train, X_test, y_train, y_test = model_selection.train_test_split(X, y, test_size=0.2, random_state=4)\n",
    "\n",
    "# Check trace values sum to 1 for first 5 rows\n",
    "model = PGMHQC_gpu_dtype(rescale=1.5, n_copies=3, encoding='stereo', measure='pgm', class_weight=None, n_splits=1, dtype=torch.float32).fit(X_train, y_train)\n",
    "np.concatenate([model.predict_proba(X_test), np.sum(model.predict_proba(X_test), axis=1).reshape(-1,1)], axis=1)[0:5,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "29aYh2DZapZa",
    "outputId": "8cd7d20a-c4d1-41c0-9ee6-9a49f3dc02a7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6040226272784412"
      ]
     },
     "execution_count": 58,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check F1 score for various n_copies values\n",
    "model = PGMHQC_gpu_dtype(rescale=1.5, n_copies=1, encoding='stereo', measure='pgm', class_weight=None, n_splits=1, dtype=torch.float32).fit(X_train, y_train)\n",
    "y_hat = model.predict(X_test)\n",
    "\n",
    "from sklearn import metrics\n",
    "metrics.f1_score(y_test, y_hat, average='weighted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NuIaZi65apdm",
    "outputId": "c97b64b3-86c1-4d8b-b068-4ca193ad587d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6040226272784412"
      ]
     },
     "execution_count": 59,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check F1 score for various n_copies values\n",
    "model = PGMHQC_gpu_dtype(rescale=1.5, n_copies=2, encoding='stereo', measure='pgm', class_weight=None, n_splits=1, dtype=torch.float32).fit(X_train, y_train)\n",
    "y_hat = model.predict(X_test)\n",
    "\n",
    "from sklearn import metrics\n",
    "metrics.f1_score(y_test, y_hat, average='weighted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "P9j0CYWgapiM",
    "outputId": "adcbc46a-f798-47ea-a1aa-854e0a67638f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6040226272784412"
      ]
     },
     "execution_count": 60,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### SCORE IS SLIGHTLY DIFFERENT BETWEEN FLOAT32 AND FLOAT64 (0.8314708547266686 vs. 0.8549863029667192, respectively) ###\n",
    "# Check F1 score for various n_copies values\n",
    "model = PGMHQC_gpu_dtype(rescale=1.5, n_copies=3, encoding='stereo', measure='pgm', class_weight=None, n_splits=1, dtype=torch.float32).fit(X_train, y_train)\n",
    "y_hat = model.predict(X_test)\n",
    "\n",
    "from sklearn import metrics\n",
    "metrics.f1_score(y_test, y_hat, average='weighted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pK6WuDkkapmw",
    "outputId": "276f3cb9-a451-4f0f-e606-4d7b82cf4265"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6040226272784412"
      ]
     },
     "execution_count": 61,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check F1 score for various n_copies values\n",
    "model = PGMHQC_gpu_dtype(rescale=1.5, n_copies=4, encoding='stereo', measure='pgm', class_weight=None, n_splits=1, dtype=torch.float32).fit(X_train, y_train)\n",
    "y_hat = model.predict(X_test)\n",
    "\n",
    "from sklearn import metrics\n",
    "metrics.f1_score(y_test, y_hat, average='weighted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uhgcvsjnappO",
    "outputId": "0da97034-dfad-4e6d-bcd0-b3c177f3132b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6040226272784412"
      ]
     },
     "execution_count": 62,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check F1 score for various n_copies values\n",
    "# 150+35+30 = 215\n",
    "model = PGMHQC_gpu_dtype(rescale=1.5, n_copies=5, encoding='stereo', measure='pgm', class_weight=None, n_splits=100, dtype=torch.float32).fit(X_train, y_train)\n",
    "y_hat = model.predict(X_test)\n",
    "\n",
    "from sklearn import metrics\n",
    "metrics.f1_score(y_test, y_hat, average='weighted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "id": "g-Z9E3QOaptd"
   },
   "outputs": [],
   "source": [
    "# Testing using scikit-learn's GridSearchCV\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Create rescale hyperparamter list [0.1, 0.5, 1, 1.5,...,10.0]\n",
    "rescale_list1 = [0.1]\n",
    "rescale_list2 = np.linspace(0.5, 10, 20).tolist()\n",
    "rescale_list1.extend(rescale_list2)\n",
    "\n",
    "# 150+35+30 = 215\n",
    "param_grid = {'rescale':rescale_list1, 'n_copies':[1, 2, 3, 4], 'encoding':['amplit', 'stereo'], 'measure':['pgm'], 'class_weight':[None, 'balanced']}\n",
    "models = GridSearchCV(PGMHQC_gpu_dtype(n_splits=1, dtype=torch.float32), param_grid, scoring='f1_weighted').fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bLSgvqbJap0S",
    "outputId": "6c9fc44a-ae65-492e-e88b-e9bcbbcf1721"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9516311369509043"
      ]
     },
     "execution_count": 65,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Best F1 score\n",
    "best_model = models.best_estimator_\n",
    "y_hat = best_model.predict(X_test)\n",
    "\n",
    "from sklearn import metrics\n",
    "metrics.f1_score(y_test, y_hat, average='weighted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LXZvoTnEap3G",
    "outputId": "e565244c-93e7-404b-ccd1-f8bade759c18"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'class_weight': 'balanced',\n",
       " 'encoding': 'amplit',\n",
       " 'measure': 'pgm',\n",
       " 'n_copies': 2,\n",
       " 'rescale': 0.1}"
      ]
     },
     "execution_count": 66,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Best hyperparameter combination\n",
    "models.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IMgf1GBCap6W",
    "outputId": "c79d92ac-5886-4044-b793-54fe0a51b114"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[100.,   0.,   0.],\n",
       "       [  0., 100.,   0.],\n",
       "       [ 20.,   0.,  80.]])"
      ]
     },
     "execution_count": 67,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "confusion_matrix(y_test, y_hat, normalize='true')*100\n",
    "# 100% from 1st class correctly predicted\n",
    "# 100% from 2nd class correctly predicted\n",
    "# 80% from 3rd class correctly predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FxqF7foNalMj"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "PGM-HQC-gpu-dtype.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
